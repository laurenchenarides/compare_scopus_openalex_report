---
title: "Methodology for Comparing Citation Database Coverage of Dataset Usage"
subtitle: "Technical Report"
author: 
  - name: "Lauren Chenarides, Ph.D."
    email: "Lauren.Chenarides@colostate.edu"
    affiliation: "Colorado State University"
  - name: "Calvin Bryan, Ph.D."
    email: "calvinrbryan@gmail.com"
    affiliation: "Colorado State University"    
  - name: "Rafael Ladislau"
    email: "rafa.ladis@gmail.com"
    affiliation: "RL Desenvolvimento de Sistemas LTDA"
  - name: "Julia Lane, Ph.D."
    email: "julia.lane@nyu.edu"
    affiliation: "New York University"
  - name: "Ming Wang"
    email: "Ming.Wang@colostate.edu"
    affiliation: "Colorado State University"
date: last-modified  # Auto-updates based on Git commits
format:
  html:
    toc: true  # Enables TOC on this page
    toc-depth: 3
    css: styles.css
bibliography: references.bib  # Link to your BibTeX file
csl: apa.csl  # Optional: Use a specific citation style (e.g., APA)
---

# Executive Summary

Federal agencies like the US Department of Agriculture (USDA) track how their datasets are referenced in research papers and disseminate data usage statistics through platforms like [DemocratizingData.ai](https://democratizingdata.ai/tools/dashboards/) and [NASS’s 5’s Data Usage Dashboard](https://www.nass.usda.gov/Data_Visualization/5W/index.php). This report presents a methodology for comparing citation databases as potential data sources for identifying dataset mentions within research papers, using Scopus, OpenAlex, and Dimensions as test cases. The methods described can be applied to evaluate other citation databases such as Web of Science, Crossref, and Microsoft Academic, to name a few.

Citation databases track academic research output. Different databases curate content (i.e., research output) in different ways - some focus on peer-reviewed journals while others include preprints and technical reports. Tracking dataset usage requires developing methods that scan publication text for dataset mentions. The accuracy of dataset tracking depends on the scope of research output we can access and analyze. Not to mention, dataset tracking requires reliable citation data from citation databases.

The **three** citation databases we are comparing are Elsevier’s Scopus, OurResearch's OpenAlex, and Digital Science's Dimensions.ai. **Scopus** (@sec-scopus) charges for access to its citation database. It focuses on peer-reviewed literature and provides metadata about authors, institutions, and citations for academic journals. **OpenAlex** (@sec-openalex), an open-source platform, offers free metadata access. It covers both traditional academic publications and other research outputs like preprints and technical reports. **Dimensions** (@sec-dimensions), developed by Digital Science, offers a hybrid model that provides both free and subscription-based access to its citation database. Unlike Scopus, which primarily indexes peer-reviewed journal articles, and OpenAlex, which emphasizes open-access content, Dimensions aggregates a broad spectrum of research outputs, including journal articles, books, clinical trials, patents, datasets, and policy documents. It integrates citation data with funding information, making it a useful tool for assessing the impact of research beyond traditional academic publishing.

Our methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. We developed procedures for:

-   Identifying publication coverage across citation databases
-   Deduplicating author records
-   Standardizing institution names
-   Cross-referencing publications between datasets
-   Analyzing research themes and institutional representation

<!---Beyond platform comparison, this methodology examines inclusivity in research coverage, particularly representation of MSIs. This component helps identify potential gaps in dataset accessibility and adoption across different types of research institutions.--->

Our comparison of citation databases found:

-   After deduplication, the number of distinct authors decreased by XX% in Scopus and XX% in OpenAlex, indicating significant duplicate entries in the raw data
-   Institutional coverage was broader in XX, with XX% more institutions represented compared to XX
-   Analysis revealed XX major themes in USDA dataset usage, with ?? and ?? being the most prominent
-   Minority-Serving Institutions (MSIs) represented only XX% of institutional users, highlighting opportunities for broader engagement

The methodology produced these reusable components:

-   Code repository for data cleaning and standardization
-   Cleaned author tables with disambiguated names and institutional affiliations
-   Standardized institution tables using IPEDS identifiers
-   Crosswalk table structure linking Scopus and OpenAlex publication records, authors, and institutions
-   Data schema documentation \[Last updated: January 3, 2025\]

# Terminology

Citation databases form the foundation of modern research tracking and analysis. Digital repositories, like the test cases featured in this report, systematically catalog scholarly publications and their references to each other [@debellis2009bibliometrics]. Citation databases differ in their approaches to curating and maintaining this information. Some focus exclusively on peer-reviewed journal articles with strict inclusion criteria, while others index a broader range of research outputs including preprints, technical reports, and conference proceedings [@martin2021google; @mongeon2016journal]. These curation approaches affect how comprehensively each database captures research impact [@visser2021large].

Understanding how these databases work requires familiarity with bibliometrics - the statistical analysis of published works and their impact [@broadus1987toward]. Bibliometric analysis examines patterns in publication, citation networks, and research influence [@hood2001literature]. The field emerged from early citation indices, which mapped relationships between papers through their references [@garfield1955citation].

For tracking USDA dataset usage, these concepts directly apply. Accurate tracking of dataset usage in scientific literature serves multiple purposes. For federal agencies like the USDA, it helps monitor the return on public data investments, find gaps in dataset use, plan future data collection, and support evidence-based policy decisions. This tracking requires reliable citation data from citation databases. Unlike standard citations, researchers often reference datasets within the text of their publications rather than citing them formally. This makes tracking dataset usage more complex.

To solve this tracking challenge, methods have been developed that scan publication text for dataset mentions [@lane2022data]. The scope and accuracy of our dataset tracking depends on what publications we can access and analyze. Because different databases curate content in different ways, it creates variation in what dataset mentions they capture and their frequency. Variations in content across sources affect our ability to accurately track dataset impact and adoption. The DemocratizingData.ai platform, for example, uses bibliometric data to monitor these dataset usage patterns, helping USDA understand how its data supports research. By comparing how different citation databases track this information, we can better understand their strengths and limitations for monitoring research impact.

# Project Background

The DemocratizingData.ai platform currently uses Scopus, a fee-based service, to track USDA dataset usage. OpenAlex, an open-source alternative, offers potential cost savings and broader access. Moving to OpenAlex would reduce operational costs while maintaining public access to USDA’s dataset usage metrics. However, the decision to transition from Scopus to OpenAlex requires systematic evaluation of their coverage and data quality.

Initial comparisons between Scopus and OpenAlex revealed unexpected patterns in coverage overlap. These findings suggested that simply replacing one database with another might affect the platform’s ability to track dataset usage accurately.

## Project Objective

The objective of this project is to determine the coverage of publications across citation databases. This information will be used to decide the viability of replacing Scopus with a less costly solution.

### Specific Aims {#sec-aims}

1. Evaluate differences in publication coverage across citation databases – Compare how well Scopus, OpenAlex, and Dimensions track dataset usage in research publications and assess variations in publication inclusion.
2. Assess journal coverage – Determine which journals each platform indexes and analyze how these differences impact dataset visibility.
3. Analyze publication-level discrepancies – Compare how each platform captures research publications within indexed journals and identify potential gaps in dataset tracking.
4. Examine author and institutional representation – Investigate how each platform attributes authorship and institutional affiliations, with a focus on variations by research institution type (e.g., Minority-Serving Institutions).
5. Develop a reproducible methodology for database comparison – Establish a systematic approach for evaluating other citation databases beyond Scopus, OpenAlex, and Dimensions.

These aims guided the development of a methodology for comparing citation databases, focusing on four areas:

1.  Journal coverage: Determining which journals each platform indexes
2.  Publication tracking: Comparing how each platform captures publications within indexed journals
3.  Author identification: Evaluating how each platform handles author names and affiliations
4.  Institution recognition: Determining how each platform records and standardizes institutional information

Research partners at the University of Utah have access to Dimensions. The scope of work expanded to include Dimensions alongside Scopus and OpenAlex. This inclusion provides a more comprehensive assessment of citation databases, particularly in evaluating dataset coverage across both proprietary and open-access platforms.

Our methodology provides a systematic approach for assessing citation databases' strengths and limitations in tracking dataset usage across research papers. Beyond platform comparison, this methodology examines differences in research coverage, particularly the presence of MSIs. This component helps identify variations in dataset coverage and usage across different types of research institutions.

The methods described in this report can be applied to other citation databases as alternatives to current data sources.

## Scopus {#sec-scopus}

## OpenAlex {#sec-openalex}

## Dimensions {#sec-dimensions}

---

# Project Workflow {#sec-workflow-overview}

The project workflow outlines the steps involved to evaluate how different citation databases track USDA dataset mentions in research papers. In searching for dataset mentions, the goal is to identify a set of publications that can be compared across the citation database test cases.

![](workflow/workflowv2.jpg)

The process of deriving the list of publications from a citation database consists of four steps. Each step produces an *output* which is used as the *input* for the following step:

**1. Define Scope of Data Assets to Be Searched** (@sec-data)

-   Identify USDA datasets that will be searched for and tracked.
-   Collect official dataset names along with common abbreviations, acronyms, and alternative references used.

**Result:** A structured list of dataset names and aliases.

**2. Extract Dataset Mentions from Publications** (@sec-data-idn)

-   Conduct searches across citation databases using multiple methods:
    (1)   Full-Text (String) Search: Scan entire articles for relevant dataset names.
    (2)   Reference Search: Identify dataset citations within publication references.
    (3)   Machine Learning Models: Apply Kaggle competition models trained to detect dataset mentions.
    
**Note:** In cases where full-text search is not supported by the citation database API (e.g., Scopus), an initial seed corpus of publications was collected separately to train machine learning models. Refer to @sec-seed-corpus for more details.

**Result:** Publication dataset for each data asset across each citation database.

**3. Pre-Process and Clean Publication Datasets** (@sec-disambiguation)

-   Pre-process and clean publication metadata generated from each citation database.
-   Standardize journal, institution, and author names.
-   Deduplicate records.

**Result:** Cleaned publication metadata, removed of duplicates, inconsistencies, and missing information.

**4. Compare across Citation Databases** (@sec-matching)

-   Compare dataset coverage across Scopus, OpenAlex, and Dimensions.
-   Apply fuzzy matching techniques to identify overlapping and unique dataset mentions.
-   Analyze differences in journal coverage, citation patterns, and author affiliations.

**Result:** A set of statistics used to evaluate dataset tracking accuracy.

---

## Define Scope of Data Assets to Be Searched {#sec-data}

<div style="background-color: #f9f5d7; padding: 10px; border-radius: 5px; border: 1px solid #ddd; margin-bottom: 0;">
<strong>The goal of this step is to compile a structured list of dataset names and their commonly used variations.</strong> </div> 

::: {.callout-note}
The data assets featured here consist of those collected by the USDA, primarily from the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). These data assets are widely used in agricultural economics and food systems research. 
:::

### National Agricultural Statistics Service (NASS) Data Assets {.unnumbered}

In late July 2023, an initial set of 21 reports were received containing a report name and a URL link to database curated by Cornell University. These reports are part of the USDA's efforts to track data usage across various research applications. However, the names of these reports were highly generic, making it difficult to precisely identify them in citation databases. Examples include reports titled "Agricultural Prices" and "Farm Labor," which lack specificity when compared to more structured dataset identifiers.

**Data Processing and Standardization**

To improve identification and searchability, the input was analyzed and transformed into a structured list that included:

-   International Standard Serial Numbers (ISSNs): Each of the 21 reports was assigned an ISSN, where available, to provide a standardized identifier.
-   Alias Creation: Generic report names were appended with the term report to better distinguish them from other similarly named publications in research literature.
-   Expanded Search Terms: Additional variations of dataset names were included to account for different citation styles and possible ways authors reference these reports.

The final dataset classification involved:

-   Main Data Asset (Parent Record): The original 21 reports, each representing a distinct dataset.
-   Aliases: ISSNs and URLs served as aliases to improve retrieval accuracy.
-   Search Term Expansion: Combining report names with different citation formats led to a total of 64 search terms (21 parent records + 43 aliases).

This standardization process improved the efficiency of identifying NASS datasets across publications indexed by citation databases such as Scopus, OpenAlex, and Dimensions.

### Economic Research Service (ERS) Data Assets {.unnumbered}

The process of identifying ERS data assets occurred in two phases: (1) an initial dataset compilation, and (2) a refinement process incorporating feedback from a team of agricultural economists at Colorado State University (CSU). This process was meant to yield a list of data assets was both comprehensive and relevant to the research community tracking USDA dataset usage.

**Phase 1: Initial Compilation of ERS Data Assets**

In October 2023, an initial list of 2,103 ERS records was compiled. These records included dataset names and, in some cases, associated aliases. The list was then reviewed by Professor Julia Lane, who identified and removed 144 records that were not suitable for machine learning-based dataset tracking.

**Reasons for Exclusion**

-   Records were too generic – Terms such as "Milk, Cotton, and CSV Format of National Data" were too broad to be meaningfully identified in citation databases.
-   Records were too specific – Entries such as "Table 15—Agricultural Chemical Input" and "Southeast: 1982-91, 1992-97" were references within broader reports rather than standalone data assets.

After these exclusions, the remaining 1,959 records represented the initial list of ERS data assets.

**Phase 2: Refinement with CSU Team**

A team of agricultural economists at CSU were consulted to refine the list so that it accurately captured key USDA datasets that may have been overlooked in the initial process. This involved:

-   Reviewing dataset usage in prior USDA research – Identifying which datasets were frequently cited.
-   Cross-checking with known data users – Ensuring that key datasets used by agricultural economists were included.
-   Expanding alias definitions – Recognizing dataset acronyms and alternative naming conventions.

As part of this process, an additional set of assets was incorporated, including datasets that had been previously identified in the Year 1 USDA project. Notably, datasets such as the Census of Agriculture and the Agricultural Resource Management Survey (ARMS) were added, along with key acronyms like FoodAPS. This phase contributed:

-   12 new parent records
-   8 additional alias records
-   Total: 20 new search terms

**Final Data Asset Identification**

Unlike NASS data assets, which had ISSNs and DOIs, ERS datasets were primarily linked through URLs. The final structured dataset included:

-   1,959 parent records (main ERS datasets)
-   1,959 alias records (URLs serving as dataset identifiers)
-   20 additional records from the CSU consultation
-   Total: 3,918 search terms

Through this two-phase process, the list of ERS data assets evolved from an initial broad set of records into a refined, structured collection of datasets that could be effectively tracked across citation databases.

### Final List of USDA Data Assets {.unnumbered}

The data assets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. The final set of data assets, their producing agencies, and descriptions are presented in @tbl-usda-datasets.

| Dataset Name | Produced By | Description |
|:-----------------------|:-----------------------|:-----------------------|
| [Census of Agriculture](https://www.nass.usda.gov/AgCensus/) | NASS | Conducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers. |
| [Agricultural Resource Management Survey (ARMS)](https://www.ers.usda.gov/data-products/arms-farm-financial-and-crop-production-practices/) | ERS | A USDA survey on farm financials, production practices, and resource use. |
| [Food Acquisition and Purchase Survey (FoodAPS)](https://www.ers.usda.gov/data-products/foodaps-national-household-food-acquisition-and-purchase-survey/) | ERS | A nationally representative survey tracking U.S. household food purchases and acquisitions. |
| [Current Population Survey Food Security Supplement (CPS-FSS)](https://www.ers.usda.gov/data-products/food-security-in-the-united-states) | ERS | An annual supplement to the Current Population Survey (CPS) measuring U.S. household food security. |
| [Food Access Research Atlas (FARA)](https://www.ers.usda.gov/data-products/food-access-research-atlas/) | ERS | A USDA tool mapping food access based on store locations and socioeconomic data. |
| [Rural-Urban Continuum Code (RUCC)](https://www.ers.usda.gov/data-products/rural-urban-continuum-codes/) | ERS | A classification system distinguishing U.S. counties by rural and urban characteristics. |
| [Household Food Security Survey Module](https://www.ers.usda.gov/topics/food-nutrition-assistance/food-security-in-the-u-s/survey-tools/) | ERS | A USDA survey module used to assess food insecurity levels in households. |
| [Local Food Marketing Practices Survey](https://www.nass.usda.gov/Surveys/Guide_to_NASS_Surveys/Local_Food/) | NASS | A USDA survey on U.S. farms' local food sales, direct-to-consumer marketing, and supply chains. |
| [Farm to School Census](https://farmtoschoolcensus.fns.usda.gov/) | FNS | A USDA survey tracking school food procurement and local farm partnerships. |
| [Quarterly Food at Home Price Database (QFAHPD)](https://www.ers.usda.gov/data-products/quarterly-food-at-home-price-database/) | ERS | A database of U.S. retail food prices by product, region, and time. |
| [Tenure Ownership and Transition of Agricultural Land (TOTAL)](https://www.nass.usda.gov/Surveys/Guide_to_NASS_Surveys/TOTAL/) | NASS | A survey collecting data on farmland ownership, leasing, and transfer. |
| [Transition of Agricultural Land Survey](https://www.nass.usda.gov/Surveys/Guide_to_NASS_Surveys/TOTAL/) | NASS | A component of TOTAL that examines farmland ownership changes and succession plans. |
| [Information Resources, Inc. (IRI) InfoScan](https://www.iriworldwide.com/en-us/solutions/consumer-panel/infoscan) | Circana (formerly IRI) | A commercial scanner dataset tracking retail food and consumer goods purchases. |

: List of USDA Data Assets {#tbl-usda-datasets}

#### Appendix: Data Asset - Alias Dyads {.unnumbered}

To provide a comprehensive reference for dataset tracking, the following appendix includes a detailed list of data assets and their corresponding aliases, collectively referred to as dyads. Each dyad represents a dataset-name and alias pair used in citation database searches, allowing for more precise identification of dataset mentions in research publications. These aliases include acronyms, alternate spellings, dataset variations, and associated URLs, ensuring broad coverage across different citation practices. The dyad list serves as the foundation for dataset extraction and disambiguation across Scopus, OpenAlex, and Dimensions. A complete reference of these dataset aliases is included in [Appendix XX](appendices/app_dyads.qmd).

## Extract Dataset Mentions to Build a Publication Dataset {#sec-data-idn}

<div style="background-color: #f9f5d7; padding: 10px; border-radius: 5px; border: 1px solid #ddd; margin-bottom: 0;">
<strong>The goal of this step is to build a dataset of publications that reference the dataset name aliases for the USDA data assets (@tbl-usda-datasets) across Scopus, OpenAlex, and Dimensions.</strong> </div> 

To generate this dataset, the process requires:

- Dataset name aliases (from Step 1)
- Search routines tailored to each citation database to extract relevant publications

Search routines, described below, guide this step, as dataset mentions are often inconsistent across publications—appearing in titles, abstracts, full text, or reference lists. Scopus uses a structured seed corpus to refine searches, while OpenAlex and Dimensions rely on direct queries across their full publication records. The outputs of this step are three publication-level datasets, one for each citation database, which will be further analyzed in subsequent steps.

::: panel-tabset
{{< include workflow/03scopus.qmd >}}

{{< include workflow/03openalex.qmd >}}

{{< include workflow/03dimensions.qmd >}}
:::

## Scopus-Specific: Create a Seed Corpus {#sec-seed-corpus .unnumbered}

After defining the data assets and aliases in Step 1 (@sec-data), the next step is to identify where these datasets are referenced in research publications. **Unlike OpenAlex and Dimensions, which allow for direct full-text searches, Scopus requires a structured seed corpus to establish a more targeted search space.**

For **Scopus**, this seed corpus approach was necessary to balance recall (capturing relevant mentions) and precision (minimizing false positives). The process involved:

- Restricted search strategies using reference lists and available full-text searches
- Machine learning-assisted review to refine dataset mentions
- Manual refinements to resolve ambiguities, consolidate duplicate aliases, and incorporate missing terms.

This structured search space helped mitigate the constraints of Scopus's API and ensured that dataset mentions were captured as comprehensively as possible before proceeding to the data identification step (@sec-data-idn).

::: panel-tabset
{{< include workflow/02scopus.qmd >}}

{{< include workflow/02openalex.qmd >}}

{{< include workflow/02dimensions.qmd >}}
:::


<!-- -   Collect an initial set of publications. -->
<!-- -   Analyze how datasets are referenced. -->
<!-- -   Determine how well the dataset name variations (from Step 1) are retrieved from the publications. -->
<!-- -   Adjust searches to improve accuracy. -->

<!-- **Result:** A set of seed publications to inform dataset identification and search procedures. -->



## Pre-Process and Clean Publication Datasets {#sec-disambiguation}

<div style="background-color: #f9f5d7; padding: 10px; border-radius: 5px; border: 1px solid #ddd; margin-bottom: 0;">
<strong>The goal of this step is to standardize and resolve inconsistencies in publication records by disambiguating journal names, author affiliations, and institutions across the three databases.</strong> </div> 

To compare publication coverage across citation databases, we first identify all journals that contain publications using each dataset in Scopus, OpenAlex, and Dimensions. @tbl-usda-datasets displays a list of the datasets for which we evaluate coverage. 

We subset the publication dataset from Step 2 by filtering for dataset mentions. For example, if a publication references Ag Census, it is included in the Ag Census sub-dataset; otherwise, it is excluded. This process identifies dataset-specific publication patterns in Scopus, OpenAlex, and Dimensions.

Our approach follows a hierarchical approach to understand how USDA data assets appear in these citation databases.  

1. **Journal Level** – Identifies journals publishing research using USDA datasets. A journal is included if at least one of its publications references the dataset, but this does not indicate overall dataset prevalence within that journal.

2. **Publication Level** – Examines individual publications within these journals to assess how often and in what context USDA datasets appear.

3. **Author Level** – Tracks authors of these publications, analyzing institutional affiliations and research networks to understand dataset reach.

4. **Institution Level** – Maps dataset usage across institutions to identify geographic and organizational research patterns.

This structured approach standardizes dataset mention analysis across databases, allowing for direct comparisons of coverage and research impact.

### Case Study: Census of Agriculture

::: {.custom-box}

To illustrate the data cleaning and disambiguation process, we use the Census of Agriculture as a case study to systematically compare coverage, overlap, and differences between the three citation databases. The Census of Agriculture (also referred to as "Ag Census") is widely used in agricultural and economic research, making it an ideal dataset for assessing database differences.


### Pre-Processing Steps by Citation Database {.unnumbered}

::: panel-tabset
{{< include workflow/04scopus.qmd >}}

{{< include workflow/04openalex.qmd >}}

{{< include workflow/04dimensions.qmd >}}
:::

### Results from Data Cleaning {.unnumbered}

*This section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset*

Step 4 produces two publication-level datasets: one of all academic papers released through Scopus that use Ag Census data and a similar one for OpenAlex. 

There are 4712 unique publications reported in Scopus and 1266 in OpenAlex. These data are collapsed into a journal-level dataset based on the International Standard Serial Number (ISSN) that is unique to each academic journal. 


::: panel-tabset
{{< include workflow/04scopus_r.qmd >}}

{{< include workflow/04openalex_r.qmd >}}

{{< include workflow/04dimensions_r.qmd >}}
:::
:::

## Compare Results across Citation Databases {#sec-matching}

<div style="background-color: #f9f5d7; padding: 10px; border-radius: 5px; border: 1px solid #ddd; margin-bottom: 0;">
<strong>The goal of this step is to develop statistics that measure dataset tracking accuracy.</strong> </div> 

### Case Study: Census of Agriculture

::: {.custom-box}
Continuing with our case study, we use the datasets produced in Step 4 to produce counts of the number of journals with Ag Census publications that:

(i) only appear in Scopus, 
(ii) only appear in OpenAlex, or 
(iii) appear in both.

For journals that contain Ag Census data in both citation databases, we summarize the coverage of publications that appear in both Scopus and OpenAlex. 

Then, we investigate discrepancies based on factors like missing identifiers, mismatched journal information (ISSNs), and additional publications accessed through OpenAlex’s API.

**Add here: What are the steps in producing Table AA**


::: panel-tabset
{{< include workflow/05scopus.qmd >}}

{{< include workflow/05openalex.qmd >}}

{{< include workflow/05dimensions.qmd >}}
:::

### Results from Database Comparison

*This section presents results after matching (which type varies – deterministic vs fuzzy)*

::: panel-tabset
{{< include workflow/05scopus_r.qmd >}}

{{< include workflow/05openalex_r.qmd >}}

{{< include workflow/05dimensions_r.qmd >}}
:::

:::

::: {.callout-note title="Summary of Matching Methods" collapse=true}

1.  Rule-based matching for exact matches
2.  Probabilistic matching for handling variations
3.  Machine learning methods for complex cases

| Method | Considerations | Example | Pros | Cons |
|:--------------|:--------------|:--------------|:--------------|:--------------|
| Searching for dataset names within Scopus |  |  |  |  |
| Searching for dataset names within OpenAlex | “Location” field set to “journal” |  |  |  |
| Disambiguation of authors |  |  |  |  |
| Disambiguation of institutions |  |  |  |  |
| Standardization of institutions |  |  |  |  |
| Searching based on the frequency of dataset appearance in journals |  |  |  |  |
| MORE . . . |  |  |  |  |
| Filtering on keywords to determine themes |  |  |  |  |

: Summary of Methods {#tbl-methods-summary}
:::

# Recommendations

## Technical Recommendations

## Expanding Dataset Usage

This report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.

Given the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database.

# Conclusion and Next Steps

# Appendices

All appendices referenced throughout the report are located on [this page](appendices.qmd).

# References
