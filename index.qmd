---
title: "Methodology for Comparing Citation Database Coverage of Dataset Usage"
subtitle: "Technical Report"
author: 
  - name: "Lauren Chenarides, Ph.D."
    email: "Lauren.Chenarides@colostate.edu"
    affiliation: "Colorado State University"
  - name: "Calvin Bryan, Ph.D."
    email: "calvinrbryan@gmail.com"
    affiliation: "Colorado State University"    
  - name: "Rafael Ladislau"
    email: "rafa.ladis@gmail.com"
    affiliation: "RL Desenvolvimento de Sistemas LTDA"
  - name: "Julia Lane, Ph.D."
    email: "julia.lane@nyu.edu"
    affiliation: "New York University"
  - name: "Ming Wang"
    email: "Ming.Wang@colostate.edu"
    affiliation: "Colorado State University"
date: last-modified  # Auto-updates based on Git commits
format:
  html:
    toc: true  # Enables TOC on this page
    toc-depth: 3
bibliography: references.bib  # Link to your BibTeX file
csl: apa.csl  # Optional: Use a specific citation style (e.g., APA)
---

# Executive Summary

Federal agencies like the US Department of Agriculture (USDA) track how their datasets are referenced in research papers and disseminate data usage statistics through platforms like [DemocratizingData.ai](https://democratizingdata.ai/tools/dashboards/) and [NASS’s 5’s Data Usage Dashboard](https://www.nass.usda.gov/Data_Visualization/5W/index.php). This report presents a methodology for comparing citation databases as potential data sources for identifying dataset mentions within research papers, using Scopus, OpenAlex, and Dimensions as test cases. The methods described can be applied to evaluate other citation databases such as Web of Science, Crossref, and Microsoft Academic, to name a few.

Citation databases track academic research output. Different databases curate content (i.e., research output) in different ways - some focus on peer-reviewed journals while others include preprints and technical reports. Tracking dataset usage requires developing methods that scan publication text for dataset mentions. The accuracy of dataset tracking depends on the scope of research output we can access and analyze. Not to mention, dataset tracking requires reliable citation data from citation databases.

The **three** citation databases we are comparing are Elsevier’s Scopus, OurResearch's OpenAlex, and Digital Science's Dimensions.ai. **Scopus** (@sec-scopus) charges for access to its citation database. It focuses on peer-reviewed literature and provides metadata about authors, institutions, and citations for academic journals. **OpenAlex** (@sec-openalex), an open-source platform, offers free metadata access. It covers both traditional academic publications and other research outputs like preprints and technical reports. **Dimensions** (@sec-dimensions), developed by Digital Science, offers a hybrid model that provides both free and subscription-based access to its citation database. Unlike Scopus, which primarily indexes peer-reviewed journal articles, and OpenAlex, which emphasizes open-access content, Dimensions aggregates a broad spectrum of research outputs, including journal articles, books, clinical trials, patents, datasets, and policy documents. It integrates citation data with funding information, making it a useful tool for assessing the impact of research beyond traditional academic publishing.

Our methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. We developed procedures for:

-   Identifying publication coverage across citation databases
-   Deduplicating author records
-   Standardizing institution names
-   Cross-referencing publications between datasets
-   Analyzing research themes and institutional representation

<!---Beyond platform comparison, this methodology examines inclusivity in research coverage, particularly representation of MSIs. This component helps identify potential gaps in dataset accessibility and adoption across different types of research institutions.--->

Our comparison of citation databases found:

-   After deduplication, the number of distinct authors decreased by XX% in Scopus and XX% in OpenAlex, indicating significant duplicate entries in the raw data
-   Institutional coverage was broader in XX, with XX% more institutions represented compared to XX
-   Analysis revealed XX major themes in USDA dataset usage, with ?? and ?? being the most prominent
-   Minority-Serving Institutions (MSIs) represented only XX% of institutional users, highlighting opportunities for broader engagement

The methodology produced these reusable components:

-   Code repository for data cleaning and standardization
-   Cleaned author tables with disambiguated names and institutional affiliations
-   Standardized institution tables using IPEDS identifiers
-   Crosswalk table structure linking Scopus and OpenAlex publication records, authors, and institutions
-   Data schema documentation \[Last updated: January 3, 2025\]

# Terminology

Citation databases form the foundation of modern research tracking and analysis. Digital repositories, like the test cases featured in this report, systematically catalog scholarly publications and their references to each other [@debellis2009bibliometrics]. Citation databases differ in their approaches to curating and maintaining this information. Some focus exclusively on peer-reviewed journal articles with strict inclusion criteria, while others index a broader range of research outputs including preprints, technical reports, and conference proceedings [@martin2021google; @mongeon2016journal]. These curation approaches affect how comprehensively each database captures research impact [@visser2021large].

Understanding how these databases work requires familiarity with bibliometrics - the statistical analysis of published works and their impact [@broadus1987toward]. Bibliometric analysis examines patterns in publication, citation networks, and research influence [@hood2001literature]. The field emerged from early citation indices, which mapped relationships between papers through their references [@garfield1955citation].

For tracking USDA dataset usage, these concepts directly apply. Accurate tracking of dataset usage in scientific literature serves multiple purposes. For federal agencies like the USDA, it helps monitor the return on public data investments, find gaps in dataset use, plan future data collection, and support evidence-based policy decisions. This tracking requires reliable citation data from citation databases. Unlike standard citations, researchers often reference datasets within the text of their publications rather than citing them formally. This makes tracking dataset usage more complex.

To solve this tracking challenge, methods have been developed that scan publication text for dataset mentions [@lane2022data]. The scope and accuracy of our dataset tracking depends on what publications we can access and analyze. Because different databases curate content in different ways, it creates variation in what dataset mentions they capture and their frequency. Variations in content across sources affect our ability to accurately track dataset impact and adoption. The DemocratizingData.ai platform, for example, uses bibliometric data to monitor these dataset usage patterns, helping USDA understand how its data supports research. By comparing how different citation databases track this information, we can better understand their strengths and limitations for monitoring research impact.

# Project Background

The DemocratizingData.ai platform currently uses Scopus, a fee-based service, to track USDA dataset usage. OpenAlex, an open-source alternative, offers potential cost savings and broader access. Moving to OpenAlex would reduce operational costs while maintaining public access to USDA’s dataset usage metrics. However, the decision to transition from Scopus to OpenAlex requires systematic evaluation of their coverage and data quality.

Initial comparisons between Scopus and OpenAlex revealed unexpected patterns in coverage overlap. These findings suggested that simply replacing one database with another might affect the platform’s ability to track dataset usage accurately.

This led to the development of a methodology for comparing citation databases, focusing on four areas:

1.  Journal coverage: Determining which journals each platform indexes
2.  Publication tracking: Comparing how each platform captures publications within indexed journals
3.  Author identification: Evaluating how each platform handles author names and affiliations
4.  Institution recognition: Determining how each platform records and standardizes institutional information

Research partners at the University of Utah have access to Dimensions. The scope of work expanded to include Dimensions alongside Scopus and OpenAlex. This inclusion provides a more comprehensive assessment of citation databases, particularly in evaluating dataset coverage across both proprietary and open-access platforms. 

Our methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. Beyond platform comparison, this methodology examines differences in research coverage, particularly the presence of MSIs. This component helps identify variations in dataset coverage and usage across different types of research institutions.

The methods described in this report can be applied to other citation databases as alternatives to current data sources.

## Scopus {#sec-scopus}

## OpenAlex {#sec-openalex}

## Dimensions {#sec-dimensions}

# Analysis Workflow

## Overview 

The workflow we developed systematically evaluates how different citation databases track USDA dataset mentions in research papers. In searching for dataset mentions, the goal is to identify a set of publications that can be compared across the citation database test cases.

The process of deriving the list of publications consists of four steps:

**1. Define Data Assets and Aliases** (@sec-data)

- Identify key USDA datasets that need to be tracked.
- Collect official dataset names along with common abbreviations, acronyms, and alternative references used in research.

Result: A structured list of dataset names and aliases to be used for search queries.

**2. Create a Seed Corpus** (@sec-seed-corpus)

- Identify variations in how datasets are referenced in research papers.
- Compile a list of dataset names, acronyms, and alternative phrasings.

Result: A comprehensive vocabulary to standardize dataset mentions.

**3. Dataset Extraction** (@sec-data-extraction)

- Extract dataset mentions using multiple search strategies:
  - Machine Learning Models: Apply Kaggle competition models trained to detect dataset mentions.
  - Full-Text (String) Search: Scan entire research articles for relevant dataset names.
  - Reference Search: Identify dataset citations within research references.

Result: Different extraction methods yield different sets of publication results.

**4. Disambiguation Process** (@sec-disambiguation)

- Pre-process and clean extracted data for each citation database.
- Resolve inconsistencies in author names and institutional affiliations through deduplication.
- Standardize institution names using external reference data (e.g., IPEDS).

Result: Clean datasets, removed of duplicates and inconsistencies, which are used to link publications, authors, and institutions across citation databases.

**5. Comparison across Databases** (@sec-matching)

- Compare dataset coverage across Scopus, OpenAlex, and Dimensions.
- Apply fuzzy matching techniques to identify overlapping and unique dataset mentions.
- Analyze differences in journal coverage, citation patterns, and author affiliations.

Result: In generating these statistics, we can evaluate the impact of switching databases on dataset tracking accuracy.

## Step 1: Define Data Assets and Aliases {#sec-data}

The data assets featured here consist of those collected by the USDA, primarily from the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). These datasets are widely used in agricultural economics and food systems research. The goal of this step is to compile a structured list of dataset names and their commonly used variations to achieve adequate search coverage.

### National Agricultural Statistics Service (NASS) Data Assets

In late July 2023, an initial set of 21 reports were received containing a report name and a URL link to the Cornell database. The names of these reports were characterized be being very generic in nature e.g. “Agricultural Prices” and “Farm Labor”. This input was analysed and transformed into a list that also contained ISSN inputs and which appended generic names with the term report. An ISSN term was identified for each of the 21 reports. The report names were characterized as the main data asset (the parent record) and the ISSNs and URLs were used as aliases. There were 43 such aliases. In total there were therefore 64 (43 + 21) search terms associated with this input.

### Economic Research Service (ERS) Data Assets

The list of ERS Data Assets was provided during October 2023. In the original list 2,103 records were provided, many of which also had identified aliases. The list was reviewed by professor Julia Lane and 144 records were removed on the basis that they were not amendable for use within the Machine Learning routines. That review was completed on 29 October. The principal reasons for exclusion were terms either being too generic or they were too specific (and likely to be references within a broader report). Examples of the former were records such as “Milk”, “Cotton” and “CSV Format of National Data “. Examples of the latter were records such as “Table 15—Agricultural chemical input” and “Southeast: 1982-91 1992-97”. Once these records were removed, a total of 1,959 records were left from this input.

These records represented the main data assets (the parent record). These records were then subject to review to identify their associated aliases. Unlike the NASS data assets, it was not possible to reliably find an ISSNs of DoIs for these assets. However, for each of these records, a URL link had been provided and these were used as aliases. There were this a further 1,959 terms used as an alias. In total there were therefore 3,918 (1,959 + 1,959) search terms associated with this input.

Before the run to create the seed corpus was conducted, a further list of assets was identified. This includes some terms that had been searched for in the Year 1 USDA project (e.g. Census of Agriculture and Agricultural Resource Management Survey) alongside some associated acronyms e.g. FoodAPS. It proved possible to incorporate these late additions. In total these added an additional 20 search terms, comprising 12 main (parent) records and a further 8 alias records.

Through consultation with USDA stakeholders and recognition that agricultural economists represent a significant user group of USDA-produced datasets, we identified 13 key datasets to feature on the dashboard. This selection emerged from a broader review of USDA data assets that included over 2,000 ERS records and 21 NASS reports, as described above. The selected datasets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. @tbl-usda-datasets presents these datasets, their producing agencies, and brief descriptions.

| Dataset Name | Produced By | Description |
|:---|:---|:---|
| [Census of Agriculture](https://www.nass.usda.gov/AgCensus/) | NASS | Conducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers. |
| [Agricultural Resource Management Survey (ARMS)](https://www.ers.usda.gov/data-products/arms-farm-financial-and-crop-production-practices/) | ERS | A USDA survey on farm financials, production practices, and resource use. |
| [Food Acquisition and Purchase Survey (FoodAPS)](https://www.ers.usda.gov/data-products/foodaps-national-household-food-acquisition-and-purchase-survey/) | ERS | A nationally representative survey tracking U.S. household food purchases and acquisitions. |
| [Current Population Survey Food Security Supplement (CPS-FSS)](https://www.ers.usda.gov/data-products/food-security-in-the-united-states) | ERS | An annual supplement to the Current Population Survey (CPS) measuring U.S. household food security. |
| [Food Access Research Atlas (FARA)](https://www.ers.usda.gov/data-products/food-access-research-atlas/) | ERS | A USDA tool mapping food access based on store locations and socioeconomic data. |
| [Rural-Urban Continuum Code (RUCC)](https://www.ers.usda.gov/data-products/rural-urban-continuum-codes/) | ERS | A classification system distinguishing U.S. counties by rural and urban characteristics. |
| [Household Food Security Survey Module](https://www.ers.usda.gov/topics/food-nutrition-assistance/food-security-in-the-u-s/survey-tools/) | ERS | A USDA survey module used to assess food insecurity levels in households. |
| [Local Food Marketing Practices Survey](https://www.nass.usda.gov/Surveys/Guide_to_NASS_Surveys/Local_Food/) | NASS | A USDA survey on U.S. farms' local food sales, direct-to-consumer marketing, and supply chains. |
| [Farm to School Census](https://farmtoschoolcensus.fns.usda.gov/) | FNS | A USDA survey tracking school food procurement and local farm partnerships. |
| [Quarterly Food at Home Price Database (QFAHPD)](https://www.ers.usda.gov/data-products/quarterly-food-at-home-price-database/) | ERS | A database of U.S. retail food prices by product, region, and time. |
| [Tenure Ownership and Transition of Agricultural Land (TOTAL)](https://www.nass.usda.gov/Surveys/Guide_to_NASS_Surveys/TOTAL/) | NASS | A survey collecting data on farmland ownership, leasing, and transfer. |
| [Transition of Agricultural Land Survey](https://www.nass.usda.gov/Surveys/Guide_to_NASS_Surveys/TOTAL/) | NASS | A component of TOTAL that examines farmland ownership changes and succession plans. |
| [Information Resources, Inc. (IRI) InfoScan](https://www.iriworldwide.com/en-us/solutions/consumer-panel/infoscan) | Circana (formerly IRI) | A commercial scanner dataset tracking retail food and consumer goods purchases. |

: List of USDA Data Assets {#tbl-usda-datasets}

## Step 2: Create a Seed Corpus {#sec-seed-corpus}

Once the data assets (@tbl-usda-datasets) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.

::: {.panel-tabset}

{{< include scopus01.qmd >}}

{{< include openalex01.qmd >}}

{{< include dimensions01.qmd >}}

:::

## Step 3: Dataset Extraction {#sec-data-extraction}

Our analysis builds a hierarchical understanding of how USDA datasets appear in Scopus[^1], OpenAlex, and Dimensions. We start at the journal level, identifying which journals publish research using USDA data to establish publication patterns across different research fields. We then examine individual publications within these journals, focusing on how researchers use and cite USDA datasets in their work. At a more granular level, we analyze the authors who work with these datasets, tracking their institutional affiliations and research networks (e.g., coauthors). Finally, we examine the institutions themselves, mapping where USDA data usage concentrates geographically and across different types of research organizations.

[^1]: The Scopus work was generated by a team provided under contract to USDA (NASS and ERS), which included NYU, Rafael Ladislau, and Elsevier.

Before conducting this analysis, we perform a series of steps to prepare the data. This includes extracting dataset mentions from publication text, disambiguating author names to identify distinct individuals, standardizing institution names, and developing methods to match records across the two databases. The following sections detail these preparatory steps.

::: {.panel-tabset}

{{< include scopus01.qmd >}}

{{< include openalex01.qmd >}}

{{< include dimensions01.qmd >}}

:::



## Step 4: Disambiguation Process {#sec-disambiguation}

::: {.panel-tabset}

{{< include scopus02.qmd >}}

{{< include openalex02.qmd >}}

{{< include dimensions02.qmd >}}

:::

### Results from Disambiguation

*This section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset*

::: {.panel-tabset}

{{< include scopus02r.qmd >}}

{{< include openalex02r.qmd >}}

{{< include dimensions02r.qmd >}}

:::



## Step 5: Comparison across Datasets {#sec-matching}

Matching methods:

1.  Rule-based matching for exact matches
2.  Probabilistic matching for handling variations
3.  Machine learning methods for complex cases

| Method | Considerations | Example | Pros | Cons |
|:---|:---|:---|:---|:---|
| Searching for dataset names within Scopus |  |  |  |  |
| Searching for dataset names within OpenAlex | “Location” field set to “journal” |  |  |  |
| Disambiguation of authors |  |  |  |  |
| Disambiguation of institutions |  |  |  |  |
| Standardization of institutions |  |  |  |  |
| Searching based on the frequency of dataset appearance in journals |  |  |  |  |
| MORE . . . |  |  |  |  |
| Filtering on keywords to determine themes |  |  |  |  |

: Summary of Methods {#tbl-methods-summary}

::: {.panel-tabset}

{{< include scopus03.qmd >}}

{{< include openalex03.qmd >}}

{{< include dimensions03.qmd >}}

:::

### Results from Database Comparison

*This section presents results after matching (which type varies – deterministic vs fuzzy)*

::: {.panel-tabset}

{{< include scopus03r.qmd >}}

{{< include openalex03r.qmd >}}

{{< include dimensions03r.qmd >}}

:::


# Recommendations

## Technical Recommendations

## Expanding Dataset Usage

This report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.

Given the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database.

# Conclusion and Next Steps

# References
