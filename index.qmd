---
title: "Methodology for Comparing Citation Database Coverage of Dataset Usage"
subtitle: "Technical Report"
author: 
  - name: "Lauren Chenarides, Ph.D."
    email: "Lauren.Chenarides@colostate.edu"
    affiliation: "Colorado State University"
  - name: "Calvin Bryan, Ph.D."
    email: "calvinrbryan@gmail.com"
    affiliation: "Colorado State University"    
  - name: "Rafael Ladislau"
    email: "rafa.ladis@gmail.com"
    affiliation: "RL Desenvolvimento de Sistemas LTDA"
  - name: "Julia Lane, Ph.D."
    email: "julia.lane@nyu.edu"
    affiliation: "New York University"
  - name: "Ming Wang"
    email: "Ming.Wang@colostate.edu"
    affiliation: "Colorado State University"
date: last-modified  # Auto-updates based on Git commits
format:
  html:
    toc: true  # Enables TOC on this page
    toc-depth: 3
bibliography: references.bib  # Link to your BibTeX file
csl: apa.csl  # Optional: Use a specific citation style (e.g., APA)
---

# Executive Summary

Federal agencies like the USDA track how their datasets are referenced in research papers and disseminate data usage statistics through platforms like [DemocratizingData.ai](https://democratizingdata.ai/tools/dashboards/) and [NASS’s 5’s Data Usage Dashboard](https://www.nass.usda.gov/Data_Visualization/5W/index.php). This report presents a methodology for comparing citation databases as potential data sources for identifying dataset mentions within research papers, using Scopus and OpenAlex as a test case. The methods described can be applied to evaluate other citation databases such as Web of Science, Crossref, Dimensions, and Microsoft Academic, to name a few.

Citation databases track academic research output. Different databases curate content (i.e., research output) in different ways - some focus on peer-reviewed journals while others include preprints and technical reports. Tracking dataset usage requires developing methods that scan publication text for dataset mentions. The accuracy of dataset tracking depends on the scope of research output we can access and analyze. Not to mention, dataset tracking requires reliable citation data from citation databases.

The two citation databases we are comparing are Elsevier’s Scopus and OpenAlex. Scopus charges for access to its citation database. It focuses on peer-reviewed literature and provides metadata about authors, institutions, and citations for academic journals. OpenAlex, an open-source platform, offers free metadata access. It covers both traditional academic publications and other research outputs like preprints and technical reports.

Our methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. We developed procedures for:

  - Deduplicating author records
  - Standardizing institution names
  - Cross-referencing publications between datasets
  - Analyzing research themes and institutional representation

Beyond platform comparison, this methodology examines inclusivity in research coverage, particularly representation of MSIs. This component helps identify potential gaps in dataset accessibility and adoption across different types of research institutions.

Our comparison of Scopus and OpenAlex found:

  - After deduplication, the number of distinct authors decreased by XX% in Scopus and XX% in OpenAlex, indicating significant duplicate entries in the raw data
  - Institutional coverage was broader in XX, with XX% more institutions represented compared to XX
  - Analysis revealed XX major themes in USDA dataset usage, with ?? and ?? being the most prominent
  - Minority-Serving Institutions (MSIs) represented only XX% of institutional users, highlighting opportunities for broader engagement

The methodology produced these reusable components:

  - Code repository for data cleaning and standardization
  - Cleaned author tables with disambiguated names and institutional affiliations
  - Standardized institution tables using IPEDS identifiers
  - Crosswalk table structure linking Scopus and OpenAlex publication records, authors, and institutions
  - Data schema documentation [Last updated: January 3, 2025]

# Terminology

Citation databases form the foundation of modern research tracking and analysis. Digital repositories, like Scopus and OpenAlex, systematically catalog scholarly publications and their references to each other [@debellis2009bibliometrics]. Citation databases differ in their approaches to curating and maintaining this information. Some focus exclusively on peer-reviewed journal articles with strict inclusion criteria, while others index a broader range of research outputs including preprints, technical reports, and conference proceedings [@martin2021google; @mongeon2016journal]. These curation approaches affect how comprehensively each database captures research impact [@visser2021large].

Understanding how these databases work requires familiarity with bibliometrics - the statistical analysis of published works and their impact [@broadus1987toward]. Bibliometric analysis examines patterns in publication, citation networks, and research influence [@hood2001literature]. The field emerged from early citation indices, which mapped relationships between papers through their references [@garfield1955citation].

For tracking USDA dataset usage, these concepts directly apply. Accurate tracking of dataset usage in scientific literature serves multiple purposes. For federal agencies like the USDA, it helps monitor the return on public data investments, find gaps in dataset use, plan future data collection, and support evidence-based policy decisions. This tracking requires reliable citation data from citation databases. Unlike standard citations, researchers often reference datasets within the text of their publications rather than citing them formally. This makes tracking dataset usage more complex.

To solve this tracking challenge, methods have been developed that scan publication text for dataset mentions [@lane2022data]. The scope and accuracy of our dataset tracking depends on what publications we can access and analyze. Because different databases curate content in different ways, it creates variation in what dataset mentions they capture and their frequency. Variations in content across sources affect our ability to accurately track dataset impact and adoption. The DemocratizingData.ai platform, for example, uses bibliometric data to monitor these dataset usage patterns, helping USDA understand how its data supports research. By comparing how different citation databases track this information, we can better understand their strengths and limitations for monitoring research impact.

# Project Background

The DemocratizingData.ai platform currently uses Scopus, a fee-based service, to track USDA dataset usage. OpenAlex, an open-source alternative, offers potential cost savings and broader access. Moving to OpenAlex would reduce operational costs while maintaining public access to USDA’s dataset usage metrics. However, the decision to transition from Scopus to OpenAlex requires systematic evaluation of their coverage and data quality.

Initial comparisons between Scopus and OpenAlex revealed unexpected patterns in coverage overlap. These findings suggested that simply replacing one database with another might affect the platform’s ability to track dataset usage accurately. This led to the development of a methodology for comparing citation databases, focusing on four key areas:

1. Journal coverage: Determining which journals each platform indexes
2. Publication tracking: Comparing how each platform captures publications within indexed journals
3. Author identification: Evaluating how each platform handles author names and affiliations
4. Institution recognition: Determining how each platform records and standardizes institutional information

Our methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. Beyond platform comparison, this methodology examines inclusivity in research coverage, particularly representation of MSIs. This component helps identify potential gaps in dataset accessibility and adoption across different types of research institutions.

The methods described in this report can be applied to other citation databases as alternatives to current data sources.

# Data Sources

## USDA Datasets *Lauren*

## Scopus *Lauren*

## OpenAlex *Lauren*

## IPEDS *Ming*

## MSI Data *Ming*


# Methodology

Our analysis builds a hierarchical understanding of how USDA datasets appear in Scopus  and OpenAlex, the two citation databases in question. We start at the journal level, identifying which journals publish research using USDA data to establish publication patterns across different research fields. We then examine individual publications within these journals, focusing on how researchers use and cite USDA datasets in their work. At a more granular level, we analyze the authors who work with these datasets, tracking their institutional affiliations and research networks (e.g., coauthors). Finally, we examine the institutions themselves, mapping where USDA data usage concentrates geographically and across different types of research organizations.

Before conducting this analysis, we perform a series of steps to prepare the data. This includes extracting dataset mentions from publication text, disambiguating author names to identify distinct individuals, standardizing institution names, and developing methods to match records across the two databases. The following sections detail these preparatory steps.

## Dataset Extraction in Scopus *Julia*



# References
