---
title: "Methodology for Comparing Citation Database Coverage of Dataset Usage"
subtitle: "Technical Report"
author: 
  - name: "Lauren Chenarides, Ph.D."
    email: "Lauren.Chenarides@colostate.edu"
    affiliation: "Colorado State University"
  - name: "Calvin Bryan, Ph.D."
    email: "calvinrbryan@gmail.com"
    affiliation: "Colorado State University"    
  - name: "Rafael Ladislau"
    email: "rafa.ladis@gmail.com"
    affiliation: "RL Desenvolvimento de Sistemas LTDA"
  - name: "Julia Lane, Ph.D."
    email: "julia.lane@nyu.edu"
    affiliation: "New York University"
  - name: "Ming Wang"
    email: "Ming.Wang@colostate.edu"
    affiliation: "Colorado State University"
date: last-modified  # Auto-updates based on Git commits
format:
  html:
    toc: true  # Enables TOC on this page
    toc-depth: 3
bibliography: references.bib  # Link to your BibTeX file
csl: apa.csl  # Optional: Use a specific citation style (e.g., APA)
---

# Executive Summary

Federal agencies like the USDA track how their datasets are referenced in research papers and disseminate data usage statistics through platforms like [DemocratizingData.ai](https://democratizingdata.ai/tools/dashboards/) and [NASS’s 5’s Data Usage Dashboard](https://www.nass.usda.gov/Data_Visualization/5W/index.php). This report presents a methodology for comparing citation databases as potential data sources for identifying dataset mentions within research papers, using Scopus and OpenAlex as a test case. The methods described can be applied to evaluate other citation databases such as Web of Science, Crossref, Dimensions, and Microsoft Academic, to name a few.

Citation databases track academic research output. Different databases curate content (i.e., research output) in different ways - some focus on peer-reviewed journals while others include preprints and technical reports. Tracking dataset usage requires developing methods that scan publication text for dataset mentions. The accuracy of dataset tracking depends on the scope of research output we can access and analyze. Not to mention, dataset tracking requires reliable citation data from citation databases.

The two citation databases we are comparing are Elsevier’s Scopus and OpenAlex. Scopus charges for access to its citation database. It focuses on peer-reviewed literature and provides metadata about authors, institutions, and citations for academic journals. OpenAlex, an open-source platform, offers free metadata access. It covers both traditional academic publications and other research outputs like preprints and technical reports.

Our methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. We developed procedures for:

-   Deduplicating author records
-   Standardizing institution names
-   Cross-referencing publications between datasets
-   Analyzing research themes and institutional representation

Beyond platform comparison, this methodology examines inclusivity in research coverage, particularly representation of MSIs. This component helps identify potential gaps in dataset accessibility and adoption across different types of research institutions.

Our comparison of Scopus and OpenAlex found:

-   After deduplication, the number of distinct authors decreased by XX% in Scopus and XX% in OpenAlex, indicating significant duplicate entries in the raw data
-   Institutional coverage was broader in XX, with XX% more institutions represented compared to XX
-   Analysis revealed XX major themes in USDA dataset usage, with ?? and ?? being the most prominent
-   Minority-Serving Institutions (MSIs) represented only XX% of institutional users, highlighting opportunities for broader engagement

The methodology produced these reusable components:

-   Code repository for data cleaning and standardization
-   Cleaned author tables with disambiguated names and institutional affiliations
-   Standardized institution tables using IPEDS identifiers
-   Crosswalk table structure linking Scopus and OpenAlex publication records, authors, and institutions
-   Data schema documentation \[Last updated: January 3, 2025\]

# Terminology

Citation databases form the foundation of modern research tracking and analysis. Digital repositories, like Scopus and OpenAlex, systematically catalog scholarly publications and their references to each other [@debellis2009bibliometrics]. Citation databases differ in their approaches to curating and maintaining this information. Some focus exclusively on peer-reviewed journal articles with strict inclusion criteria, while others index a broader range of research outputs including preprints, technical reports, and conference proceedings [@martin2021google; @mongeon2016journal]. These curation approaches affect how comprehensively each database captures research impact [@visser2021large].

Understanding how these databases work requires familiarity with bibliometrics - the statistical analysis of published works and their impact [@broadus1987toward]. Bibliometric analysis examines patterns in publication, citation networks, and research influence [@hood2001literature]. The field emerged from early citation indices, which mapped relationships between papers through their references [@garfield1955citation].

For tracking USDA dataset usage, these concepts directly apply. Accurate tracking of dataset usage in scientific literature serves multiple purposes. For federal agencies like the USDA, it helps monitor the return on public data investments, find gaps in dataset use, plan future data collection, and support evidence-based policy decisions. This tracking requires reliable citation data from citation databases. Unlike standard citations, researchers often reference datasets within the text of their publications rather than citing them formally. This makes tracking dataset usage more complex.

To solve this tracking challenge, methods have been developed that scan publication text for dataset mentions [@lane2022data]. The scope and accuracy of our dataset tracking depends on what publications we can access and analyze. Because different databases curate content in different ways, it creates variation in what dataset mentions they capture and their frequency. Variations in content across sources affect our ability to accurately track dataset impact and adoption. The DemocratizingData.ai platform, for example, uses bibliometric data to monitor these dataset usage patterns, helping USDA understand how its data supports research. By comparing how different citation databases track this information, we can better understand their strengths and limitations for monitoring research impact.

# Project Background

The DemocratizingData.ai platform currently uses Scopus, a fee-based service, to track USDA dataset usage. OpenAlex, an open-source alternative, offers potential cost savings and broader access. Moving to OpenAlex would reduce operational costs while maintaining public access to USDA’s dataset usage metrics. However, the decision to transition from Scopus to OpenAlex requires systematic evaluation of their coverage and data quality.

Initial comparisons between Scopus and OpenAlex revealed unexpected patterns in coverage overlap. These findings suggested that simply replacing one database with another might affect the platform’s ability to track dataset usage accurately. This led to the development of a methodology for comparing citation databases, focusing on four key areas:

1.  Journal coverage: Determining which journals each platform indexes
2.  Publication tracking: Comparing how each platform captures publications within indexed journals
3.  Author identification: Evaluating how each platform handles author names and affiliations
4.  Institution recognition: Determining how each platform records and standardizes institutional information

Our methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. Beyond platform comparison, this methodology examines inclusivity in research coverage, particularly representation of MSIs. This component helps identify potential gaps in dataset accessibility and adoption across different types of research institutions.

The methods described in this report can be applied to other citation databases as alternatives to current data sources.

# Data Sources

## USDA Datasets *Lauren/Julia*

### NASS Datasets

In late July 2023, an initial set of 21 reports were received containing a report name and a URL link to the Cornell database. The names of these reports were characterized be being very generic in nature e.g. “Agricultural Prices” and “Farm Labor”. This input was analysed and transformed into a list that also contained ISSN inputs and which appended generic names with the term report. An ISSN term was identified for each of the 21 reports. The report names were characterized as the main data asset (the parent record) and the ISSNs and URLs were used as aliases. There were 43 such aliases. In total there were therefore 64 (43 + 21) search terms associated with this input.

### ERS Data Assets

The list of ERS Data Assets was provided during October 2023. In the original list 2,103 records were provided, many of which also had identified aliases. The list was reviewed by professor Julia Lane and 144 records were removed on the basis that they were not amendable for use within the Machine Learning routines. That review was completed on 29 October. The principal reasons for exclusion were terms either being too generic or they were too specific (and likely to be references within a broader report). Examples of the former were records such as “Milk”, “Cotton” and “CSV Format of National Data “. Examples of the latter were records such as “Table 15—Agricultural chemical input” and “Southeast: 1982-91 1992-97”. Once these records were removed, a total of 1,959 records were left from this input.

These records represented the main data assets (the parent record). These records were then subject to review to identify their associated aliases. Unlike the NASS data assets, it was not possible to reliably find an ISSNs of DoIs for these assets. However, for each of these records, a URL link had been provided and these were used as aliases. There were this a further 1,959 terms used as an alias. In total there were therefore 3,918 (1,959 + 1,959) search terms associated with this input.

### ERS / CSU Additional Datasets

Before the run to create the seed corpus was conducted, a further list of assets was identified. This includes some terms that had been searched for in the Year 1 USDA project (e.g. Census of Agriculture and Agricultural Resource Management Survey) alongside some associated acronyms e.g. FoodAPS. It proved possible to incorporate these late additions. In total these added an additional 20 search terms, comprising 12 main (parent) records and a further 8 alias records.

Once the data assets and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.

There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.

Following the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.

Specifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.

Finally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.

## Scopus *Lauren*

## OpenAlex *Lauren*

## IPEDS *Ming*

## MSI Data *Ming*

# Dataset Extraction

Our analysis builds a hierarchical understanding of how USDA datasets appear in Scopus[^1] and OpenAlex, the two citation databases in question. We start at the journal level, identifying which journals publish research using USDA data to establish publication patterns across different research fields. We then examine individual publications within these journals, focusing on how researchers use and cite USDA datasets in their work. At a more granular level, we analyze the authors who work with these datasets, tracking their institutional affiliations and research networks (e.g., coauthors). Finally, we examine the institutions themselves, mapping where USDA data usage concentrates geographically and across different types of research organizations.

[^1]: The Scopus work was generated by a team provided under contract to USDA (NASS and ERS), which included NYU, Rafael Ladislau, and Elsevier.

Before conducting this analysis, we perform a series of steps to prepare the data. This includes extracting dataset mentions from publication text, disambiguating author names to identify distinct individuals, standardizing institution names, and developing methods to match records across the two databases. The following sections detail these preparatory steps.

## Dataset Extraction in Scopus *Julia*

### Creating a Seed Corpus

The process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).

Because some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,

-   12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.
-   71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service

The search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.

The metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:

-   SciVal Topic – 2,699 unique topics in the seed corpus
-   Journal – 2,650 unique journals in the seed corpus
-   Top Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.

It should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.

As well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.

The results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus

| Parameter | Seed Corpus Detection | Consequence / Implication for Seed Corpus |
|------------------------|------------------------|------------------------|
| SciVal Topics | Include those SciVal Topics where the article count in the Seed Corpus |  |
| was 7 or more | All articles associated with 262 SciVal Topics |  |
| Journals | Include those Journals where the article count in the Seed Corpus was 7 or more | All articles associated with 280 journals |
| Authors | Include those with US affiliation | All articles associated with the US-affiliated 769 Top Authors |

### Creating the Full Text Search Corpus

The search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.

Scopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.

In summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.

The full text records associated with the USDA search corpus is shown below:

|   | Number of Records |
|------------------------------------|------------------------------------|
| 2017-2023 Articles from Topics | 726,423 |
| 2017-2023 Articles from Journals | 1,537,851 |
| 2017-2023 Articles from Top Authors | 21,938 |
| De-duplicated Articles from Above | 2,089,728 |
| Deduplicated articles where we have full text | 1,630,958 |
| Deduplicated articles where we have full text and are licensed to search | 1,450,086 |

: Temp

### Scopus References Search Corpus

A search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.

Because of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.

### Running the Search Routines

The process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.

Identifying references to datasets within scientific publications is inherently difficult for a number of reasons including:

1.  No defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.
2.  Name disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,
3.  Conflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.
4.  Simple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.

To address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.

### Machine Learning (Kaggle) Routines (Full Text Search)

The three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.

The models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.

As well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.

With a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:

> NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service

In total, the use of flags was identified as being appropriate for 112 of the data assets.

The Kaggle routines were run in early December 2023 with the process completing on 14 December.

A summary of some of the key results from the Full Text search is provided in the Table below:

| Process Step Outputs | Number of Records |
|------------------------------------|------------------------------------|
| Number of unique Scopus publications identified by the three Kaggle algorithms | 635,831 |
| Number of unique publications identified after Fuzzy text matching to target data assets | 4,104 |
| Number of target data assets matched in the above publications | 4,392[^2] |
| Number of snippets generated | 14,377[^3] |

: Temp

[^2]: Explanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.

[^3]: Explanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.

### Scopus References Search Routine

The reference search employs an exact text string matching routine across the references of the identified Scopus records.

Because of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.

| Process Step Outputs | Number of Records |
|------------------------------------|------------------------------------|
| Number of unique Scopus publications identified in reference search | 25,588 |
| Number of those publications that were unique to the reference search (i.e. not found by Kaggle models). | 22,818 |
| Number of target data assets matched with the above publications | 34,526 |

: Temp

### Post Processing Adjustments – RUCC and QuickStat Increment

Note that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:

-   A new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.
-   A fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.

## Dataset Extraction in OpenAlex *Rafael/Lauren/Julia/Cal*

Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API

# Data Processing

## Author Disambiguation *Cal/Lauren*

## Institution Disambiguation *Cal/Ming/Lauren*

# Matching Methods

Matching methods:

1.  Rule-based matching for exact matches
2.  Probabilistic matching for handling variations
3.  Machine learning methods for complex cases

| Method | Considerations | Example | Pros | Cons |
|----|----|----|----|----|
| Searching for dataset names within Scopus |  |  |  |  |
| Searching for dataset names within OpenAlex | “Location” field set to “journal” |  |  |  |
| Disambiguation of authors |  |  |  |  |
| Disambiguation of institutions |  |  |  |  |
| Standardization of institutions |  |  |  |  |
| Searching based on the frequency of dataset appearance in journals |  |  |  |  |
| MORE . . . |  |  |  |  |
| Filtering on keywords to determine themes |  |  |  |  |

: Summary of Methods

# Results from Metadata Cleaning
*This section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset*

## Journal Identification *Cal*

## Author Disambiguation *Cal*

## Institution Disambiguation *Cal/Ming*

## Institution Standardization *Ming*

# Results from Database Comparison
*This section presents results after matching (which type varies – deterministic vs fuzzy)*

## Journal Coverage *Cal*

## Publication Coverage *Cal*

## Author Coverage *Cal*

## Institution Coverage *Ming*

## Insitutional Representation *Ming*

### Geographic Coverage *Ming*

### Institution Types *Ming* {#sec-inst-rep}

## Thematic Analysis *Cal/Lauren*

### Publication Topics *Cal/Lauren*

### Patterns over Time *Cal/Lauren*


# Recommendations

## Technical Recommendations

## Expanding Dataset Usage

This report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.

Given the small percentage of MSI’s represented in our institutional analysis (@sec-inst-rep), it is evident that user engagement is central to increasing utilization rates of the datasets, regardless of Scopus or OpenAlex platform.


## Potential Future Collaborations *Lauren/Julia*

- Inter-agency Cooperative Frameworks 
- Artificial Intelligence and Machine Learning Integration 
- National Artificial Intelligence Research Resource (NAIRR) Pilot 

# Conclusion and Next Steps *Lauren*

# References
