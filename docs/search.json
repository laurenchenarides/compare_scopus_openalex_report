[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Federal agencies like the USDA track how their datasets are referenced in research papers and disseminate data usage statistics through platforms like DemocratizingData.ai and NASS’s 5’s Data Usage Dashboard. This report presents a methodology for comparing citation databases as potential data sources for identifying dataset mentions within research papers, using Scopus and OpenAlex as a test case. The methods described can be applied to evaluate other citation databases such as Web of Science, Crossref, Dimensions, and Microsoft Academic, to name a few.\nCitation databases track academic research output. Different databases curate content (i.e., research output) in different ways - some focus on peer-reviewed journals while others include preprints and technical reports. Tracking dataset usage requires developing methods that scan publication text for dataset mentions. The accuracy of dataset tracking depends on the scope of research output we can access and analyze. Not to mention, dataset tracking requires reliable citation data from citation databases.\nThe two citation databases we are comparing are Elsevier’s Scopus and OpenAlex. Scopus charges for access to its citation database. It focuses on peer-reviewed literature and provides metadata about authors, institutions, and citations for academic journals. OpenAlex, an open-source platform, offers free metadata access. It covers both traditional academic publications and other research outputs like preprints and technical reports.\nOur methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. We developed procedures for:\n\nDeduplicating author records\nStandardizing institution names\nCross-referencing publications between datasets\nAnalyzing research themes and institutional representation\n\nBeyond platform comparison, this methodology examines inclusivity in research coverage, particularly representation of MSIs. This component helps identify potential gaps in dataset accessibility and adoption across different types of research institutions.\nOur comparison of Scopus and OpenAlex found:\n\nAfter deduplication, the number of distinct authors decreased by XX% in Scopus and XX% in OpenAlex, indicating significant duplicate entries in the raw data\nInstitutional coverage was broader in XX, with XX% more institutions represented compared to XX\nAnalysis revealed XX major themes in USDA dataset usage, with ?? and ?? being the most prominent\nMinority-Serving Institutions (MSIs) represented only XX% of institutional users, highlighting opportunities for broader engagement\n\nThe methodology produced these reusable components:\n\nCode repository for data cleaning and standardization\nCleaned author tables with disambiguated names and institutional affiliations\nStandardized institution tables using IPEDS identifiers\nCrosswalk table structure linking Scopus and OpenAlex publication records, authors, and institutions\nData schema documentation [Last updated: January 3, 2025]"
  },
  {
    "objectID": "index.html#usda-datasets-lauren",
    "href": "index.html#usda-datasets-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 USDA Datasets Lauren",
    "text": "4.1 USDA Datasets Lauren"
  },
  {
    "objectID": "index.html#scopus-lauren",
    "href": "index.html#scopus-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.2 Scopus Lauren",
    "text": "4.2 Scopus Lauren"
  },
  {
    "objectID": "index.html#openalex-lauren",
    "href": "index.html#openalex-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.3 OpenAlex Lauren",
    "text": "4.3 OpenAlex Lauren"
  },
  {
    "objectID": "index.html#ipeds-ming",
    "href": "index.html#ipeds-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.4 IPEDS Ming",
    "text": "4.4 IPEDS Ming"
  },
  {
    "objectID": "index.html#msi-data-ming",
    "href": "index.html#msi-data-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.5 MSI Data Ming",
    "text": "4.5 MSI Data Ming"
  },
  {
    "objectID": "index.html#dataset-extraction-in-scopus-julia",
    "href": "index.html#dataset-extraction-in-scopus-julia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 Dataset Extraction in Scopus Julia",
    "text": "5.1 Dataset Extraction in Scopus Julia\n\n5.1.1 Creating a Seed Corpus\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\n\n\n\nwas 7 or more\nAll articles associated with 262 SciVal Topics\n\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nAuthors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n5.1.2 Creating the Full Text Search Corpus\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\nTemp\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n5.1.3 Scopus References Search Corpus\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n5.1.4 Running the Search Routines\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n5.1.5 Machine Learning (Kaggle) Routines (Full Text Search)\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\nTemp\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3922\n\n\nNumber of snippets generated\n14,3773\n\n\n\n\n\n5.1.6 Scopus References Search Routine\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\nTemp\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n5.1.7 Post Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe Scopus work was generated by a team provided under contract to USDA (NASS and ERS), which included NYU, Rafael Ladislau, and Elsevier.↩︎\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "index.html#usda-datasets-laurenjulia",
    "href": "index.html#usda-datasets-laurenjulia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 USDA Datasets Lauren/Julia",
    "text": "4.1 USDA Datasets Lauren/Julia\n\n4.1.1 NASS Datasets\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to the Cornell database. The names of these reports were characterized be being very generic in nature e.g. “Agricultural Prices” and “Farm Labor”. This input was analysed and transformed into a list that also contained ISSN inputs and which appended generic names with the term report. An ISSN term was identified for each of the 21 reports. The report names were characterized as the main data asset (the parent record) and the ISSNs and URLs were used as aliases. There were 43 such aliases. In total there were therefore 64 (43 + 21) search terms associated with this input.\n\n\n4.1.2 ERS Data Assets\nThe list of ERS Data Assets was provided during October 2023. In the original list 2,103 records were provided, many of which also had identified aliases. The list was reviewed by professor Julia Lane and 144 records were removed on the basis that they were not amendable for use within the Machine Learning routines. That review was completed on 29 October. The principal reasons for exclusion were terms either being too generic or they were too specific (and likely to be references within a broader report). Examples of the former were records such as “Milk”, “Cotton” and “CSV Format of National Data “. Examples of the latter were records such as “Table 15—Agricultural chemical input” and “Southeast: 1982-91 1992-97”. Once these records were removed, a total of 1,959 records were left from this input.\nThese records represented the main data assets (the parent record). These records were then subject to review to identify their associated aliases. Unlike the NASS data assets, it was not possible to reliably find an ISSNs of DoIs for these assets. However, for each of these records, a URL link had been provided and these were used as aliases. There were this a further 1,959 terms used as an alias. In total there were therefore 3,918 (1,959 + 1,959) search terms associated with this input.\n\n\n4.1.3 ERS / CSU Additional Datasets\nBefore the run to create the seed corpus was conducted, a further list of assets was identified. This includes some terms that had been searched for in the Year 1 USDA project (e.g. Census of Agriculture and Agricultural Resource Management Survey) alongside some associated acronyms e.g. FoodAPS. It proved possible to incorporate these late additions. In total these added an additional 20 search terms, comprising 12 main (parent) records and a further 8 alias records.\nOnce the data assets and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later."
  },
  {
    "objectID": "index.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "href": "index.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.2 Dataset Extraction in OpenAlex Rafael/Lauren/Julia/Cal",
    "text": "5.2 Dataset Extraction in OpenAlex Rafael/Lauren/Julia/Cal\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API"
  },
  {
    "objectID": "index.html#author-disambiguation-callauren",
    "href": "index.html#author-disambiguation-callauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "6.1 Author Disambiguation Cal/Lauren",
    "text": "6.1 Author Disambiguation Cal/Lauren"
  },
  {
    "objectID": "index.html#institution-disambiguation-calminglauren",
    "href": "index.html#institution-disambiguation-calminglauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "6.2 Institution Disambiguation Cal/Ming/Lauren",
    "text": "6.2 Institution Disambiguation Cal/Ming/Lauren"
  },
  {
    "objectID": "index.html#journal-identification-cal",
    "href": "index.html#journal-identification-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "8.1 Journal Identification Cal",
    "text": "8.1 Journal Identification Cal"
  },
  {
    "objectID": "index.html#author-disambiguation-cal",
    "href": "index.html#author-disambiguation-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "8.2 Author Disambiguation Cal",
    "text": "8.2 Author Disambiguation Cal"
  },
  {
    "objectID": "index.html#institution-disambiguation-calming",
    "href": "index.html#institution-disambiguation-calming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "8.3 Institution Disambiguation Cal/Ming",
    "text": "8.3 Institution Disambiguation Cal/Ming"
  },
  {
    "objectID": "index.html#institution-standardization-ming",
    "href": "index.html#institution-standardization-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "8.4 Institution Standardization Ming",
    "text": "8.4 Institution Standardization Ming"
  },
  {
    "objectID": "index.html#journal-coverage-cal",
    "href": "index.html#journal-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.1 Journal Coverage Cal",
    "text": "9.1 Journal Coverage Cal"
  },
  {
    "objectID": "index.html#publication-coverage-cal",
    "href": "index.html#publication-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.2 Publication Coverage Cal",
    "text": "9.2 Publication Coverage Cal"
  },
  {
    "objectID": "index.html#author-coverage-cal",
    "href": "index.html#author-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.3 Author Coverage Cal",
    "text": "9.3 Author Coverage Cal"
  },
  {
    "objectID": "index.html#institution-coverage-ming",
    "href": "index.html#institution-coverage-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.4 Institution Coverage Ming",
    "text": "9.4 Institution Coverage Ming"
  },
  {
    "objectID": "index.html#insitutional-representation-ming",
    "href": "index.html#insitutional-representation-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.5 Insitutional Representation Ming",
    "text": "9.5 Insitutional Representation Ming\n\n9.5.1 Geographic Coverage Ming\n\n\n9.5.2 Institution Types Ming"
  },
  {
    "objectID": "index.html#thematic-analysis-callauren",
    "href": "index.html#thematic-analysis-callauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.6 Thematic Analysis Cal/Lauren",
    "text": "9.6 Thematic Analysis Cal/Lauren\n\n9.6.1 Publication Topics Cal/Lauren\n\n\n9.6.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "index.html#technical-recommendations",
    "href": "index.html#technical-recommendations",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "10.1 Technical Recommendations",
    "text": "10.1 Technical Recommendations"
  },
  {
    "objectID": "index.html#expanding-dataset-usage",
    "href": "index.html#expanding-dataset-usage",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "10.2 Expanding Dataset Usage",
    "text": "10.2 Expanding Dataset Usage\nThis report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.\nGiven the small percentage of MSI’s represented in our institutional analysis (Section 9.5.2), it is evident that user engagement is central to increasing utilization rates of the datasets, regardless of Scopus or OpenAlex platform."
  },
  {
    "objectID": "index.html#potential-future-collaborations-laurenjulia",
    "href": "index.html#potential-future-collaborations-laurenjulia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "10.3 Potential Future Collaborations Lauren/Julia",
    "text": "10.3 Potential Future Collaborations Lauren/Julia\n\nInter-agency Cooperative Frameworks\nArtificial Intelligence and Machine Learning Integration\nNational Artificial Intelligence Research Resource (NAIRR) Pilot"
  }
]