[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Federal agencies like the US Department of Agriculture (USDA) track how their datasets are referenced in research papers and disseminate data usage statistics through platforms like DemocratizingData.ai and NASS’s 5’s Data Usage Dashboard. This report presents a methodology for comparing citation databases as potential data sources for identifying dataset mentions within research papers, using Scopus, OpenAlex, and Dimensions as test cases. The methods described can be applied to evaluate other citation databases such as Web of Science, Crossref, and Microsoft Academic, to name a few.\nCitation databases track academic research output. Different databases curate content (i.e., research output) in different ways - some focus on peer-reviewed journals while others include preprints and technical reports. Tracking dataset usage requires developing methods that scan publication text for dataset mentions. The accuracy of dataset tracking depends on the scope of research output we can access and analyze. Not to mention, dataset tracking requires reliable citation data from citation databases.\nThe three citation databases we are comparing are Elsevier’s Scopus, OurResearch’s OpenAlex, and Digital Science’s Dimensions.ai. Scopus (Section 3.1) charges for access to its citation database. It focuses on peer-reviewed literature and provides metadata about authors, institutions, and citations for academic journals. OpenAlex (Section 3.2), an open-source platform, offers free metadata access. It covers both traditional academic publications and other research outputs like preprints and technical reports. Dimensions (Section 3.3), developed by Digital Science, offers a hybrid model that provides both free and subscription-based access to its citation database. Unlike Scopus, which primarily indexes peer-reviewed journal articles, and OpenAlex, which emphasizes open-access content, Dimensions aggregates a broad spectrum of research outputs, including journal articles, books, clinical trials, patents, datasets, and policy documents. It integrates citation data with funding information, making it a useful tool for assessing the impact of research beyond traditional academic publishing.\nOur methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. We developed procedures for:\n\nIdentifying publication coverage across citation databases\nDeduplicating author records\nStandardizing institution names\nCross-referencing publications between datasets\nAnalyzing research themes and institutional representation\n\n\nOur comparison of citation databases found:\n\nAfter deduplication, the number of distinct authors decreased by XX% in Scopus and XX% in OpenAlex, indicating significant duplicate entries in the raw data\nInstitutional coverage was broader in XX, with XX% more institutions represented compared to XX\nAnalysis revealed XX major themes in USDA dataset usage, with ?? and ?? being the most prominent\nMinority-Serving Institutions (MSIs) represented only XX% of institutional users, highlighting opportunities for broader engagement\n\nThe methodology produced these reusable components:\n\nCode repository for data cleaning and standardization\nCleaned author tables with disambiguated names and institutional affiliations\nStandardized institution tables using IPEDS identifiers\nCrosswalk table structure linking Scopus and OpenAlex publication records, authors, and institutions\nData schema documentation [Last updated: January 3, 2025]"
  },
  {
    "objectID": "index.html#usda-datasets-lauren",
    "href": "index.html#usda-datasets-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 USDA Datasets Lauren",
    "text": "4.1 USDA Datasets Lauren"
  },
  {
    "objectID": "index.html#scopus-lauren",
    "href": "index.html#scopus-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.2 Scopus Lauren",
    "text": "4.2 Scopus Lauren"
  },
  {
    "objectID": "index.html#openalex-lauren",
    "href": "index.html#openalex-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.3 OpenAlex Lauren",
    "text": "4.3 OpenAlex Lauren"
  },
  {
    "objectID": "index.html#ipeds-ming",
    "href": "index.html#ipeds-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.4 IPEDS Ming",
    "text": "4.4 IPEDS Ming"
  },
  {
    "objectID": "index.html#msi-data-ming",
    "href": "index.html#msi-data-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.5 MSI Data Ming",
    "text": "4.5 MSI Data Ming"
  },
  {
    "objectID": "index.html#dataset-extraction-in-scopus-julia",
    "href": "index.html#dataset-extraction-in-scopus-julia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 Dataset Extraction in Scopus Julia",
    "text": "5.1 Dataset Extraction in Scopus Julia\nOnce the data assets (Table 1) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n5.1.1 Creating a Seed Corpus\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 2: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n5.1.2 Creating the Full Text Search Corpus\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 3: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n5.1.3 Scopus References Search Corpus\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n5.1.4 Running the Search Routines\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n5.1.5 Machine Learning (Kaggle) Routines (Full Text Search)\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 4: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3922\n\n\nNumber of snippets generated\n14,3773\n\n\n\n\n\n\n\n\n5.1.6 Scopus References Search Routine\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 5: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n5.1.7 Post Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎\nThe Scopus work was generated by a team provided under contract to USDA (NASS and ERS), which included NYU, Rafael Ladislau, and Elsevier.↩︎\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "index.html#usda-datasets-laurenjulia",
    "href": "index.html#usda-datasets-laurenjulia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 USDA Datasets Lauren/Julia",
    "text": "4.1 USDA Datasets Lauren/Julia\n\n4.1.1 National Agricultural Statistics Service (NASS) Data Assets\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to the Cornell database. The names of these reports were characterized be being very generic in nature e.g. “Agricultural Prices” and “Farm Labor”. This input was analysed and transformed into a list that also contained ISSN inputs and which appended generic names with the term report. An ISSN term was identified for each of the 21 reports. The report names were characterized as the main data asset (the parent record) and the ISSNs and URLs were used as aliases. There were 43 such aliases. In total there were therefore 64 (43 + 21) search terms associated with this input.\n\n\n4.1.2 Economic Research Service (ERS) Data Assets\nThe list of ERS Data Assets was provided during October 2023. In the original list 2,103 records were provided, many of which also had identified aliases. The list was reviewed by professor Julia Lane and 144 records were removed on the basis that they were not amendable for use within the Machine Learning routines. That review was completed on 29 October. The principal reasons for exclusion were terms either being too generic or they were too specific (and likely to be references within a broader report). Examples of the former were records such as “Milk”, “Cotton” and “CSV Format of National Data “. Examples of the latter were records such as “Table 15—Agricultural chemical input” and “Southeast: 1982-91 1992-97”. Once these records were removed, a total of 1,959 records were left from this input.\nThese records represented the main data assets (the parent record). These records were then subject to review to identify their associated aliases. Unlike the NASS data assets, it was not possible to reliably find an ISSNs of DoIs for these assets. However, for each of these records, a URL link had been provided and these were used as aliases. There were this a further 1,959 terms used as an alias. In total there were therefore 3,918 (1,959 + 1,959) search terms associated with this input.\nBefore the run to create the seed corpus was conducted, a further list of assets was identified. This includes some terms that had been searched for in the Year 1 USDA project (e.g. Census of Agriculture and Agricultural Resource Management Survey) alongside some associated acronyms e.g. FoodAPS. It proved possible to incorporate these late additions. In total these added an additional 20 search terms, comprising 12 main (parent) records and a further 8 alias records.\nThrough consultation with USDA stakeholders and recognition that agricultural economists represent a significant user group of USDA-produced datasets, we identified 13 key datasets to feature on the dashboard. This selection emerged from a broader review of USDA data assets that included over 2,000 ERS records and 21 NASS reports, as described above. The selected datasets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. Table 1 presents these datasets, their producing agencies, and brief descriptions.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases."
  },
  {
    "objectID": "index.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "href": "index.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 Dataset Extraction in OpenAlex Rafael/Lauren/Julia/Cal",
    "text": "5.1 Dataset Extraction in OpenAlex Rafael/Lauren/Julia/Cal\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#author-disambiguation-callauren",
    "href": "index.html#author-disambiguation-callauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.2 Author Disambiguation Cal/Lauren",
    "text": "7.2 Author Disambiguation Cal/Lauren"
  },
  {
    "objectID": "index.html#institution-disambiguation-calminglauren",
    "href": "index.html#institution-disambiguation-calminglauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.3 Institution Disambiguation Cal/Ming/Lauren",
    "text": "7.3 Institution Disambiguation Cal/Ming/Lauren\n\n7.3.1 IPEDS Ming\n\n\n7.3.2 MSI Data Ming"
  },
  {
    "objectID": "index.html#journal-identification-cal",
    "href": "index.html#journal-identification-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.1 Journal Identification Cal",
    "text": "7.1 Journal Identification Cal"
  },
  {
    "objectID": "index.html#author-disambiguation-cal",
    "href": "index.html#author-disambiguation-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.2 Author Disambiguation Cal",
    "text": "7.2 Author Disambiguation Cal"
  },
  {
    "objectID": "index.html#institution-disambiguation-calming",
    "href": "index.html#institution-disambiguation-calming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.3 Institution Disambiguation Cal/Ming",
    "text": "7.3 Institution Disambiguation Cal/Ming"
  },
  {
    "objectID": "index.html#institution-standardization-ming",
    "href": "index.html#institution-standardization-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.4 Institution Standardization Ming",
    "text": "7.4 Institution Standardization Ming"
  },
  {
    "objectID": "index.html#journal-coverage-cal",
    "href": "index.html#journal-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.1 Journal Coverage Cal",
    "text": "9.1 Journal Coverage Cal"
  },
  {
    "objectID": "index.html#publication-coverage-cal",
    "href": "index.html#publication-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.2 Publication Coverage Cal",
    "text": "9.2 Publication Coverage Cal"
  },
  {
    "objectID": "index.html#author-coverage-cal",
    "href": "index.html#author-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.3 Author Coverage Cal",
    "text": "9.3 Author Coverage Cal"
  },
  {
    "objectID": "index.html#institution-coverage-ming",
    "href": "index.html#institution-coverage-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.4 Institution Coverage Ming",
    "text": "9.4 Institution Coverage Ming"
  },
  {
    "objectID": "index.html#insitutional-representation-ming",
    "href": "index.html#insitutional-representation-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.5 Insitutional Representation Ming",
    "text": "9.5 Insitutional Representation Ming\n\n9.5.1 Geographic Coverage Ming\n\n\n9.5.2 Institution Types Ming"
  },
  {
    "objectID": "index.html#thematic-analysis-callauren",
    "href": "index.html#thematic-analysis-callauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.6 Thematic Analysis Cal/Lauren",
    "text": "9.6 Thematic Analysis Cal/Lauren\n\n9.6.1 Publication Topics Cal/Lauren\n\n\n9.6.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "index.html#technical-recommendations",
    "href": "index.html#technical-recommendations",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 Technical Recommendations",
    "text": "5.1 Technical Recommendations"
  },
  {
    "objectID": "index.html#expanding-dataset-usage",
    "href": "index.html#expanding-dataset-usage",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.2 Expanding Dataset Usage",
    "text": "5.2 Expanding Dataset Usage\nThis report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.\nGiven the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database."
  },
  {
    "objectID": "index.html#potential-future-collaborations-laurenjulia",
    "href": "index.html#potential-future-collaborations-laurenjulia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "10.3 Potential Future Collaborations Lauren/Julia",
    "text": "10.3 Potential Future Collaborations Lauren/Julia\n\nInter-agency Cooperative Frameworks\nArtificial Intelligence and Machine Learning Integration\nNational Artificial Intelligence Research Resource (NAIRR) Pilot"
  },
  {
    "objectID": "appendices.html#publication.csv",
    "href": "appendices.html#publication.csv",
    "title": "Appendices",
    "section": "3.1 1. publication.csv",
    "text": "3.1 1. publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7"
  },
  {
    "objectID": "appendices.html#dataset_alias.csv",
    "href": "appendices.html#dataset_alias.csv",
    "title": "Appendices",
    "section": "3.2 2. dataset_alias.csv",
    "text": "3.2 2. dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices"
  },
  {
    "objectID": "appendices.html#dyad.csv",
    "href": "appendices.html#dyad.csv",
    "title": "Appendices",
    "section": "3.3 3. dyad.csv",
    "text": "3.3 3. dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture"
  },
  {
    "objectID": "appendices.html#model.csv",
    "href": "appendices.html#model.csv",
    "title": "Appendices",
    "section": "3.4 4. model.csv",
    "text": "3.4 4. model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch"
  },
  {
    "objectID": "appendices.html#dyad_model.csv",
    "href": "appendices.html#dyad_model.csv",
    "title": "Appendices",
    "section": "3.5 5. dyad_model.csv",
    "text": "3.5 5. dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#publication.csv-1",
    "href": "appendices.html#publication.csv-1",
    "title": "Appendices",
    "section": "10.1 1. publication.csv",
    "text": "10.1 1. publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html#dataset_alias.csv-1",
    "href": "appendices.html#dataset_alias.csv-1",
    "title": "Appendices",
    "section": "10.2 2. dataset_alias.csv",
    "text": "10.2 2. dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html#dyad.csv-1",
    "href": "appendices.html#dyad.csv-1",
    "title": "Appendices",
    "section": "10.3 3. dyad.csv",
    "text": "10.3 3. dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html#model.csv-1",
    "href": "appendices.html#model.csv-1",
    "title": "Appendices",
    "section": "10.4 4. model.csv",
    "text": "10.4 4. model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html#dyad_model.csv-1",
    "href": "appendices.html#dyad_model.csv-1",
    "title": "Appendices",
    "section": "10.5 5. dyad_model.csv",
    "text": "10.5 5. dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html",
    "href": "appendices.html",
    "title": "Appendices",
    "section": "",
    "text": "This section describes the process of construction CSV files extracted from a SQL Server database. These files contain interconnected data about publications and datasets, specifically focusing on how datasets are mentioned within publications. The main goal is to enable you to analyze the relationships between publications and datasets, particularly those identified using specific search models.\n\n\nBelow is a detailed explanation of each CSV file and how they relate to one another:\n\n\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0\n\n\n\n\n\n\n\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv.\n\n\n\n\n\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2.\n\n\n\n\n\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv.\n\n\n\n\n\nFrom dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv.\n\n\n\n\nBy following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex."
  },
  {
    "objectID": "appendices.html#contents-of-the-csv-files",
    "href": "appendices.html#contents-of-the-csv-files",
    "title": "Appendices",
    "section": "",
    "text": "Below is a detailed explanation of each CSV file and how they relate to one another:\n\n\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#how-to-extract-publications-for-a-specific-dataset",
    "href": "appendices.html#how-to-extract-publications-for-a-specific-dataset",
    "title": "Appendices",
    "section": "",
    "text": "To find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices.html#filtering-publications-by-specific-models",
    "href": "appendices.html#filtering-publications-by-specific-models",
    "title": "Appendices",
    "section": "",
    "text": "Since we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices.html#practical-example",
    "href": "appendices.html#practical-example",
    "title": "Appendices",
    "section": "",
    "text": "Objective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices.html#sample-data-illustration",
    "href": "appendices.html#sample-data-illustration",
    "title": "Appendices",
    "section": "",
    "text": "From dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices.html#conclusion",
    "href": "appendices.html#conclusion",
    "title": "Appendices",
    "section": "",
    "text": "By following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex."
  },
  {
    "objectID": "appendices.html#contents-of-the-csv-files-1",
    "href": "appendices.html#contents-of-the-csv-files-1",
    "title": "Appendices",
    "section": "2.1 Contents of the CSV Files",
    "text": "2.1 Contents of the CSV Files\nBelow is a detailed explanation of the SQL Server-related CSV files and how they relate to one another:\n\n2.1.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n2.1.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n2.1.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n2.1.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n2.1.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#how-to-extract-publications-for-a-specific-dataset-1",
    "href": "appendices.html#how-to-extract-publications-for-a-specific-dataset-1",
    "title": "Appendices",
    "section": "2.3 How to Extract Publications for a Specific Dataset",
    "text": "2.3 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices.html#filtering-publications-by-specific-models-1",
    "href": "appendices.html#filtering-publications-by-specific-models-1",
    "title": "Appendices",
    "section": "2.4 Filtering Publications by Specific Models",
    "text": "2.4 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices.html#practical-example-1",
    "href": "appendices.html#practical-example-1",
    "title": "Appendices",
    "section": "2.5 Practical Example",
    "text": "2.5 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices.html#sample-data-illustration-1",
    "href": "appendices.html#sample-data-illustration-1",
    "title": "Appendices",
    "section": "2.6 Sample Data Illustration",
    "text": "2.6 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices.html#conclusion-1",
    "href": "appendices.html#conclusion-1",
    "title": "Appendices",
    "section": "2.7 Conclusion",
    "text": "2.7 Conclusion\nBy following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex.\nKeep in mind that the OpenAlex data now may associate a single publication with multiple datasets, and due to the direct application of aliases and flag terms (with no optimizations), certain datasets like the NASS Census of Agriculture might show fewer publications than before."
  },
  {
    "objectID": "appendices.html#how-the-files-are-related",
    "href": "appendices.html#how-the-files-are-related",
    "title": "Appendices",
    "section": "2.2 How the Files are Related",
    "text": "2.2 How the Files are Related\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files. The link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nBy using the OpenAlex identifiers, you can join these files to perform comprehensive analyses. For example:\n\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id."
  },
  {
    "objectID": "scopus.html",
    "href": "scopus.html",
    "title": "scopus",
    "section": "",
    "text": "Once the data assets (?@tbl-usda-datasets) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "scopus.html#dataset-extraction-in-scopus-julia",
    "href": "scopus.html#dataset-extraction-in-scopus-julia",
    "title": "scopus",
    "section": "",
    "text": "Once the data assets (?@tbl-usda-datasets) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "scopus.html#footnotes",
    "href": "scopus.html#footnotes",
    "title": "scopus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "openalex.html",
    "href": "openalex.html",
    "title": "openalex",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "openalex.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "href": "openalex.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "title": "openalex",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#scopus",
    "href": "index.html#scopus",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.2 Scopus",
    "text": "4.2 Scopus"
  },
  {
    "objectID": "index.html#openalex",
    "href": "index.html#openalex",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.3 OpenAlex",
    "text": "4.3 OpenAlex"
  },
  {
    "objectID": "index.html#dimensions",
    "href": "index.html#dimensions",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.4 Dimensions",
    "text": "4.4 Dimensions"
  },
  {
    "objectID": "index.html#journal-identification-cal-1",
    "href": "index.html#journal-identification-cal-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.1 Journal Identification Cal",
    "text": "7.1 Journal Identification Cal"
  },
  {
    "objectID": "index.html#author-disambiguation-callauren-1",
    "href": "index.html#author-disambiguation-callauren-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.2 Author Disambiguation Cal/Lauren",
    "text": "7.2 Author Disambiguation Cal/Lauren"
  },
  {
    "objectID": "index.html#institution-disambiguation-calminglauren-1",
    "href": "index.html#institution-disambiguation-calminglauren-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.3 Institution Disambiguation Cal/Ming/Lauren",
    "text": "7.3 Institution Disambiguation Cal/Ming/Lauren"
  },
  {
    "objectID": "scopus03.html#journal-identification",
    "href": "scopus03.html#journal-identification",
    "title": "Citation Database Assessment",
    "section": "2 Journal Identification",
    "text": "2 Journal Identification"
  },
  {
    "objectID": "scopus03.html#author-disambiguation",
    "href": "scopus03.html#author-disambiguation",
    "title": "Citation Database Assessment",
    "section": "3 Author Disambiguation",
    "text": "3 Author Disambiguation"
  },
  {
    "objectID": "scopus03.html#institution-disambiguation",
    "href": "scopus03.html#institution-disambiguation",
    "title": "Citation Database Assessment",
    "section": "4 Institution Disambiguation",
    "text": "4 Institution Disambiguation\n\n4.1 IPEDS Ming\n\n\n4.2 MSI Data Ming"
  },
  {
    "objectID": "scopus01.html",
    "href": "scopus01.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "scopus01.html#dataset-extraction-in-scopus-julia",
    "href": "scopus01.html#dataset-extraction-in-scopus-julia",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Once the data assets (?@tbl-usda-datasets) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "scopus01.html#footnotes",
    "href": "scopus01.html#footnotes",
    "title": "Citation Database Assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "dimensions03.html#journal-identification",
    "href": "dimensions03.html#journal-identification",
    "title": "Citation Database Assessment",
    "section": "2 Journal Identification",
    "text": "2 Journal Identification"
  },
  {
    "objectID": "dimensions03.html#author-disambiguation",
    "href": "dimensions03.html#author-disambiguation",
    "title": "Citation Database Assessment",
    "section": "3 Author Disambiguation",
    "text": "3 Author Disambiguation"
  },
  {
    "objectID": "dimensions03.html#institution-disambiguation",
    "href": "dimensions03.html#institution-disambiguation",
    "title": "Citation Database Assessment",
    "section": "4 Institution Disambiguation",
    "text": "4 Institution Disambiguation\n\n4.1 IPEDS Ming\n\n\n4.2 MSI Data Ming"
  },
  {
    "objectID": "openalex01.html",
    "href": "openalex01.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "openalex01.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "href": "openalex01.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "openalex03.html#journal-identification",
    "href": "openalex03.html#journal-identification",
    "title": "Citation Database Assessment",
    "section": "2 Journal Identification",
    "text": "2 Journal Identification"
  },
  {
    "objectID": "openalex03.html#author-disambiguation",
    "href": "openalex03.html#author-disambiguation",
    "title": "Citation Database Assessment",
    "section": "3 Author Disambiguation",
    "text": "3 Author Disambiguation"
  },
  {
    "objectID": "openalex03.html#institution-disambiguation",
    "href": "openalex03.html#institution-disambiguation",
    "title": "Citation Database Assessment",
    "section": "4 Institution Disambiguation",
    "text": "4 Institution Disambiguation\n\n4.1 IPEDS Ming\n\n\n4.2 MSI Data Ming"
  },
  {
    "objectID": "openalex04.html#journal-coverage-cal",
    "href": "openalex04.html#journal-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "2 Journal Coverage Cal",
    "text": "2 Journal Coverage Cal"
  },
  {
    "objectID": "openalex04.html#publication-coverage-cal",
    "href": "openalex04.html#publication-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "3 Publication Coverage Cal",
    "text": "3 Publication Coverage Cal"
  },
  {
    "objectID": "openalex04.html#author-coverage-cal",
    "href": "openalex04.html#author-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "4 Author Coverage Cal",
    "text": "4 Author Coverage Cal"
  },
  {
    "objectID": "openalex04.html#institution-coverage-ming",
    "href": "openalex04.html#institution-coverage-ming",
    "title": "Citation Database Assessment",
    "section": "5 Institution Coverage Ming",
    "text": "5 Institution Coverage Ming"
  },
  {
    "objectID": "openalex04.html#insitutional-representation-ming",
    "href": "openalex04.html#insitutional-representation-ming",
    "title": "Citation Database Assessment",
    "section": "6 Insitutional Representation Ming",
    "text": "6 Insitutional Representation Ming\n\n6.1 Geographic Coverage Ming\n\n\n6.2 Institution Types Ming"
  },
  {
    "objectID": "openalex04.html#thematic-analysis-callauren",
    "href": "openalex04.html#thematic-analysis-callauren",
    "title": "Citation Database Assessment",
    "section": "7 Thematic Analysis Cal/Lauren",
    "text": "7 Thematic Analysis Cal/Lauren\n\n7.1 Publication Topics Cal/Lauren\n\n\n7.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "scopus04.html#journal-coverage-cal",
    "href": "scopus04.html#journal-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "2 Journal Coverage Cal",
    "text": "2 Journal Coverage Cal"
  },
  {
    "objectID": "scopus04.html#publication-coverage-cal",
    "href": "scopus04.html#publication-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "3 Publication Coverage Cal",
    "text": "3 Publication Coverage Cal"
  },
  {
    "objectID": "scopus04.html#author-coverage-cal",
    "href": "scopus04.html#author-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "4 Author Coverage Cal",
    "text": "4 Author Coverage Cal"
  },
  {
    "objectID": "scopus04.html#institution-coverage-ming",
    "href": "scopus04.html#institution-coverage-ming",
    "title": "Citation Database Assessment",
    "section": "5 Institution Coverage Ming",
    "text": "5 Institution Coverage Ming"
  },
  {
    "objectID": "scopus04.html#insitutional-representation-ming",
    "href": "scopus04.html#insitutional-representation-ming",
    "title": "Citation Database Assessment",
    "section": "6 Insitutional Representation Ming",
    "text": "6 Insitutional Representation Ming\n\n6.1 Geographic Coverage Ming\n\n\n6.2 Institution Types Ming"
  },
  {
    "objectID": "scopus04.html#thematic-analysis-callauren",
    "href": "scopus04.html#thematic-analysis-callauren",
    "title": "Citation Database Assessment",
    "section": "7 Thematic Analysis Cal/Lauren",
    "text": "7 Thematic Analysis Cal/Lauren\n\n7.1 Publication Topics Cal/Lauren\n\n\n7.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "dimensions04.html#journal-coverage-cal",
    "href": "dimensions04.html#journal-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "2 Journal Coverage Cal",
    "text": "2 Journal Coverage Cal"
  },
  {
    "objectID": "dimensions04.html#publication-coverage-cal",
    "href": "dimensions04.html#publication-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "3 Publication Coverage Cal",
    "text": "3 Publication Coverage Cal"
  },
  {
    "objectID": "dimensions04.html#author-coverage-cal",
    "href": "dimensions04.html#author-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "4 Author Coverage Cal",
    "text": "4 Author Coverage Cal"
  },
  {
    "objectID": "dimensions04.html#institution-coverage-ming",
    "href": "dimensions04.html#institution-coverage-ming",
    "title": "Citation Database Assessment",
    "section": "5 Institution Coverage Ming",
    "text": "5 Institution Coverage Ming"
  },
  {
    "objectID": "dimensions04.html#insitutional-representation-ming",
    "href": "dimensions04.html#insitutional-representation-ming",
    "title": "Citation Database Assessment",
    "section": "6 Insitutional Representation Ming",
    "text": "6 Insitutional Representation Ming\n\n6.1 Geographic Coverage Ming\n\n\n6.2 Institution Types Ming"
  },
  {
    "objectID": "dimensions04.html#thematic-analysis-callauren",
    "href": "dimensions04.html#thematic-analysis-callauren",
    "title": "Citation Database Assessment",
    "section": "7 Thematic Analysis Cal/Lauren",
    "text": "7 Thematic Analysis Cal/Lauren\n\n7.1 Publication Topics Cal/Lauren\n\n\n7.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "index.html#openalex-1",
    "href": "index.html#openalex-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 OpenAlex",
    "text": "5.1 OpenAlex"
  },
  {
    "objectID": "index.html#dimensions-1",
    "href": "index.html#dimensions-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.2 Dimensions",
    "text": "5.2 Dimensions\n\n\n\n:::"
  },
  {
    "objectID": "index.html#sec-scopus",
    "href": "index.html#sec-scopus",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "3.1 Scopus",
    "text": "3.1 Scopus"
  },
  {
    "objectID": "index.html#sec-openalex",
    "href": "index.html#sec-openalex",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "3.2 OpenAlex",
    "text": "3.2 OpenAlex"
  },
  {
    "objectID": "index.html#sec-dimensions",
    "href": "index.html#sec-dimensions",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "3.3 Dimensions",
    "text": "3.3 Dimensions"
  },
  {
    "objectID": "scopus01.html#scopus",
    "href": "scopus01.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "openalex01.html#openalex",
    "href": "openalex01.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#usda-data-assets",
    "href": "index.html#usda-data-assets",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 USDA Data Assets",
    "text": "5.1 USDA Data Assets\n\n5.1.1 National Agricultural Statistics Service (NASS) Data Assets\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to the Cornell database. The names of these reports were characterized be being very generic in nature e.g. “Agricultural Prices” and “Farm Labor”. This input was analysed and transformed into a list that also contained ISSN inputs and which appended generic names with the term report. An ISSN term was identified for each of the 21 reports. The report names were characterized as the main data asset (the parent record) and the ISSNs and URLs were used as aliases. There were 43 such aliases. In total there were therefore 64 (43 + 21) search terms associated with this input.\n\n\n5.1.2 Economic Research Service (ERS) Data Assets\nThe list of ERS Data Assets was provided during October 2023. In the original list 2,103 records were provided, many of which also had identified aliases. The list was reviewed by professor Julia Lane and 144 records were removed on the basis that they were not amendable for use within the Machine Learning routines. That review was completed on 29 October. The principal reasons for exclusion were terms either being too generic or they were too specific (and likely to be references within a broader report). Examples of the former were records such as “Milk”, “Cotton” and “CSV Format of National Data “. Examples of the latter were records such as “Table 15—Agricultural chemical input” and “Southeast: 1982-91 1992-97”. Once these records were removed, a total of 1,959 records were left from this input.\nThese records represented the main data assets (the parent record). These records were then subject to review to identify their associated aliases. Unlike the NASS data assets, it was not possible to reliably find an ISSNs of DoIs for these assets. However, for each of these records, a URL link had been provided and these were used as aliases. There were this a further 1,959 terms used as an alias. In total there were therefore 3,918 (1,959 + 1,959) search terms associated with this input.\nBefore the run to create the seed corpus was conducted, a further list of assets was identified. This includes some terms that had been searched for in the Year 1 USDA project (e.g. Census of Agriculture and Agricultural Resource Management Survey) alongside some associated acronyms e.g. FoodAPS. It proved possible to incorporate these late additions. In total these added an additional 20 search terms, comprising 12 main (parent) records and a further 8 alias records.\nThrough consultation with USDA stakeholders and recognition that agricultural economists represent a significant user group of USDA-produced datasets, we identified 13 key datasets to feature on the dashboard. This selection emerged from a broader review of USDA data assets that included over 2,000 ERS records and 21 NASS reports, as described above. The selected datasets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. Table 1 presents these datasets, their producing agencies, and brief descriptions.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases."
  },
  {
    "objectID": "index.html#results-from-disambiguation",
    "href": "index.html#results-from-disambiguation",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "8.1 Results from Disambiguation",
    "text": "8.1 Results from Disambiguation\nThis section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset\n\nMetadata ProcessingMetadata ProcessingMetadata Processing\n\n\n\n8.1.1 Disambiguating Journals\n\n\n8.1.2 Disambiguating Authors\n\n\n8.1.3 Disambiguating Institutions\n\n\n\n\n8.1.4 Disambiguating Journals\n\n\n8.1.5 Disambiguating Authors\n\n\n8.1.6 Disambiguating Institutions\n\n\n\n\n8.1.7 Disambiguating Journals\n\n\n8.1.8 Disambiguating Authors\n\n\n8.1.9 Disambiguating Institutions"
  },
  {
    "objectID": "index.html#results-from-database-comparison",
    "href": "index.html#results-from-database-comparison",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.1 Results from Database Comparison",
    "text": "9.1 Results from Database Comparison\nThis section presents results after matching (which type varies – deterministic vs fuzzy)\n\nResults from Database ComparisonJournal Coverage CalPublication Coverage CalAuthor Coverage CalInstitution Coverage MingInsitutional Representation MingThematic Analysis Cal/LaurenResults from Database ComparisonJournal Coverage CalPublication Coverage CalAuthor Coverage CalInstitution Coverage MingInsitutional Representation MingThematic Analysis Cal/LaurenResults from Database ComparisonJournal Coverage CalPublication Coverage CalAuthor Coverage CalInstitution Coverage MingInsitutional Representation MingThematic Analysis Cal/Lauren\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.1 Geographic Coverage Ming\n\n\n9.1.2 Institution Types Ming\n\n\n\n\n9.1.3 Publication Topics Cal/Lauren\n\n\n9.1.4 Patterns over Time Cal/Lauren\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.5 Geographic Coverage Ming\n\n\n9.1.6 Institution Types Ming\n\n\n\n\n9.1.7 Publication Topics Cal/Lauren\n\n\n9.1.8 Patterns over Time Cal/Lauren\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.9 Geographic Coverage Ming\n\n\n9.1.10 Institution Types Ming\n\n\n\n\n9.1.11 Publication Topics Cal/Lauren\n\n\n9.1.12 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nThe project workflow outlines the steps involved to evaluate how different citation databases track USDA dataset mentions in research papers. In searching for dataset mentions, the goal is to identify a set of publications that can be compared across the citation database test cases.\nThe process of deriving the list of publications consists of five steps:\n1. Define Data Assets and Aliases (Section 4.2)\n\nIdentify USDA datasets that will be searched for and tracked.\nCollect official dataset names along with common abbreviations, acronyms, and alternative references used.\n\nResult: A structured list of dataset names and aliases to be used for search strategies.\n2. Create a Seed Corpus (Section 4.3)\n\nCollect an initial set of publications.\nAnalyze how datasets are referenced.\nDetermine how well the dataset name variations (from Step 1) are retreived from the publications.\nAdjust searches to improve accuracy.\n\nResult: A set of seed publications to inform dataset extraction procedure.\n3. Dataset Extraction (Section 4.4)\n\nExtract dataset mentions using various search strategies:\n\nMachine Learning Models: Apply Kaggle competition models trained to detect dataset mentions.\nFull-Text (String) Search: Scan entire articles for relevant dataset names.\nReference Search: Identify dataset citations within publication references.\n\n\nResult: Different extraction methods yield different sets of publication results.\n4. Disambiguation Process (Section 4.5)\n\nPre-process and clean publication metadata for each citation database.\nStandardize journal, institution, and author names.\nDeduplicate records.\n\nResult: Cleaned publication metadata, removed of duplicates, inconsistencies, and missing information.\n5. Comparison across Databases (Section 4.6)\n\nCompare dataset coverage across Scopus, OpenAlex, and Dimensions.\nApply fuzzy matching techniques to identify overlapping and unique dataset mentions.\nAnalyze differences in journal coverage, citation patterns, and author affiliations.\n\nResult: In generating these statistics, we can evaluate the impact of switching databases on dataset tracking accuracy."
  },
  {
    "objectID": "index.html#sec-data",
    "href": "index.html#sec-data",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.2 Step 1: Define Data Assets and Aliases",
    "text": "4.2 Step 1: Define Data Assets and Aliases\nThe data assets featured here consist of those collected by the USDA, primarily from the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). These datasets are widely used in agricultural economics and food systems research. The goal of this step is to compile a structured list of dataset names and their commonly used variations.\n\n4.2.1 National Agricultural Statistics Service (NASS) Data Assets\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to the Cornell database. The names of these reports were characterized be being very generic in nature e.g. “Agricultural Prices” and “Farm Labor”. This input was analysed and transformed into a list that also contained ISSN inputs and which appended generic names with the term report. An ISSN term was identified for each of the 21 reports. The report names were characterized as the main data asset (the parent record) and the ISSNs and URLs were used as aliases. There were 43 such aliases. In total there were therefore 64 (43 + 21) search terms associated with this input.\n\n\n4.2.2 Economic Research Service (ERS) Data Assets\nThe list of ERS Data Assets was provided during October 2023. In the original list 2,103 records were provided, many of which also had identified aliases. The list was reviewed by professor Julia Lane and 144 records were removed on the basis that they were not amendable for use within the Machine Learning routines. That review was completed on 29 October. The principal reasons for exclusion were terms either being too generic or they were too specific (and likely to be references within a broader report). Examples of the former were records such as “Milk”, “Cotton” and “CSV Format of National Data “. Examples of the latter were records such as “Table 15—Agricultural chemical input” and “Southeast: 1982-91 1992-97”. Once these records were removed, a total of 1,959 records were left from this input.\nThese records represented the main data assets (the parent record). These records were then subject to review to identify their associated aliases. Unlike the NASS data assets, it was not possible to reliably find an ISSNs of DoIs for these assets. However, for each of these records, a URL link had been provided and these were used as aliases. There were this a further 1,959 terms used as an alias. In total there were therefore 3,918 (1,959 + 1,959) search terms associated with this input.\nBefore the run to create the seed corpus was conducted, a further list of assets was identified. This includes some terms that had been searched for in the Year 1 USDA project (e.g. Census of Agriculture and Agricultural Resource Management Survey) alongside some associated acronyms e.g. FoodAPS. It proved possible to incorporate these late additions. In total these added an additional 20 search terms, comprising 12 main (parent) records and a further 8 alias records.\nThrough consultation with USDA stakeholders and recognition that agricultural economists represent a significant user group of USDA-produced datasets, we identified 13 key datasets to feature on the dashboard. This selection emerged from a broader review of USDA data assets that included over 2,000 ERS records and 21 NASS reports, as described above. The selected datasets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. Table 1 presents these datasets, their producing agencies, and brief descriptions.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases."
  },
  {
    "objectID": "index.html#sec-seed-corpus",
    "href": "index.html#sec-seed-corpus",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.3 Step 2: Create a Seed Corpus",
    "text": "4.3 Step 2: Create a Seed Corpus\nOnce the data assets (Table 1) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\n\nScopusOpenAlexDimensions\n\n\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n4.3.1 Creating a Seed Corpus\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 2: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n4.3.2 Creating the Full Text Search Corpus\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 3: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n4.3.3 Scopus References Search Corpus\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n4.3.4 Running the Search Routines\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n4.3.5 Machine Learning (Kaggle) Routines (Full Text Search)\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 4: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n4.3.6 Scopus References Search Routine\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 5: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n4.3.7 Post Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\n\n\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#sec-data-extraction",
    "href": "index.html#sec-data-extraction",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.4 Step 3: Dataset Extraction",
    "text": "4.4 Step 3: Dataset Extraction\nOur analysis builds a hierarchical understanding of how USDA datasets appear in Scopus3, OpenAlex, and Dimensions. We start at the journal level, identifying which journals publish research using USDA data to establish publication patterns across different research fields. We then examine individual publications within these journals, focusing on how researchers use and cite USDA datasets in their work. At a more granular level, we analyze the authors who work with these datasets, tracking their institutional affiliations and research networks (e.g., coauthors). Finally, we examine the institutions themselves, mapping where USDA data usage concentrates geographically and across different types of research organizations.\nBefore conducting this analysis, we perform a series of steps to prepare the data. This includes extracting dataset mentions from publication text, disambiguating author names to identify distinct individuals, standardizing institution names, and developing methods to match records across the two databases. The following sections detail these preparatory steps.\n\nScopusOpenAlexDimensions\n\n\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n4.4.1 Creating a Seed Corpus\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 6: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n4.4.2 Creating the Full Text Search Corpus\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 7: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n4.4.3 Scopus References Search Corpus\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n4.4.4 Running the Search Routines\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n4.4.5 Machine Learning (Kaggle) Routines (Full Text Search)\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 8: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3924\n\n\nNumber of snippets generated\n14,3775\n\n\n\n\n\n\n\n\n4.4.6 Scopus References Search Routine\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 9: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n4.4.7 Post Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\n\n\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#sec-disambiguation",
    "href": "index.html#sec-disambiguation",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.5 Step 4: Disambiguation Process",
    "text": "4.5 Step 4: Disambiguation Process\n\nMetadata ProcessingMetadata ProcessingMetadata Processing\n\n\n\n4.5.1 Disambiguating Journals\n\n\n4.5.2 Disambiguating Authors\n\n\n4.5.3 Disambiguating Institutions\n\n\n\n\n4.5.4 Disambiguating Journals\n\n\n4.5.5 Disambiguating Authors\n\n\n4.5.6 Disambiguating Institutions\n\n\n\n\n4.5.7 Disambiguating Journals\n\n\n4.5.8 Disambiguating Authors\n\n\n4.5.9 Disambiguating Institutions\n\n\n\n\n\n4.5.10 Results from Disambiguation\nThis section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset\n\nMetadata ProcessingMetadata ProcessingMetadata Processing\n\n\n\n4.5.11 Disambiguating Journals\n\n\n4.5.12 Disambiguating Authors\n\n\n4.5.13 Disambiguating Institutions\n\n\n\n\n4.5.14 Disambiguating Journals\n\n\n4.5.15 Disambiguating Authors\n\n\n4.5.16 Disambiguating Institutions\n\n\n\n\n4.5.17 Disambiguating Journals\n\n\n4.5.18 Disambiguating Authors\n\n\n4.5.19 Disambiguating Institutions"
  },
  {
    "objectID": "index.html#sec-matching",
    "href": "index.html#sec-matching",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.6 Step 5: Comparison across Datasets",
    "text": "4.6 Step 5: Comparison across Datasets\nMatching methods:\n\nRule-based matching for exact matches\nProbabilistic matching for handling variations\nMachine learning methods for complex cases\n\n\n\n\nTable 10: Summary of Methods\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nConsiderations\nExample\nPros\nCons\n\n\n\n\nSearching for dataset names within Scopus\n\n\n\n\n\n\nSearching for dataset names within OpenAlex\n“Location” field set to “journal”\n\n\n\n\n\nDisambiguation of authors\n\n\n\n\n\n\nDisambiguation of institutions\n\n\n\n\n\n\nStandardization of institutions\n\n\n\n\n\n\nSearching based on the frequency of dataset appearance in journals\n\n\n\n\n\n\nMORE . . .\n\n\n\n\n\n\nFiltering on keywords to determine themes\n\n\n\n\n\n\n\n\n\n\n\nScopusOpenAlexDimensions\n\n\n\n4.6.1 Journal Identification\n\n\n4.6.2 Author Disambiguation\n\n\n4.6.3 Institution Disambiguation\n\n4.6.3.1 IPEDS Ming\n\n\n4.6.3.2 MSI Data Ming\n\n\n\n\n\n4.6.4 Journal Identification\n\n\n4.6.5 Author Disambiguation\n\n\n4.6.6 Institution Disambiguation\n\n4.6.6.1 IPEDS Ming\n\n\n4.6.6.2 MSI Data *Ming**\n\n\n\n\n\n4.6.7 Journal Identification\n\n\n4.6.8 Author Disambiguation\n\n\n4.6.9 Institution Disambiguation\n\n4.6.9.1 IPEDS Ming\n\n\n4.6.9.2 MSI Data Ming\n\n\n\n\n\n\n4.6.10 Results from Database Comparison\nThis section presents results after matching (which type varies – deterministic vs fuzzy)\n\nScopusOpenAlexDimensions\n\n\n\n4.6.11 Journal Coverage Cal\n\n\n4.6.12 Publication Coverage Cal\n\n\n4.6.13 Author Coverage Cal\n\n\n4.6.14 Institution Coverage Ming\n\n\n4.6.15 Insitutional Representation Ming\n\n4.6.15.1 Geographic Coverage Ming\n\n\n4.6.15.2 Institution Types Ming\n\n\n\n4.6.16 Thematic Analysis Cal/Lauren\n\n4.6.16.1 Publication Topics Cal/Lauren\n\n\n4.6.16.2 Patterns over Time Cal/Lauren\n\n\n\n\n\n4.6.17 Journal Coverage Cal\n\n\n4.6.18 Publication Coverage Cal\n\n\n4.6.19 Author Coverage Cal\n\n\n4.6.20 Institution Coverage Ming\n\n\n4.6.21 Insitutional Representation Ming\n\n4.6.21.1 Geographic Coverage Ming\n\n\n4.6.21.2 Institution Types Ming\n\n\n\n4.6.22 Thematic Analysis Cal/Lauren\n\n4.6.22.1 Publication Topics Cal/Lauren\n\n\n4.6.22.2 Patterns over Time Cal/Lauren\n\n\n\n\n\n4.6.23 Journal Coverage Cal\n\n\n4.6.24 Publication Coverage Cal\n\n\n4.6.25 Author Coverage Cal\n\n\n4.6.26 Institution Coverage Ming\n\n\n4.6.27 Insitutional Representation Ming\n\n4.6.27.1 Geographic Coverage Ming\n\n\n4.6.27.2 Institution Types Ming\n\n\n\n4.6.28 Thematic Analysis Cal/Lauren\n\n4.6.28.1 Publication Topics Cal/Lauren\n\n\n4.6.28.2 Patterns over Time Cal/Lauren"
  }
]