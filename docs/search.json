[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The DemocratizingData.ai platform currently uses Scopus, a fee-based service, to track USDA dataset usage. OpenAlex, an open-source alternative, offers potential cost savings and broader access. Moving to OpenAlex would reduce operational costs while maintaining public access to USDA’s dataset usage metrics. However, the decision to transition from Scopus to OpenAlex requires systematic evaluation of their coverage and data quality.\nInitial comparisons between Scopus and OpenAlex revealed unexpected patterns in coverage overlap. These findings suggested that simply replacing one database with another might affect the platform’s ability to track dataset usage accurately.\n\n\nThe objective of this project is to determine the coverage of publications across citation databases. This information will be used to decide the viability of replacing Scopus with a less costly solution.\n\n\n\n\nEvaluate differences in publication coverage across citation databases. Compare how well Scopus, OpenAlex, and Dimensions track dataset usage in research publications and assess variations in publication inclusion.\nAssess journal coverage. Determine which journals each platform indexes and analyze how these differences impact dataset visibility.\nAnalyze publication-level discrepancies. Compare how each platform captures research publications within indexed journals and identify potential gaps in dataset tracking.\nExamine author and institutional representation. Investigate how each platform attributes authorship and institutional affiliations, with a focus on variations by research institution type (e.g., Minority-Serving Institutions).\nDevelop a reproducible methodology for database comparison. Establish a systematic approach for evaluating other citation databases beyond Scopus, OpenAlex, and Dimensions.\n\nThese aims guide the development of a methodology for comparing citation databases, focusing on four areas:\n\nJournal coverage: Determining which journals each platform indexes\nPublication tracking: Comparing how each platform captures publications within indexed journals\nAuthor identification: Evaluating how each platform handles author names and affiliations\nInstitution recognition: Determining how each platform records and standardizes institutional information\n\nThe scope of work includes comparing publication coverage across Scopus, OpenAlex, and Dimensions. This inclusion provides a comprehensive assessment of citation databases, particularly in evaluating dataset coverage across both proprietary and open-access platforms. For more information on each citation database, refer to this Appendix.\nOur methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. Beyond platform comparison, this methodology examines variations in dataset usage across different types of research institutions.\nThe methods described in this report can be applied to other citation databases as alternatives to current data sources.",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#usda-datasets-lauren",
    "href": "index.html#usda-datasets-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 USDA Datasets Lauren",
    "text": "4.1 USDA Datasets Lauren"
  },
  {
    "objectID": "index.html#scopus-lauren",
    "href": "index.html#scopus-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.2 Scopus Lauren",
    "text": "4.2 Scopus Lauren"
  },
  {
    "objectID": "index.html#openalex-lauren",
    "href": "index.html#openalex-lauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.3 OpenAlex Lauren",
    "text": "4.3 OpenAlex Lauren"
  },
  {
    "objectID": "index.html#ipeds-ming",
    "href": "index.html#ipeds-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.4 IPEDS Ming",
    "text": "4.4 IPEDS Ming"
  },
  {
    "objectID": "index.html#msi-data-ming",
    "href": "index.html#msi-data-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.5 MSI Data Ming",
    "text": "4.5 MSI Data Ming"
  },
  {
    "objectID": "index.html#dataset-extraction-in-scopus-julia",
    "href": "index.html#dataset-extraction-in-scopus-julia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 Dataset Extraction in Scopus Julia",
    "text": "5.1 Dataset Extraction in Scopus Julia\nOnce the data assets (Table 1) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n5.1.1 Creating a Seed Corpus\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 2: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n5.1.2 Creating the Full Text Search Corpus\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 3: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n5.1.3 Scopus References Search Corpus\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n5.1.4 Running the Search Routines\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n5.1.5 Machine Learning (Kaggle) Routines (Full Text Search)\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 4: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3922\n\n\nNumber of snippets generated\n14,3773\n\n\n\n\n\n\n\n\n5.1.6 Scopus References Search Routine\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 5: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n5.1.7 Post Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#usda-datasets-laurenjulia",
    "href": "index.html#usda-datasets-laurenjulia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 USDA Datasets Lauren/Julia",
    "text": "4.1 USDA Datasets Lauren/Julia\n\n4.1.1 National Agricultural Statistics Service (NASS) Data Assets\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to the Cornell database. The names of these reports were characterized be being very generic in nature e.g. “Agricultural Prices” and “Farm Labor”. This input was analysed and transformed into a list that also contained ISSN inputs and which appended generic names with the term report. An ISSN term was identified for each of the 21 reports. The report names were characterized as the main data asset (the parent record) and the ISSNs and URLs were used as aliases. There were 43 such aliases. In total there were therefore 64 (43 + 21) search terms associated with this input.\n\n\n4.1.2 Economic Research Service (ERS) Data Assets\nThe list of ERS Data Assets was provided during October 2023. In the original list 2,103 records were provided, many of which also had identified aliases. The list was reviewed by professor Julia Lane and 144 records were removed on the basis that they were not amendable for use within the Machine Learning routines. That review was completed on 29 October. The principal reasons for exclusion were terms either being too generic or they were too specific (and likely to be references within a broader report). Examples of the former were records such as “Milk”, “Cotton” and “CSV Format of National Data “. Examples of the latter were records such as “Table 15—Agricultural chemical input” and “Southeast: 1982-91 1992-97”. Once these records were removed, a total of 1,959 records were left from this input.\nThese records represented the main data assets (the parent record). These records were then subject to review to identify their associated aliases. Unlike the NASS data assets, it was not possible to reliably find an ISSNs of DoIs for these assets. However, for each of these records, a URL link had been provided and these were used as aliases. There were this a further 1,959 terms used as an alias. In total there were therefore 3,918 (1,959 + 1,959) search terms associated with this input.\nBefore the run to create the seed corpus was conducted, a further list of assets was identified. This includes some terms that had been searched for in the Year 1 USDA project (e.g. Census of Agriculture and Agricultural Resource Management Survey) alongside some associated acronyms e.g. FoodAPS. It proved possible to incorporate these late additions. In total these added an additional 20 search terms, comprising 12 main (parent) records and a further 8 alias records.\nThrough consultation with USDA stakeholders and recognition that agricultural economists represent a significant user group of USDA-produced datasets, we identified 13 key datasets to feature on the dashboard. This selection emerged from a broader review of USDA data assets that included over 2,000 ERS records and 21 NASS reports, as described above. The selected datasets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. Table 1 presents these datasets, their producing agencies, and brief descriptions.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases."
  },
  {
    "objectID": "index.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "href": "index.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 Dataset Extraction in OpenAlex Rafael/Lauren/Julia/Cal",
    "text": "5.1 Dataset Extraction in OpenAlex Rafael/Lauren/Julia/Cal\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#author-disambiguation-callauren",
    "href": "index.html#author-disambiguation-callauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.2 Author Disambiguation Cal/Lauren",
    "text": "7.2 Author Disambiguation Cal/Lauren"
  },
  {
    "objectID": "index.html#institution-disambiguation-calminglauren",
    "href": "index.html#institution-disambiguation-calminglauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.3 Institution Disambiguation Cal/Ming/Lauren",
    "text": "7.3 Institution Disambiguation Cal/Ming/Lauren\n\n7.3.1 IPEDS Ming\n\n\n7.3.2 MSI Data Ming"
  },
  {
    "objectID": "index.html#journal-identification-cal",
    "href": "index.html#journal-identification-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.1 Journal Identification Cal",
    "text": "7.1 Journal Identification Cal"
  },
  {
    "objectID": "index.html#author-disambiguation-cal",
    "href": "index.html#author-disambiguation-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.2 Author Disambiguation Cal",
    "text": "7.2 Author Disambiguation Cal"
  },
  {
    "objectID": "index.html#institution-disambiguation-calming",
    "href": "index.html#institution-disambiguation-calming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.3 Institution Disambiguation Cal/Ming",
    "text": "7.3 Institution Disambiguation Cal/Ming"
  },
  {
    "objectID": "index.html#institution-standardization-ming",
    "href": "index.html#institution-standardization-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.4 Institution Standardization Ming",
    "text": "7.4 Institution Standardization Ming"
  },
  {
    "objectID": "index.html#journal-coverage-cal",
    "href": "index.html#journal-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.1 Journal Coverage Cal",
    "text": "9.1 Journal Coverage Cal"
  },
  {
    "objectID": "index.html#publication-coverage-cal",
    "href": "index.html#publication-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.2 Publication Coverage Cal",
    "text": "9.2 Publication Coverage Cal"
  },
  {
    "objectID": "index.html#author-coverage-cal",
    "href": "index.html#author-coverage-cal",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.3 Author Coverage Cal",
    "text": "9.3 Author Coverage Cal"
  },
  {
    "objectID": "index.html#institution-coverage-ming",
    "href": "index.html#institution-coverage-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.4 Institution Coverage Ming",
    "text": "9.4 Institution Coverage Ming"
  },
  {
    "objectID": "index.html#insitutional-representation-ming",
    "href": "index.html#insitutional-representation-ming",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.5 Insitutional Representation Ming",
    "text": "9.5 Insitutional Representation Ming\n\n9.5.1 Geographic Coverage Ming\n\n\n9.5.2 Institution Types Ming"
  },
  {
    "objectID": "index.html#thematic-analysis-callauren",
    "href": "index.html#thematic-analysis-callauren",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.6 Thematic Analysis Cal/Lauren",
    "text": "9.6 Thematic Analysis Cal/Lauren\n\n9.6.1 Publication Topics Cal/Lauren\n\n\n9.6.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "index.html#technical-recommendations",
    "href": "index.html#technical-recommendations",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "3.1 Technical Recommendations",
    "text": "3.1 Technical Recommendations",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#expanding-dataset-usage",
    "href": "index.html#expanding-dataset-usage",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "3.2 Expanding Dataset Usage",
    "text": "3.2 Expanding Dataset Usage\nThis report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.\nGiven the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database.",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#potential-future-collaborations-laurenjulia",
    "href": "index.html#potential-future-collaborations-laurenjulia",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "10.3 Potential Future Collaborations Lauren/Julia",
    "text": "10.3 Potential Future Collaborations Lauren/Julia\n\nInter-agency Cooperative Frameworks\nArtificial Intelligence and Machine Learning Integration\nNational Artificial Intelligence Research Resource (NAIRR) Pilot"
  },
  {
    "objectID": "appendices.html#publication.csv",
    "href": "appendices.html#publication.csv",
    "title": "Appendices",
    "section": "3.1 1. publication.csv",
    "text": "3.1 1. publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7"
  },
  {
    "objectID": "appendices.html#dataset_alias.csv",
    "href": "appendices.html#dataset_alias.csv",
    "title": "Appendices",
    "section": "3.2 2. dataset_alias.csv",
    "text": "3.2 2. dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices"
  },
  {
    "objectID": "appendices.html#dyad.csv",
    "href": "appendices.html#dyad.csv",
    "title": "Appendices",
    "section": "3.3 3. dyad.csv",
    "text": "3.3 3. dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture"
  },
  {
    "objectID": "appendices.html#model.csv",
    "href": "appendices.html#model.csv",
    "title": "Appendices",
    "section": "3.4 4. model.csv",
    "text": "3.4 4. model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch"
  },
  {
    "objectID": "appendices.html#dyad_model.csv",
    "href": "appendices.html#dyad_model.csv",
    "title": "Appendices",
    "section": "3.5 5. dyad_model.csv",
    "text": "3.5 5. dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#publication.csv-1",
    "href": "appendices.html#publication.csv-1",
    "title": "Appendices",
    "section": "10.1 1. publication.csv",
    "text": "10.1 1. publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html#dataset_alias.csv-1",
    "href": "appendices.html#dataset_alias.csv-1",
    "title": "Appendices",
    "section": "10.2 2. dataset_alias.csv",
    "text": "10.2 2. dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html#dyad.csv-1",
    "href": "appendices.html#dyad.csv-1",
    "title": "Appendices",
    "section": "10.3 3. dyad.csv",
    "text": "10.3 3. dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html#model.csv-1",
    "href": "appendices.html#model.csv-1",
    "title": "Appendices",
    "section": "10.4 4. model.csv",
    "text": "10.4 4. model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html#dyad_model.csv-1",
    "href": "appendices.html#dyad_model.csv-1",
    "title": "Appendices",
    "section": "10.5 5. dyad_model.csv",
    "text": "10.5 5. dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0\n\n\n\n~~~"
  },
  {
    "objectID": "appendices.html",
    "href": "appendices.html",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "",
    "text": "Overview of Scopus Reference FilesOverview of OpenAlex Reference FilesOverview of Dimensions Reference Files\n\n\nThis section describes the process of construction CSV files extracted from a SQL Server database. These files contain interconnected data about publications and datasets, specifically focusing on how datasets are mentioned within publications. The main goal is to enable you to analyze the relationships between publications and datasets, particularly those identified using specific search models.\nBelow is a detailed explanation of each CSV file and how they relate to one another:\n\n\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0\n\n\n\n\n\n\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv.\n\n\n\n\n\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2.\n\n\n\n\n\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv.\n\n\n\n\n\nFrom dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv.\n\n\n\n\nBy following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex.\n\n\n\nThis section describes the process of construction CSV files extracted from a SQL Server database. These files contain interconnected data about publications and datasets, specifically focusing on how datasets are mentioned within publications. The main goal is to enable you to analyze the relationships between publications and datasets, particularly those identified using specific search models.\nUnlike the initial dataset from Scopus, the OpenAlex publications can be associated with multiple datasets. Two new files are introduced:\n\ndataset.csv: Lists all datasets identified in OpenAlex. This contains details of all USDA datasets.\npublication_dataset_links.csv: Connects publications with one or more datasets in the OpenAlex data. Indicates which publications are associated with which datasets.\n\nThe search in OpenAlex was performed using the same aliases and flag terms applied in the Scopus data, without any optimization. As a consequence, the “NASS Census of Agriculture” dataset currently shows fewer publications than in the previous sample due to this direct, unoptimized search approach.\nBelow is a detailed explanation of the SQL Server-related CSV files and how they relate to one another:\n\n\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0\n\n\n\n\n\n\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files. The link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nBy using the OpenAlex identifiers, you can join these files to perform comprehensive analyses. For example:\n\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id.\n\n\n\n\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv.\n\n\n\n\n\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2.\n\n\n\n\n\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv.\n\n\n\n\n\nFrom dataset_alias.csv:\n\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv.",
    "crumbs": [
      "Appendices"
    ]
  },
  {
    "objectID": "appendices.html#contents-of-the-csv-files",
    "href": "appendices.html#contents-of-the-csv-files",
    "title": "Appendices",
    "section": "1 Contents of the CSV Files",
    "text": "1 Contents of the CSV Files\nBelow is a detailed explanation of each CSV file and how they relate to one another:\n\n1.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n1.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n1.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n1.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n1.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#how-to-extract-publications-for-a-specific-dataset",
    "href": "appendices.html#how-to-extract-publications-for-a-specific-dataset",
    "title": "Appendices",
    "section": "2 How to Extract Publications for a Specific Dataset",
    "text": "2 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices.html#filtering-publications-by-specific-models",
    "href": "appendices.html#filtering-publications-by-specific-models",
    "title": "Appendices",
    "section": "3 Filtering Publications by Specific Models",
    "text": "3 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices.html#practical-example",
    "href": "appendices.html#practical-example",
    "title": "Appendices",
    "section": "4 Practical Example",
    "text": "4 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices.html#sample-data-illustration",
    "href": "appendices.html#sample-data-illustration",
    "title": "Appendices",
    "section": "5 Sample Data Illustration",
    "text": "5 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices.html#conclusion",
    "href": "appendices.html#conclusion",
    "title": "Appendices",
    "section": "6 Conclusion",
    "text": "6 Conclusion\nBy following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex."
  },
  {
    "objectID": "appendices.html#contents-of-the-csv-files-1",
    "href": "appendices.html#contents-of-the-csv-files-1",
    "title": "Appendices",
    "section": "7 Contents of the CSV Files",
    "text": "7 Contents of the CSV Files\nBelow is a detailed explanation of the SQL Server-related CSV files and how they relate to one another:\n\n7.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n7.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n7.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n7.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n7.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#how-to-extract-publications-for-a-specific-dataset-1",
    "href": "appendices.html#how-to-extract-publications-for-a-specific-dataset-1",
    "title": "Appendices",
    "section": "9 How to Extract Publications for a Specific Dataset",
    "text": "9 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices.html#filtering-publications-by-specific-models-1",
    "href": "appendices.html#filtering-publications-by-specific-models-1",
    "title": "Appendices",
    "section": "10 Filtering Publications by Specific Models",
    "text": "10 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices.html#practical-example-1",
    "href": "appendices.html#practical-example-1",
    "title": "Appendices",
    "section": "11 Practical Example",
    "text": "11 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices.html#sample-data-illustration-1",
    "href": "appendices.html#sample-data-illustration-1",
    "title": "Appendices",
    "section": "12 Sample Data Illustration",
    "text": "12 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices.html#conclusion-1",
    "href": "appendices.html#conclusion-1",
    "title": "Appendices",
    "section": "18 Conclusion",
    "text": "18 Conclusion\nBy following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex."
  },
  {
    "objectID": "appendices.html#how-the-files-are-related",
    "href": "appendices.html#how-the-files-are-related",
    "title": "Appendices",
    "section": "8 How the Files are Related",
    "text": "8 How the Files are Related\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files. The link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nBy using the OpenAlex identifiers, you can join these files to perform comprehensive analyses. For example:\n\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id."
  },
  {
    "objectID": "scopus.html",
    "href": "scopus.html",
    "title": "scopus",
    "section": "",
    "text": "Once the data assets (?@tbl-usda-datasets) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "scopus.html#dataset-extraction-in-scopus-julia",
    "href": "scopus.html#dataset-extraction-in-scopus-julia",
    "title": "scopus",
    "section": "",
    "text": "Once the data assets (?@tbl-usda-datasets) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "scopus.html#footnotes",
    "href": "scopus.html#footnotes",
    "title": "scopus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "openalex.html",
    "href": "openalex.html",
    "title": "openalex",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "openalex.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "href": "openalex.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "title": "openalex",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#scopus",
    "href": "index.html#scopus",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.2 Scopus",
    "text": "4.2 Scopus"
  },
  {
    "objectID": "index.html#openalex",
    "href": "index.html#openalex",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.3 OpenAlex",
    "text": "4.3 OpenAlex"
  },
  {
    "objectID": "index.html#dimensions",
    "href": "index.html#dimensions",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.4 Dimensions",
    "text": "4.4 Dimensions"
  },
  {
    "objectID": "index.html#journal-identification-cal-1",
    "href": "index.html#journal-identification-cal-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.1 Journal Identification Cal",
    "text": "7.1 Journal Identification Cal"
  },
  {
    "objectID": "index.html#author-disambiguation-callauren-1",
    "href": "index.html#author-disambiguation-callauren-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.2 Author Disambiguation Cal/Lauren",
    "text": "7.2 Author Disambiguation Cal/Lauren"
  },
  {
    "objectID": "index.html#institution-disambiguation-calminglauren-1",
    "href": "index.html#institution-disambiguation-calminglauren-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "7.3 Institution Disambiguation Cal/Ming/Lauren",
    "text": "7.3 Institution Disambiguation Cal/Ming/Lauren"
  },
  {
    "objectID": "scopus03.html#journal-identification",
    "href": "scopus03.html#journal-identification",
    "title": "Citation Database Assessment",
    "section": "2 Journal Identification",
    "text": "2 Journal Identification"
  },
  {
    "objectID": "scopus03.html#author-disambiguation",
    "href": "scopus03.html#author-disambiguation",
    "title": "Citation Database Assessment",
    "section": "3 Author Disambiguation",
    "text": "3 Author Disambiguation"
  },
  {
    "objectID": "scopus03.html#institution-disambiguation",
    "href": "scopus03.html#institution-disambiguation",
    "title": "Citation Database Assessment",
    "section": "4 Institution Disambiguation",
    "text": "4 Institution Disambiguation\n\n4.1 IPEDS Ming\n\n\n4.2 MSI Data Ming"
  },
  {
    "objectID": "scopus01.html",
    "href": "scopus01.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "scopus01.html#dataset-extraction-in-scopus-julia",
    "href": "scopus01.html#dataset-extraction-in-scopus-julia",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Once the data assets (?@tbl-usda-datasets) and aliases were defined, the team identified the set of documents within which to conduct the search (i.e., the “search space”). This was done by first creating a seed corpus, as described below, and then using the seed corpus to infer the types of publications in which the targeted data assets would most likely be found.\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "scopus01.html#footnotes",
    "href": "scopus01.html#footnotes",
    "title": "Citation Database Assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "dimensions03.html#journal-identification",
    "href": "dimensions03.html#journal-identification",
    "title": "Citation Database Assessment",
    "section": "2 Journal Identification",
    "text": "2 Journal Identification"
  },
  {
    "objectID": "dimensions03.html#author-disambiguation",
    "href": "dimensions03.html#author-disambiguation",
    "title": "Citation Database Assessment",
    "section": "3 Author Disambiguation",
    "text": "3 Author Disambiguation"
  },
  {
    "objectID": "dimensions03.html#institution-disambiguation",
    "href": "dimensions03.html#institution-disambiguation",
    "title": "Citation Database Assessment",
    "section": "4 Institution Disambiguation",
    "text": "4 Institution Disambiguation\n\n4.1 IPEDS Ming\n\n\n4.2 MSI Data Ming"
  },
  {
    "objectID": "openalex01.html",
    "href": "openalex01.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "openalex01.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "href": "openalex01.html#dataset-extraction-in-openalex-rafaellaurenjuliacal",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "openalex03.html#journal-identification",
    "href": "openalex03.html#journal-identification",
    "title": "Citation Database Assessment",
    "section": "2 Journal Identification",
    "text": "2 Journal Identification"
  },
  {
    "objectID": "openalex03.html#author-disambiguation",
    "href": "openalex03.html#author-disambiguation",
    "title": "Citation Database Assessment",
    "section": "3 Author Disambiguation",
    "text": "3 Author Disambiguation"
  },
  {
    "objectID": "openalex03.html#institution-disambiguation",
    "href": "openalex03.html#institution-disambiguation",
    "title": "Citation Database Assessment",
    "section": "4 Institution Disambiguation",
    "text": "4 Institution Disambiguation\n\n4.1 IPEDS Ming\n\n\n4.2 MSI Data Ming"
  },
  {
    "objectID": "openalex04.html#journal-coverage-cal",
    "href": "openalex04.html#journal-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "2 Journal Coverage Cal",
    "text": "2 Journal Coverage Cal"
  },
  {
    "objectID": "openalex04.html#publication-coverage-cal",
    "href": "openalex04.html#publication-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "3 Publication Coverage Cal",
    "text": "3 Publication Coverage Cal"
  },
  {
    "objectID": "openalex04.html#author-coverage-cal",
    "href": "openalex04.html#author-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "4 Author Coverage Cal",
    "text": "4 Author Coverage Cal"
  },
  {
    "objectID": "openalex04.html#institution-coverage-ming",
    "href": "openalex04.html#institution-coverage-ming",
    "title": "Citation Database Assessment",
    "section": "5 Institution Coverage Ming",
    "text": "5 Institution Coverage Ming"
  },
  {
    "objectID": "openalex04.html#insitutional-representation-ming",
    "href": "openalex04.html#insitutional-representation-ming",
    "title": "Citation Database Assessment",
    "section": "6 Insitutional Representation Ming",
    "text": "6 Insitutional Representation Ming\n\n6.1 Geographic Coverage Ming\n\n\n6.2 Institution Types Ming"
  },
  {
    "objectID": "openalex04.html#thematic-analysis-callauren",
    "href": "openalex04.html#thematic-analysis-callauren",
    "title": "Citation Database Assessment",
    "section": "7 Thematic Analysis Cal/Lauren",
    "text": "7 Thematic Analysis Cal/Lauren\n\n7.1 Publication Topics Cal/Lauren\n\n\n7.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "scopus04.html#journal-coverage-cal",
    "href": "scopus04.html#journal-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "2 Journal Coverage Cal",
    "text": "2 Journal Coverage Cal"
  },
  {
    "objectID": "scopus04.html#publication-coverage-cal",
    "href": "scopus04.html#publication-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "3 Publication Coverage Cal",
    "text": "3 Publication Coverage Cal"
  },
  {
    "objectID": "scopus04.html#author-coverage-cal",
    "href": "scopus04.html#author-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "4 Author Coverage Cal",
    "text": "4 Author Coverage Cal"
  },
  {
    "objectID": "scopus04.html#institution-coverage-ming",
    "href": "scopus04.html#institution-coverage-ming",
    "title": "Citation Database Assessment",
    "section": "5 Institution Coverage Ming",
    "text": "5 Institution Coverage Ming"
  },
  {
    "objectID": "scopus04.html#insitutional-representation-ming",
    "href": "scopus04.html#insitutional-representation-ming",
    "title": "Citation Database Assessment",
    "section": "6 Insitutional Representation Ming",
    "text": "6 Insitutional Representation Ming\n\n6.1 Geographic Coverage Ming\n\n\n6.2 Institution Types Ming"
  },
  {
    "objectID": "scopus04.html#thematic-analysis-callauren",
    "href": "scopus04.html#thematic-analysis-callauren",
    "title": "Citation Database Assessment",
    "section": "7 Thematic Analysis Cal/Lauren",
    "text": "7 Thematic Analysis Cal/Lauren\n\n7.1 Publication Topics Cal/Lauren\n\n\n7.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "dimensions04.html#journal-coverage-cal",
    "href": "dimensions04.html#journal-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "2 Journal Coverage Cal",
    "text": "2 Journal Coverage Cal"
  },
  {
    "objectID": "dimensions04.html#publication-coverage-cal",
    "href": "dimensions04.html#publication-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "3 Publication Coverage Cal",
    "text": "3 Publication Coverage Cal"
  },
  {
    "objectID": "dimensions04.html#author-coverage-cal",
    "href": "dimensions04.html#author-coverage-cal",
    "title": "Citation Database Assessment",
    "section": "4 Author Coverage Cal",
    "text": "4 Author Coverage Cal"
  },
  {
    "objectID": "dimensions04.html#institution-coverage-ming",
    "href": "dimensions04.html#institution-coverage-ming",
    "title": "Citation Database Assessment",
    "section": "5 Institution Coverage Ming",
    "text": "5 Institution Coverage Ming"
  },
  {
    "objectID": "dimensions04.html#insitutional-representation-ming",
    "href": "dimensions04.html#insitutional-representation-ming",
    "title": "Citation Database Assessment",
    "section": "6 Insitutional Representation Ming",
    "text": "6 Insitutional Representation Ming\n\n6.1 Geographic Coverage Ming\n\n\n6.2 Institution Types Ming"
  },
  {
    "objectID": "dimensions04.html#thematic-analysis-callauren",
    "href": "dimensions04.html#thematic-analysis-callauren",
    "title": "Citation Database Assessment",
    "section": "7 Thematic Analysis Cal/Lauren",
    "text": "7 Thematic Analysis Cal/Lauren\n\n7.1 Publication Topics Cal/Lauren\n\n\n7.2 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "index.html#openalex-1",
    "href": "index.html#openalex-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 OpenAlex",
    "text": "5.1 OpenAlex"
  },
  {
    "objectID": "index.html#dimensions-1",
    "href": "index.html#dimensions-1",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.2 Dimensions",
    "text": "5.2 Dimensions\n\n\n\n:::"
  },
  {
    "objectID": "index.html#sec-scopus",
    "href": "index.html#sec-scopus",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "1.2 Scopus",
    "text": "1.2 Scopus",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#sec-openalex",
    "href": "index.html#sec-openalex",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "1.3 OpenAlex",
    "text": "1.3 OpenAlex",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#sec-dimensions",
    "href": "index.html#sec-dimensions",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "1.4 Dimensions",
    "text": "1.4 Dimensions",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "scopus01.html#scopus",
    "href": "scopus01.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision. Increasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time. These three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with: i) full-text records in ScienceDirect which are within a specified range of publication years and ii) the reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of i) decisions were taken with regards to the parameters to be used for creation of the search corpus ii) the implications of that decision on search corpus\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\n\n\n\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 4: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "openalex01.html#openalex",
    "href": "openalex01.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#usda-data-assets",
    "href": "index.html#usda-data-assets",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "5.1 USDA Data Assets",
    "text": "5.1 USDA Data Assets\n\n5.1.1 National Agricultural Statistics Service (NASS) Data Assets\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to the Cornell database. The names of these reports were characterized be being very generic in nature e.g. “Agricultural Prices” and “Farm Labor”. This input was analysed and transformed into a list that also contained ISSN inputs and which appended generic names with the term report. An ISSN term was identified for each of the 21 reports. The report names were characterized as the main data asset (the parent record) and the ISSNs and URLs were used as aliases. There were 43 such aliases. In total there were therefore 64 (43 + 21) search terms associated with this input.\n\n\n5.1.2 Economic Research Service (ERS) Data Assets\nThe list of ERS Data Assets was provided during October 2023. In the original list 2,103 records were provided, many of which also had identified aliases. The list was reviewed by professor Julia Lane and 144 records were removed on the basis that they were not amendable for use within the Machine Learning routines. That review was completed on 29 October. The principal reasons for exclusion were terms either being too generic or they were too specific (and likely to be references within a broader report). Examples of the former were records such as “Milk”, “Cotton” and “CSV Format of National Data “. Examples of the latter were records such as “Table 15—Agricultural chemical input” and “Southeast: 1982-91 1992-97”. Once these records were removed, a total of 1,959 records were left from this input.\nThese records represented the main data assets (the parent record). These records were then subject to review to identify their associated aliases. Unlike the NASS data assets, it was not possible to reliably find an ISSNs of DoIs for these assets. However, for each of these records, a URL link had been provided and these were used as aliases. There were this a further 1,959 terms used as an alias. In total there were therefore 3,918 (1,959 + 1,959) search terms associated with this input.\nBefore the run to create the seed corpus was conducted, a further list of assets was identified. This includes some terms that had been searched for in the Year 1 USDA project (e.g. Census of Agriculture and Agricultural Resource Management Survey) alongside some associated acronyms e.g. FoodAPS. It proved possible to incorporate these late additions. In total these added an additional 20 search terms, comprising 12 main (parent) records and a further 8 alias records.\nThrough consultation with USDA stakeholders and recognition that agricultural economists represent a significant user group of USDA-produced datasets, we identified 13 key datasets to feature on the dashboard. This selection emerged from a broader review of USDA data assets that included over 2,000 ERS records and 21 NASS reports, as described above. The selected datasets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. Table 1 presents these datasets, their producing agencies, and brief descriptions.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases."
  },
  {
    "objectID": "index.html#results-from-disambiguation",
    "href": "index.html#results-from-disambiguation",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "8.1 Results from Disambiguation",
    "text": "8.1 Results from Disambiguation\nThis section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset\n\nMetadata ProcessingMetadata ProcessingMetadata Processing\n\n\n\n8.1.1 Disambiguating Journals\n\n\n8.1.2 Disambiguating Authors\n\n\n8.1.3 Disambiguating Institutions\n\n\n\n\n8.1.4 Disambiguating Journals\n\n\n8.1.5 Disambiguating Authors\n\n\n8.1.6 Disambiguating Institutions\n\n\n\n\n8.1.7 Disambiguating Journals\n\n\n8.1.8 Disambiguating Authors\n\n\n8.1.9 Disambiguating Institutions"
  },
  {
    "objectID": "index.html#results-from-database-comparison",
    "href": "index.html#results-from-database-comparison",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "9.1 Results from Database Comparison",
    "text": "9.1 Results from Database Comparison\nThis section presents results after matching (which type varies – deterministic vs fuzzy)\n\nResults from Database ComparisonJournal Coverage CalPublication Coverage CalAuthor Coverage CalInstitution Coverage MingInsitutional Representation MingThematic Analysis Cal/LaurenResults from Database ComparisonJournal Coverage CalPublication Coverage CalAuthor Coverage CalInstitution Coverage MingInsitutional Representation MingThematic Analysis Cal/LaurenResults from Database ComparisonJournal Coverage CalPublication Coverage CalAuthor Coverage CalInstitution Coverage MingInsitutional Representation MingThematic Analysis Cal/Lauren\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.1 Geographic Coverage Ming\n\n\n9.1.2 Institution Types Ming\n\n\n\n\n9.1.3 Publication Topics Cal/Lauren\n\n\n9.1.4 Patterns over Time Cal/Lauren\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.5 Geographic Coverage Ming\n\n\n9.1.6 Institution Types Ming\n\n\n\n\n9.1.7 Publication Topics Cal/Lauren\n\n\n9.1.8 Patterns over Time Cal/Lauren\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n9.1.9 Geographic Coverage Ming\n\n\n9.1.10 Institution Types Ming\n\n\n\n\n9.1.11 Publication Topics Cal/Lauren\n\n\n9.1.12 Patterns over Time Cal/Lauren"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nThe project workflow outlines the steps involved to evaluate how different citation databases track USDA dataset mentions in research papers. In searching for dataset mentions, the goal is to identify a set of publications that can be compared across the citation database test cases.\n\nThe process of deriving the list of publications from a citation database consists of five steps:\n1. Define Data Assets and Aliases (Section 4.2)\n\nIdentify USDA datasets that will be searched for and tracked.\nCollect official dataset names along with common abbreviations, acronyms, and alternative references used.\n\nResult: A structured list of dataset names and aliases to be used for search strategies.\n2. Create a Seed Corpus (Section 4.3)\n\nCollect an initial set of publications.\nAnalyze how datasets are referenced.\nDetermine how well the dataset name variations (from Step 1) are retreived from the publications.\nAdjust searches to improve accuracy.\n\nResult: A set of seed publications to inform dataset identification and search procedures.\n3. Identify Datasets via Search Routines (Section 4.4)\n\nExtract dataset mentions using various search strategies:\n\nMachine Learning Models: Apply Kaggle competition models trained to detect dataset mentions.\nFull-Text (String) Search: Scan entire articles for relevant dataset names.\nReference Search: Identify dataset citations within publication references.\n\n\nResult: Different search routines yield different sets of publication results.\n4. Disambiguate Publication Results (Section 4.5)\n\nPre-process and clean publication metadata generated from each citation database.\nStandardize journal, institution, and author names.\nDeduplicate records.\n\nResult: Cleaned publication metadata, removed of duplicates, inconsistencies, and missing information.\n5. Compare across Citation Databases (Section 4.6)\n\nCompare dataset coverage across Scopus, OpenAlex, and Dimensions.\nApply fuzzy matching techniques to identify overlapping and unique dataset mentions.\nAnalyze differences in journal coverage, citation patterns, and author affiliations.\n\nResult: A set of statistics that can be used to evaluate dataset tracking accuracy."
  },
  {
    "objectID": "index.html#sec-data",
    "href": "index.html#sec-data",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.1 Define Scope of Data Assets to Be Searched",
    "text": "2.1 Define Scope of Data Assets to Be Searched\n\nThe goal of this step is to compile a structured list of dataset names and their commonly used variations.\n\n\n\n\n\n\n\nNote\n\n\n\nThe data assets featured here consist of those collected by the USDA, primarily from the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). These data assets are widely used in agricultural economics and food systems research.\n\n\n\nNational Agricultural Statistics Service (NASS) Data Assets\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to database curated by Cornell University. These reports are part of the USDA’s efforts to track data usage across various research applications. However, the names of these reports were highly generic, making it difficult to precisely identify them in citation databases. Examples include reports titled “Agricultural Prices” and “Farm Labor,” which lack specificity when compared to more structured dataset identifiers.\nData Processing and Standardization\nTo improve identification and searchability, the input was analyzed and transformed into a structured list that included:\n\nInternational Standard Serial Numbers (ISSNs): Each of the 21 reports was assigned an ISSN, where available, to provide a standardized identifier.\nAlias Creation: Generic report names were appended with the term report to better distinguish them from other similarly named publications in research literature.\nExpanded Search Terms: Additional variations of dataset names were included to account for different citation styles and possible ways authors reference these reports.\n\nThe final dataset classification involved:\n\nMain Data Asset (Parent Record): The original 21 reports, each representing a distinct dataset.\nAliases: ISSNs and URLs served as aliases to improve retrieval accuracy.\nSearch Term Expansion: Combining report names with different citation formats led to a total of 64 search terms (21 parent records + 43 aliases).\n\nThis standardization process improved the efficiency of identifying NASS datasets across publications indexed by citation databases such as Scopus, OpenAlex, and Dimensions.\n\n\nEconomic Research Service (ERS) Data Assets\nThe process of identifying ERS data assets occurred in two phases: (1) an initial dataset compilation, and (2) a refinement process incorporating feedback from a team of agricultural economists at Colorado State University (CSU). This process was meant to yield a list of data assets was both comprehensive and relevant to the research community tracking USDA dataset usage.\nPhase 1: Initial Compilation of ERS Data Assets\nIn October 2023, an initial list of 2,103 ERS records was compiled. These records included dataset names and, in some cases, associated aliases. The list was then reviewed by Professor Julia Lane, who identified and removed 144 records that were not suitable for machine learning-based dataset tracking.\nReasons for Exclusion\n\nRecords were too generic – Terms such as “Milk, Cotton, and CSV Format of National Data” were too broad to be meaningfully identified in citation databases.\nRecords were too specific – Entries such as “Table 15—Agricultural Chemical Input” and “Southeast: 1982-91, 1992-97” were references within broader reports rather than standalone data assets.\n\nAfter these exclusions, the remaining 1,959 records represented the initial list of ERS data assets.\nPhase 2: Refinement with CSU Team\nA team of agricultural economists at CSU were consulted to refine the list so that it accurately captured key USDA datasets that may have been overlooked in the initial process. This involved:\n\nReviewing dataset usage in prior USDA research – Identifying which datasets were frequently cited.\nCross-checking with known data users – Ensuring that key datasets used by agricultural economists were included.\nExpanding alias definitions – Recognizing dataset acronyms and alternative naming conventions.\n\nAs part of this process, an additional set of assets was incorporated, including datasets that had been previously identified in the Year 1 USDA project. Notably, datasets such as the Census of Agriculture and the Agricultural Resource Management Survey (ARMS) were added, along with key acronyms like FoodAPS. This phase contributed:\n\n12 new parent records\n8 additional alias records\nTotal: 20 new search terms\n\nFinal Data Asset Identification\nUnlike NASS data assets, which had ISSNs and DOIs, ERS datasets were primarily linked through URLs. The final structured dataset included:\n\n1,959 parent records (main ERS datasets)\n1,959 alias records (URLs serving as dataset identifiers)\n20 additional records from the CSU consultation\nTotal: 3,918 search terms\n\nThrough this two-phase process, the list of ERS data assets evolved from an initial broad set of records into a refined, structured collection of datasets that could be effectively tracked across citation databases.\n\n\nFinal List of USDA Data Assets\nThe data assets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. The final set of data assets, their producing agencies, and descriptions are presented in Table 1.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases.\n\n\n\n\n\n\n\nAppendix: Data Asset - Alias Dyads\nTo provide a comprehensive reference for dataset tracking, the following appendix includes a detailed list of data assets and their corresponding aliases, collectively referred to as dyads. Each dyad represents a dataset-name and alias pair used in citation database searches, allowing for more precise identification of dataset mentions in research publications. These aliases include acronyms, alternate spellings, dataset variations, and associated URLs, ensuring broad coverage across different citation practices. The dyad list serves as the foundation for dataset extraction and disambiguation across Scopus, OpenAlex, and Dimensions. A complete reference of these dataset aliases is included in Appendix XX.",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#sec-seed-corpus",
    "href": "index.html#sec-seed-corpus",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Scopus-Specific: Create a Seed Corpus",
    "text": "Scopus-Specific: Create a Seed Corpus\nAfter defining the data assets and aliases in Step 1 (Section 2.1), the next step is to identify where these datasets are referenced in research publications. Unlike OpenAlex and Dimensions, which allow for direct full-text searches, Scopus requires a structured seed corpus to establish a more targeted search space.\nFor Scopus, this seed corpus approach was necessary to balance recall (capturing relevant mentions) and precision (minimizing false positives). The process involved:\n\nRestricted search strategies using reference lists and available full-text searches\nMachine learning-assisted review to refine dataset mentions\nManual refinements to resolve ambiguities, consolidate duplicate aliases, and incorporate missing terms.\n\nThis structured search space helped mitigate the constraints of Scopus’s API and ensured that dataset mentions were captured as comprehensively as possible before proceeding to the data identification step (Section 2.2).\n\nScopusOpenAlexDimensions\n\n\n\nRationale\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\nCreating a Seed Corpus\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 5: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\nRefinement of Seed Corpus\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\nReturn to Project Workflow\n\n\n\n\nThe seed corpus approach was only applied to Scopus.\n\n\nThe seed corpus approach was only applied to Scopus.",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#sec-data-extraction",
    "href": "index.html#sec-data-extraction",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.4 Step 3: Dataset Extraction",
    "text": "4.4 Step 3: Dataset Extraction\nOur analysis builds a hierarchical understanding of how USDA datasets appear in Scopus1, OpenAlex, and Dimensions. We start at the journal level, identifying which journals publish research using USDA data to establish publication patterns across different research fields. We then examine individual publications within these journals, focusing on how researchers use and cite USDA datasets in their work. At a more granular level, we analyze the authors who work with these datasets, tracking their institutional affiliations and research networks (e.g., coauthors). Finally, we examine the institutions themselves, mapping where USDA data usage concentrates geographically and across different types of research organizations.\nBefore conducting this analysis, we perform a series of steps to prepare the data. This includes extracting dataset mentions from publication text, disambiguating author names to identify distinct individuals, standardizing institution names, and developing methods to match records across the two databases. The following sections detail these preparatory steps.\n\nScopusOpenAlexDimensions\n\n\n\n4.4.1 Search Routines\n\n4.4.1.1 1. Full Text Search Corpus\nThe search of full text upon which we apply the Machine Learning algorithm relies on the availability of Scopus.\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nIn summary, the USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors as defined in the Table above.\nThe full text records associated with the USDA search corpus is shown below:\n\n\n\nTable 3: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n4.4.1.2 2. References Search Corpus\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\n\n\n4.4.1.3 3. Machine Learning (Kaggle) Routines (Full Text Search)\nThe three models differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in the Table below:\n\n\n\nTable 4: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3922\n\n\nNumber of snippets generated\n14,3773\n\n\n\n\n\n\n\n\n\n4.4.2 Running the Search Routines\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n4.4.3 Scopus References Search Routine\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 5: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n4.4.4 Post Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\n\n\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "index.html#sec-disambiguation",
    "href": "index.html#sec-disambiguation",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.3 Pre-Process and Clean Publication Datasets",
    "text": "2.3 Pre-Process and Clean Publication Datasets\n\nThe goal of this step is to standardize and resolve inconsistencies in publication records by disambiguating journal names, author affiliations, and institutions across the three databases.\n\nTo compare publication coverage across citation databases, we first identify all journals that contain publications using each dataset in Scopus, OpenAlex, and Dimensions. Table 1 displays a list of the datasets for which we evaluate coverage.\nWe subset the publication dataset from Step 2 by filtering for dataset mentions. For example, if a publication references Ag Census, it is included in the Ag Census sub-dataset; otherwise, it is excluded. This process identifies dataset-specific publication patterns in Scopus, OpenAlex, and Dimensions.\nOur approach follows a hierarchical approach to understand how USDA data assets appear in these citation databases.\n\nJournal Level – Identifies journals publishing research using USDA datasets. A journal is included if at least one of its publications references the dataset, but this does not indicate overall dataset prevalence within that journal.\nPublication Level – Examines individual publications within these journals to assess how often and in what context USDA datasets appear.\nAuthor Level – Tracks authors of these publications, analyzing institutional affiliations and research networks to understand dataset reach.\nInstitution Level – Maps dataset usage across institutions to identify geographic and organizational research patterns.\n\nThis structured approach standardizes dataset mention analysis across databases, allowing for direct comparisons of coverage and research impact.\n\n2.3.1 Case Study: Census of Agriculture\n\nTo illustrate the data cleaning and disambiguation process, we use the Census of Agriculture as a case study to systematically compare coverage, overlap, and differences between the three citation databases. The Census of Agriculture (also referred to as “Ag Census”) is widely used in agricultural and economic research, making it an ideal dataset for assessing database differences.\n\nPre-Processing Steps by Citation Database\n\nScopusOpenAlexDimensions\n\n\n\nJournal Coverage\nTo analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\nAuthor Disambiguation\nCOMING SOON\n\n\nInstitution Disambiguation\nCOMING SOON\n\nReturn to Project Workflow\n\n\n\n\n\nJournal Coverage\nTo analyze journal coverage in OpenAlex, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. The OpenAlex journal-level dataset follows a similar approach to Scopus but differs in data availability and structure.\nOpenAlex links journals to their respective publications using linkage files, which connect publication records to dataset mentions. Unlike Scopus, OpenAlex assigns a single dataset identifier per dataset, while Scopus may have multiple identifiers for the same dataset depending on how it is referenced.\nAdditionally, OpenAlex formats journal and publication identifiers differently:\n\nISSNs appear without dashes\nDOIs exclude the URL portion, storing only the core identifier\n\nTo ensure consistency with Scopus, we clean and standardize the publication-level dataset, aligning DOIs and ISSNs to a common format.\nCAL TO DESCRIBE THIS PROCESS\n\n\nAuthor Disambiguation\nCOMING SOON\n\n\nInstitution Disambiguation\nCOMING SOON\n\nReturn to Project Workflow\n\n\n\n\n\n\n\n\n\n\nResults from Data Cleaning\nThis section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset\nStep 4 produces two publication-level datasets: one of all academic papers released through Scopus that use Ag Census data and a similar one for OpenAlex.\nThere are 4712 unique publications reported in Scopus and 1266 in OpenAlex. These data are collapsed into a journal-level dataset based on the International Standard Serial Number (ISSN) that is unique to each academic journal.\n\nScopusOpenAlexDimensions",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "index.html#sec-matching",
    "href": "index.html#sec-matching",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.4 Compare Results across Citation Databases",
    "text": "2.4 Compare Results across Citation Databases\n\nThe goal of this step is to develop statistics that measure dataset tracking accuracy.\n\n\n2.4.1 Case Study: Census of Agriculture\n\nContinuing with our case study, we use the datasets produced in Step 4 to produce counts of the number of journals with Ag Census publications that:\n\nonly appear in Scopus,\nonly appear in OpenAlex, or\nappear in both.\n\nFor journals that contain Ag Census data in both citation databases, we summarize the coverage of publications that appear in both Scopus and OpenAlex.\nThen, we investigate discrepancies based on factors like missing identifiers, mismatched journal information (ISSNs), and additional publications accessed through OpenAlex’s API.\nAdd here: What are the steps in producing Table AA\n\nScopusOpenAlexDimensions\n\n\n\nJournal Coverage\n\n\nAuthor Disambiguation\n\n\nInstitution Disambiguation\n\nReturn to Project Workflow\n\n\n\n\n\nJournal Coverage\n\n\nAuthor Disambiguation\n\n\nInstitution Disambiguation\n\nReturn to Project Workflow\n\n\n\n\n\n\n\n\n\n2.4.2 Results from Database Comparison\nThis section presents results after matching (which type varies – deterministic vs fuzzy)\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Matching Methods\n\n\n\n\n\n\nRule-based matching for exact matches\nProbabilistic matching for handling variations\nMachine learning methods for complex cases\n\n\n\n\nTable 6: Summary of Methods\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nConsiderations\nExample\nPros\nCons\n\n\n\n\nSearching for dataset names within Scopus\n\n\n\n\n\n\nSearching for dataset names within OpenAlex\n“Location” field set to “journal”\n\n\n\n\n\nDisambiguation of authors\n\n\n\n\n\n\nDisambiguation of institutions\n\n\n\n\n\n\nStandardization of institutions\n\n\n\n\n\n\nSearching based on the frequency of dataset appearance in journals\n\n\n\n\n\n\nMORE . . .\n\n\n\n\n\n\nFiltering on keywords to determine themes",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "workflow/scopus01.html",
    "href": "workflow/scopus01.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/scopus01.html#scopus",
    "href": "workflow/scopus01.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/scopus01.html#footnotes",
    "href": "workflow/scopus01.html#footnotes",
    "title": "Citation Database Assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "workflow/openalex01.html",
    "href": "workflow/openalex01.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "workflow/openalex01.html#openalex",
    "href": "workflow/openalex01.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "appendices.html#contents-of-the-csv-files-2",
    "href": "appendices.html#contents-of-the-csv-files-2",
    "title": "Appendices",
    "section": "13 Contents of the CSV Files",
    "text": "13 Contents of the CSV Files\nBelow is a detailed explanation of each CSV file and how they relate to one another:\n\n13.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n13.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n13.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n13.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n13.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#how-to-extract-publications-for-a-specific-dataset-2",
    "href": "appendices.html#how-to-extract-publications-for-a-specific-dataset-2",
    "title": "Appendices",
    "section": "14 How to Extract Publications for a Specific Dataset",
    "text": "14 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices.html#filtering-publications-by-specific-models-2",
    "href": "appendices.html#filtering-publications-by-specific-models-2",
    "title": "Appendices",
    "section": "15 Filtering Publications by Specific Models",
    "text": "15 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices.html#practical-example-2",
    "href": "appendices.html#practical-example-2",
    "title": "Appendices",
    "section": "16 Practical Example",
    "text": "16 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices.html#sample-data-illustration-2",
    "href": "appendices.html#sample-data-illustration-2",
    "title": "Appendices",
    "section": "17 Sample Data Illustration",
    "text": "17 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices.html#contents-of-the-csv-files-3",
    "href": "appendices.html#contents-of-the-csv-files-3",
    "title": "Appendices",
    "section": "19 Contents of the CSV Files",
    "text": "19 Contents of the CSV Files\nBelow is a detailed explanation of the SQL Server-related CSV files and how they relate to one another:\n\n19.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n19.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n19.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n19.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n19.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#how-the-files-are-related-1",
    "href": "appendices.html#how-the-files-are-related-1",
    "title": "Appendices",
    "section": "20 How the Files are Related",
    "text": "20 How the Files are Related\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files. The link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nBy using the OpenAlex identifiers, you can join these files to perform comprehensive analyses. For example:\n\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id."
  },
  {
    "objectID": "appendices.html#how-to-extract-publications-for-a-specific-dataset-3",
    "href": "appendices.html#how-to-extract-publications-for-a-specific-dataset-3",
    "title": "Appendices",
    "section": "21 How to Extract Publications for a Specific Dataset",
    "text": "21 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices.html#filtering-publications-by-specific-models-3",
    "href": "appendices.html#filtering-publications-by-specific-models-3",
    "title": "Appendices",
    "section": "22 Filtering Publications by Specific Models",
    "text": "22 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices.html#practical-example-3",
    "href": "appendices.html#practical-example-3",
    "title": "Appendices",
    "section": "23 Practical Example",
    "text": "23 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices.html#sample-data-illustration-3",
    "href": "appendices.html#sample-data-illustration-3",
    "title": "Appendices",
    "section": "24 Sample Data Illustration",
    "text": "24 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices.html#contents-of-the-csv-files-4",
    "href": "appendices.html#contents-of-the-csv-files-4",
    "title": "Appendices",
    "section": "25 Contents of the CSV Files",
    "text": "25 Contents of the CSV Files\nBelow is a detailed explanation of each CSV file and how they relate to one another:\n\n25.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n25.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n25.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n25.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n25.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#how-to-extract-publications-for-a-specific-dataset-4",
    "href": "appendices.html#how-to-extract-publications-for-a-specific-dataset-4",
    "title": "Appendices",
    "section": "26 How to Extract Publications for a Specific Dataset",
    "text": "26 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices.html#filtering-publications-by-specific-models-4",
    "href": "appendices.html#filtering-publications-by-specific-models-4",
    "title": "Appendices",
    "section": "27 Filtering Publications by Specific Models",
    "text": "27 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices.html#practical-example-4",
    "href": "appendices.html#practical-example-4",
    "title": "Appendices",
    "section": "28 Practical Example",
    "text": "28 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices.html#sample-data-illustration-4",
    "href": "appendices.html#sample-data-illustration-4",
    "title": "Appendices",
    "section": "29 Sample Data Illustration",
    "text": "29 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices.html#conclusion-2",
    "href": "appendices.html#conclusion-2",
    "title": "Appendices",
    "section": "30 Conclusion",
    "text": "30 Conclusion\nBy following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex."
  },
  {
    "objectID": "appendices.html#contents-of-the-csv-files-5",
    "href": "appendices.html#contents-of-the-csv-files-5",
    "title": "Appendices",
    "section": "31 Contents of the CSV Files",
    "text": "31 Contents of the CSV Files\nBelow is a detailed explanation of the SQL Server-related CSV files and how they relate to one another:\n\n31.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n31.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n31.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n31.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n31.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices.html#how-the-files-are-related-2",
    "href": "appendices.html#how-the-files-are-related-2",
    "title": "Appendices",
    "section": "32 How the Files are Related",
    "text": "32 How the Files are Related\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files. The link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nBy using the OpenAlex identifiers, you can join these files to perform comprehensive analyses. For example:\n\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id."
  },
  {
    "objectID": "appendices.html#how-to-extract-publications-for-a-specific-dataset-5",
    "href": "appendices.html#how-to-extract-publications-for-a-specific-dataset-5",
    "title": "Appendices",
    "section": "33 How to Extract Publications for a Specific Dataset",
    "text": "33 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices.html#filtering-publications-by-specific-models-5",
    "href": "appendices.html#filtering-publications-by-specific-models-5",
    "title": "Appendices",
    "section": "34 Filtering Publications by Specific Models",
    "text": "34 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices.html#practical-example-5",
    "href": "appendices.html#practical-example-5",
    "title": "Appendices",
    "section": "35 Practical Example",
    "text": "35 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices.html#sample-data-illustration-5",
    "href": "appendices.html#sample-data-illustration-5",
    "title": "Appendices",
    "section": "36 Sample Data Illustration",
    "text": "36 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices.html#conclusion-3",
    "href": "appendices.html#conclusion-3",
    "title": "Appendices",
    "section": "37 Conclusion",
    "text": "37 Conclusion\nBy following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex.\nKeep in mind that the OpenAlex data now may associate a single publication with multiple datasets, and due to the direct application of aliases and flag terms (with no optimizations), certain datasets like the NASS Census of Agriculture might show fewer publications than before."
  },
  {
    "objectID": "appendices/app_scopus.html",
    "href": "appendices/app_scopus.html",
    "title": "Scopus Data Dictionary",
    "section": "",
    "text": "This section describes the process of construction CSV files extracted from a SQL Server database. These files contain interconnected data about publications and datasets, specifically focusing on how datasets are mentioned within publications. The main goal is to enable you to analyze the relationships between publications and datasets, particularly those identified using specific search models.\nBelow is a detailed explanation of primary tables and how they relate to one another. For a complete list of tables, please refer to the data schema.\n\n\nPLACEHOLDER\n\n\n\nCategory\nFile Path & Link\n\n\n\n\nProcessed IPEDS Dataset\ncompare_scopus_openalex/resources/IPEDS/IPEDS.csv\n\n\nRaw IPEDS Data\ncompare_scopus_openalex/resources/raw_data_IPEDS/\n\n\nData Processing Code\ncompare_scopus_openalex/resources/documentation/IPEDSdata.rmd\n\n\nData Documentation\ncompare_scopus_openalex/resources/documentation/IPEDS_Data.md",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#contents-of-the-csv-files",
    "href": "appendices/app_scopus.html#contents-of-the-csv-files",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "Below is a detailed explanation of each CSV file and how they relate to one another:\n\n\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices/app_scopus.html#how-to-extract-publications-for-a-specific-dataset",
    "href": "appendices/app_scopus.html#how-to-extract-publications-for-a-specific-dataset",
    "title": "Scopus Data Dictionary",
    "section": "How to Extract Publications for a Specific Dataset",
    "text": "How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#filtering-publications-by-specific-models",
    "href": "appendices/app_scopus.html#filtering-publications-by-specific-models",
    "title": "Scopus Data Dictionary",
    "section": "Filtering Publications by Specific Models",
    "text": "Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#practical-example",
    "href": "appendices/app_scopus.html#practical-example",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "Objective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices/app_scopus.html#sample-data-illustration",
    "href": "appendices/app_scopus.html#sample-data-illustration",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "From dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices/app_scopus.html#conclusion",
    "href": "appendices/app_scopus.html#conclusion",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "By following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex."
  },
  {
    "objectID": "appendices/app_scopus.html#contents-of-the-csv-files-1",
    "href": "appendices/app_scopus.html#contents-of-the-csv-files-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.1 Contents of the CSV Files",
    "text": "2.1 Contents of the CSV Files\nBelow is a detailed explanation of the SQL Server-related CSV files and how they relate to one another:\n\n2.1.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n2.1.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n2.1.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n2.1.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n2.1.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices/app_scopus.html#how-the-files-are-related",
    "href": "appendices/app_scopus.html#how-the-files-are-related",
    "title": "1 Scopus Metadata File Description",
    "section": "2.2 How the Files are Related",
    "text": "2.2 How the Files are Related\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files. The link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nBy using the OpenAlex identifiers, you can join these files to perform comprehensive analyses. For example:\n\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id."
  },
  {
    "objectID": "appendices/app_scopus.html#how-to-extract-publications-for-a-specific-dataset-1",
    "href": "appendices/app_scopus.html#how-to-extract-publications-for-a-specific-dataset-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.3 How to Extract Publications for a Specific Dataset",
    "text": "2.3 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices/app_scopus.html#filtering-publications-by-specific-models-1",
    "href": "appendices/app_scopus.html#filtering-publications-by-specific-models-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.4 Filtering Publications by Specific Models",
    "text": "2.4 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices/app_scopus.html#practical-example-1",
    "href": "appendices/app_scopus.html#practical-example-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.5 Practical Example",
    "text": "2.5 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices/app_scopus.html#sample-data-illustration-1",
    "href": "appendices/app_scopus.html#sample-data-illustration-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.6 Sample Data Illustration",
    "text": "2.6 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices/app_dimensions.html",
    "href": "appendices/app_dimensions.html",
    "title": "Dimensions Data Dictionary",
    "section": "",
    "text": "Research partners at the University of Utah have access to Dimensions.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Dimensions Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_dimensions.html#contents-of-the-csv-files",
    "href": "appendices/app_dimensions.html#contents-of-the-csv-files",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "Below is a detailed explanation of each CSV file and how they relate to one another:\n\n\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices/app_dimensions.html#how-to-extract-publications-for-a-specific-dataset",
    "href": "appendices/app_dimensions.html#how-to-extract-publications-for-a-specific-dataset",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "To find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices/app_dimensions.html#filtering-publications-by-specific-models",
    "href": "appendices/app_dimensions.html#filtering-publications-by-specific-models",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "Since we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices/app_dimensions.html#practical-example",
    "href": "appendices/app_dimensions.html#practical-example",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "Objective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices/app_dimensions.html#sample-data-illustration",
    "href": "appendices/app_dimensions.html#sample-data-illustration",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "From dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices/app_dimensions.html#conclusion",
    "href": "appendices/app_dimensions.html#conclusion",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "By following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex."
  },
  {
    "objectID": "appendices/app_dimensions.html#contents-of-the-csv-files-1",
    "href": "appendices/app_dimensions.html#contents-of-the-csv-files-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.1 Contents of the CSV Files",
    "text": "2.1 Contents of the CSV Files\nBelow is a detailed explanation of the SQL Server-related CSV files and how they relate to one another:\n\n2.1.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n2.1.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n2.1.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n2.1.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n2.1.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices/app_dimensions.html#how-the-files-are-related",
    "href": "appendices/app_dimensions.html#how-the-files-are-related",
    "title": "1 Scopus Metadata File Description",
    "section": "2.2 How the Files are Related",
    "text": "2.2 How the Files are Related\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files. The link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nBy using the OpenAlex identifiers, you can join these files to perform comprehensive analyses. For example:\n\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id."
  },
  {
    "objectID": "appendices/app_dimensions.html#how-to-extract-publications-for-a-specific-dataset-1",
    "href": "appendices/app_dimensions.html#how-to-extract-publications-for-a-specific-dataset-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.3 How to Extract Publications for a Specific Dataset",
    "text": "2.3 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices/app_dimensions.html#filtering-publications-by-specific-models-1",
    "href": "appendices/app_dimensions.html#filtering-publications-by-specific-models-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.4 Filtering Publications by Specific Models",
    "text": "2.4 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices/app_dimensions.html#practical-example-1",
    "href": "appendices/app_dimensions.html#practical-example-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.5 Practical Example",
    "text": "2.5 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices/app_dimensions.html#sample-data-illustration-1",
    "href": "appendices/app_dimensions.html#sample-data-illustration-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.6 Sample Data Illustration",
    "text": "2.6 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices/app_openalex.html",
    "href": "appendices/app_openalex.html",
    "title": "OpenAlex Data Dictionary",
    "section": "",
    "text": "This section describes the process of construction CSV files extracted from a SQL Server database. These files contain interconnected data about publications and datasets, specifically focusing on how datasets are mentioned within publications. The main goal is to enable you to analyze the relationships between publications and datasets, particularly those identified using specific search models.\nBelow is a detailed explanation of primary tables and how they relate to one another. For a complete list of tables, please refer to the data schema.\n\n\nPLACEHOLDER\n\n\n\nCategory\nFile Path & Link\n\n\n\n\nProcessed IPEDS Dataset\ncompare_scopus_openalex/resources/IPEDS/IPEDS.csv\n\n\nRaw IPEDS Data\ncompare_scopus_openalex/resources/raw_data_IPEDS/\n\n\nData Processing Code\ncompare_scopus_openalex/resources/documentation/IPEDSdata.rmd\n\n\nData Documentation\ncompare_scopus_openalex/resources/documentation/IPEDS_Data.md",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "OpenAlex Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_openalex.html#contents-of-the-csv-files",
    "href": "appendices/app_openalex.html#contents-of-the-csv-files",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "Below is a detailed explanation of each CSV file and how they relate to one another:\n\n\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices/app_openalex.html#how-to-extract-publications-for-a-specific-dataset",
    "href": "appendices/app_openalex.html#how-to-extract-publications-for-a-specific-dataset",
    "title": "OpenAlex Data Dictionary",
    "section": "How to Extract Publications for a Specific Dataset",
    "text": "How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "OpenAlex Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_openalex.html#filtering-publications-by-specific-models",
    "href": "appendices/app_openalex.html#filtering-publications-by-specific-models",
    "title": "OpenAlex Data Dictionary",
    "section": "Filtering Publications by Specific Models",
    "text": "Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "OpenAlex Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_openalex.html#practical-example",
    "href": "appendices/app_openalex.html#practical-example",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "Objective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices/app_openalex.html#sample-data-illustration",
    "href": "appendices/app_openalex.html#sample-data-illustration",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "From dataset_alias.csv:\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices/app_openalex.html#conclusion",
    "href": "appendices/app_openalex.html#conclusion",
    "title": "1 Scopus Metadata File Description",
    "section": "",
    "text": "By following the steps outlined above, you can effectively navigate the dataset to extract publications related to specific datasets and identified by particular models. This structured approach allows for targeted analysis and comparison with data from sources like OpenAlex."
  },
  {
    "objectID": "appendices/app_openalex.html#contents-of-the-csv-files-1",
    "href": "appendices/app_openalex.html#contents-of-the-csv-files-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.1 Contents of the CSV Files",
    "text": "2.1 Contents of the CSV Files\nBelow is a detailed explanation of the SQL Server-related CSV files and how they relate to one another:\n\n2.1.1 publication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n2.1.2 dataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\nSample Data:\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n2.1.3 dyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\nSample Data:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n2.1.4 model.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\nSample Data:\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n2.1.5 dyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\nSample Data:\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0"
  },
  {
    "objectID": "appendices/app_openalex.html#how-the-files-are-related",
    "href": "appendices/app_openalex.html#how-the-files-are-related",
    "title": "1 Scopus Metadata File Description",
    "section": "2.2 How the Files are Related",
    "text": "2.2 How the Files are Related\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files. The link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nBy using the OpenAlex identifiers, you can join these files to perform comprehensive analyses. For example:\n\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id."
  },
  {
    "objectID": "appendices/app_openalex.html#how-to-extract-publications-for-a-specific-dataset-1",
    "href": "appendices/app_openalex.html#how-to-extract-publications-for-a-specific-dataset-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.3 How to Extract Publications for a Specific Dataset",
    "text": "2.3 How to Extract Publications for a Specific Dataset\nTo find all publications associated with a particular dataset, such as the NASS Census of Agriculture, follow these steps:\n\nIdentify the Main Alias:\n\nFind the alias_id where alias_id equals parent_alias_id for the dataset.\nFor NASS Census of Agriculture, the main alias has alias_id = 89.\n\nGet All Aliases:\n\nIn dataset_alias.csv, filter rows where parent_alias_id equals 89.\nThis gives you all aliases associated with the NASS Census of Agriculture dataset.\n\nLink Aliases to Publications:\n\nIn dyad.csv, filter rows where alias_id matches any of the alias_ids obtained in step 2.\nThis will give you publication_ids of publications mentioning any alias of the dataset.\n\nRetrieve Publication Details:\n\nUsing the publication_ids from step 3, retrieve the corresponding records from publication.csv."
  },
  {
    "objectID": "appendices/app_openalex.html#filtering-publications-by-specific-models-1",
    "href": "appendices/app_openalex.html#filtering-publications-by-specific-models-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.4 Filtering Publications by Specific Models",
    "text": "2.4 Filtering Publications by Specific Models\nSince we’re interested in mentions identified by the string_matching and refmatch models (models with id 1 and 5), follow these steps:\n\nFilter Dyads by Model:\n\nIn dyad_model.csv, filter rows where model_id is 1 or 5.\nThis gives you dyad_ids linked to these models.\n\nGet Relevant Dyads:\n\nPerform an inner join with dyad.csv on dyad_id.\nThis filters dyads to only those identified by the specified models.\n\nProceed as Before:\n\nContinue with the steps in the previous section, but using the filtered dyads from step 2."
  },
  {
    "objectID": "appendices/app_openalex.html#practical-example-1",
    "href": "appendices/app_openalex.html#practical-example-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.5 Practical Example",
    "text": "2.5 Practical Example\nObjective: Find all publications mentioning the NASS Census of Agriculture dataset using the string_matching and refmatch models.\nSteps:\n\nFind Main Alias ID:\n\nMain alias alias_id = 89.\n\nGet All Aliases:\n\nFrom dataset_alias.csv, extract all alias_ids where parent_alias_id = 89.\n\nFilter Dyads by Models:\n\nFrom dyad_model.csv, get dyad_ids where model_id is 1 or 5.\n\nGet Dyads for Dataset:\n\nFrom dyad.csv, select rows where:\n\nalias_id is in the list from step 2, and\nid (dyad_id) is in the list from step 3.\n\n\nRetrieve Publications:\n\nUse publication_ids from the filtered dyads to get detailed information from publication.csv."
  },
  {
    "objectID": "appendices/app_openalex.html#sample-data-illustration-1",
    "href": "appendices/app_openalex.html#sample-data-illustration-1",
    "title": "1 Scopus Metadata File Description",
    "section": "2.6 Sample Data Illustration",
    "text": "2.6 Sample Data Illustration\nFrom dataset_alias.csv:\n\n\n\n\nalias_id\nparent_alias_id\nalias\n\n\n\n\n87\n89\nCensus of Agriculture\n\n\n88\n89\nUSDA Census of Agriculture\n\n\n\nFrom dyad.csv:\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\nFrom dyad_model.csv (filtered for model_id 1 or 5):\n\n\n\n\ndyad_id\nmodel_id\n\n\n\n\n2569\n1\n\n\n2573\n5\n\n\n\nResulting Publications:\n\nPublications with id 1211491 and 1199598 from publication.csv."
  },
  {
    "objectID": "appendices/app_openalex.html#sec-app-openalex",
    "href": "appendices/app_openalex.html#sec-app-openalex",
    "title": "OpenAlex Data Dictionary",
    "section": "",
    "text": "This section describes the process of construction CSV files extracted from a SQL Server database. These files contain interconnected data about publications and datasets, specifically focusing on how datasets are mentioned within publications. The main goal is to enable you to analyze the relationships between publications and datasets, particularly those identified using specific search models.\nBelow is a detailed explanation of primary tables and how they relate to one another. For a complete list of tables, please refer to the data schema.\n\n\nPLACEHOLDER\n\n\n\nCategory\nFile Path & Link\n\n\n\n\nProcessed IPEDS Dataset\ncompare_scopus_openalex/resources/IPEDS/IPEDS.csv\n\n\nRaw IPEDS Data\ncompare_scopus_openalex/resources/raw_data_IPEDS/\n\n\nData Processing Code\ncompare_scopus_openalex/resources/documentation/IPEDSdata.rmd\n\n\nData Documentation\ncompare_scopus_openalex/resources/documentation/IPEDS_Data.md",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "OpenAlex Data Dictionary"
    ]
  },
  {
    "objectID": "workflow/scopus02.html",
    "href": "workflow/scopus02.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 1:\n\n\n\nTable 1: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 2: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 3:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/scopus02.html#scopus",
    "href": "workflow/scopus02.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 1:\n\n\n\nTable 1: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 2: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 3:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/scopus02.html#footnotes",
    "href": "workflow/scopus02.html#footnotes",
    "title": "Citation Database Assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "index.html#sec-data-idn",
    "href": "index.html#sec-data-idn",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.2 Extract Dataset Mentions to Build a Publication Dataset",
    "text": "2.2 Extract Dataset Mentions to Build a Publication Dataset\n\nThe goal of this step is to build a dataset of publications that reference the dataset name aliases for the USDA data assets (Table 1) across Scopus, OpenAlex, and Dimensions.\n\nTo generate this dataset, the process requires:\n\nDataset name aliases (from Step 1)\nSearch routines tailored to each citation database to extract relevant publications\n\nSearch routines, described below, guide this step, as dataset mentions are often inconsistent across publications—appearing in titles, abstracts, full text, or reference lists. Scopus uses a structured seed corpus to refine searches, while OpenAlex and Dimensions rely on direct queries across their full publication records. The outputs of this step are three publication-level datasets, one for each citation database, which will be further analyzed in subsequent steps.\n\nScopusOpenAlexDimensions\n\n\n\nSearch Routines\nThe search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n1. Full Text Search Corpus\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 2:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n2. References Search Corpus\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 3: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n3. Machine Learning (Kaggle) Routines (Full Text Search)\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 4:\n\n\n\nTable 4: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\nReturn to Project Workflow\n\n\n\n\n\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction.",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "workflow/openalex02.html",
    "href": "workflow/openalex02.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "workflow/openalex02.html#openalex",
    "href": "workflow/openalex02.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "appendices/app_scopus.html#sec-app-scopus",
    "href": "appendices/app_scopus.html#sec-app-scopus",
    "title": "Scopus Data Dictionary",
    "section": "",
    "text": "This section describes the process of construction CSV files extracted from a SQL Server database. These files contain interconnected data about publications and datasets, specifically focusing on how datasets are mentioned within publications. The main goal is to enable you to analyze the relationships between publications and datasets, particularly those identified using specific search models.\nBelow is a detailed explanation of primary tables and how they relate to one another. For a complete list of tables, please refer to the data schema.\n\n\nPLACEHOLDER\n\n\n\nCategory\nFile Path & Link\n\n\n\n\nProcessed IPEDS Dataset\ncompare_scopus_openalex/resources/IPEDS/IPEDS.csv\n\n\nRaw IPEDS Data\ncompare_scopus_openalex/resources/raw_data_IPEDS/\n\n\nData Processing Code\ncompare_scopus_openalex/resources/documentation/IPEDSdata.rmd\n\n\nData Documentation\ncompare_scopus_openalex/resources/documentation/IPEDS_Data.md",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "workflow/dimensions01.html",
    "href": "workflow/dimensions01.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "workflow/dimensions01.html#dimensions",
    "href": "workflow/dimensions01.html#dimensions",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "index.html#sec-workflow-overview",
    "href": "index.html#sec-workflow-overview",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "4.1 Overview",
    "text": "4.1 Overview\nThe project workflow outlines the steps involved to evaluate how different citation databases track USDA dataset mentions in research papers. In searching for dataset mentions, the goal is to identify a set of publications that can be compared across the citation database test cases.\n\nThe process of deriving the list of publications from a citation database consists of five steps:\n1. Define Data Assets and Aliases (Section 4.2)\n\nIdentify USDA datasets that will be searched for and tracked.\nCollect official dataset names along with common abbreviations, acronyms, and alternative references used.\n\nResult: A structured list of dataset names and aliases to be used for search strategies.\n2. Create a Seed Corpus (Section 4.3)\n\nCollect an initial set of publications.\nAnalyze how datasets are referenced.\nDetermine how well the dataset name variations (from Step 1) are retreived from the publications.\nAdjust searches to improve accuracy.\n\nResult: A set of seed publications to inform dataset identification and search procedures.\n3. Identify Datasets via Search Routines (Section 4.4)\n\nExtract dataset mentions using various search strategies:\n\nMachine Learning Models: Apply Kaggle competition models trained to detect dataset mentions.\nFull-Text (String) Search: Scan entire articles for relevant dataset names.\nReference Search: Identify dataset citations within publication references.\n\n\nResult: Different search routines yield different sets of publication results.\n4. Disambiguate Publication Results (Section 4.5)\n\nPre-process and clean publication metadata generated from each citation database.\nStandardize journal, institution, and author names.\nDeduplicate records.\n\nResult: Cleaned publication metadata, removed of duplicates, inconsistencies, and missing information.\n5. Compare across Citation Databases (Section 4.6)\n\nCompare dataset coverage across Scopus, OpenAlex, and Dimensions.\nApply fuzzy matching techniques to identify overlapping and unique dataset mentions.\nAnalyze differences in journal coverage, citation patterns, and author affiliations.\n\nResult: A set of statistics that can be used to evaluate dataset tracking accuracy."
  },
  {
    "objectID": "index.html#project-objective",
    "href": "index.html#project-objective",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The objective of this project is to determine the coverage of publications across citation databases. This information will be used to decide the viability of replacing Scopus with a less costly solution.",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "workflow/scopus03.html",
    "href": "workflow/scopus03.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/scopus03.html#scopus",
    "href": "workflow/scopus03.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/openalex03.html",
    "href": "workflow/openalex03.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in OpenAlex, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. The OpenAlex journal-level dataset follows a similar approach to Scopus but differs in data availability and structure.\nOpenAlex links journals to their respective publications using linkage files, which connect publication records to dataset mentions. Unlike Scopus, OpenAlex assigns a single dataset identifier per dataset, while Scopus may have multiple identifiers for the same dataset depending on how it is referenced.\nAdditionally, OpenAlex formats journal and publication identifiers differently:\n\nISSNs appear without dashes\nDOIs exclude the URL portion, storing only the core identifier\n\nTo ensure consistency with Scopus, we clean and standardize the publication-level dataset, aligning DOIs and ISSNs to a common format.\nCAL TO DESCRIBE THIS PROCESS\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/openalex03.html#openalex",
    "href": "workflow/openalex03.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in OpenAlex, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. The OpenAlex journal-level dataset follows a similar approach to Scopus but differs in data availability and structure.\nOpenAlex links journals to their respective publications using linkage files, which connect publication records to dataset mentions. Unlike Scopus, OpenAlex assigns a single dataset identifier per dataset, while Scopus may have multiple identifiers for the same dataset depending on how it is referenced.\nAdditionally, OpenAlex formats journal and publication identifiers differently:\n\nISSNs appear without dashes\nDOIs exclude the URL portion, storing only the core identifier\n\nTo ensure consistency with Scopus, we clean and standardize the publication-level dataset, aligning DOIs and ISSNs to a common format.\nCAL TO DESCRIBE THIS PROCESS\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/openalex04.html",
    "href": "workflow/openalex04.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in OpenAlex, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. The OpenAlex journal-level dataset follows a similar approach to Scopus but differs in data availability and structure.\nOpenAlex links journals to their respective publications using linkage files, which connect publication records to dataset mentions. Unlike Scopus, OpenAlex assigns a single dataset identifier per dataset, while Scopus may have multiple identifiers for the same dataset depending on how it is referenced.\nAdditionally, OpenAlex formats journal and publication identifiers differently:\n\nISSNs appear without dashes\nDOIs exclude the URL portion, storing only the core identifier\n\nTo ensure consistency with Scopus, we clean and standardize the publication-level dataset, aligning DOIs and ISSNs to a common format.\nCAL TO DESCRIBE THIS PROCESS\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/openalex04.html#openalex",
    "href": "workflow/openalex04.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in OpenAlex, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. The OpenAlex journal-level dataset follows a similar approach to Scopus but differs in data availability and structure.\nOpenAlex links journals to their respective publications using linkage files, which connect publication records to dataset mentions. Unlike Scopus, OpenAlex assigns a single dataset identifier per dataset, while Scopus may have multiple identifiers for the same dataset depending on how it is referenced.\nAdditionally, OpenAlex formats journal and publication identifiers differently:\n\nISSNs appear without dashes\nDOIs exclude the URL portion, storing only the core identifier\n\nTo ensure consistency with Scopus, we clean and standardize the publication-level dataset, aligning DOIs and ISSNs to a common format.\nCAL TO DESCRIBE THIS PROCESS\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/scopus04.html",
    "href": "workflow/scopus04.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/scopus04.html#scopus",
    "href": "workflow/scopus04.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "appendices/app_ipeds.html",
    "href": "appendices/app_ipeds.html",
    "title": "Classifying Institutions with the IPEDS Database",
    "section": "",
    "text": "The Integrated Postsecondary Education Data System (IPEDS) is a national dataset maintained by the National Center for Education Statistics (NCES). It collects data from all U.S. institutions participating in federal financial aid programs. In this project, IPEDS data is used to analyze institutional characteristics (e.g., size, classification, location, and financial indicators) in relation to research publications identified in Scopus, OpenAlex, and Dimensions.\nThe raw IPEDS data was obtained from the IPEDS website: IPEDS Data Center.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Classifying Institutions with the IPEDS Database"
    ]
  },
  {
    "objectID": "appendices/app_ipeds.html#data-files-explanation",
    "href": "appendices/app_ipeds.html#data-files-explanation",
    "title": "1 IPEDS Data Explanation",
    "section": "",
    "text": "The IPEDS data were obtained from the IPEDS website: https://nces.ed.gov/ipeds/datacenter/DataFiles.aspx?year=2023&surveyNumber=-1&sid=f98b3f39-0633-4dd3-bbc6-ec4e29b9098b&rtid=7\n\n\n\n\n\n\n\n\ndescription\nSource Table/Field in IPEDS website\nVariable name\n\n\n\n\nInstitution size\nEFFY2017,EFFY2018,EFFY2019,EFFY2020,EFFY2021,EFFY2022,EFFY2023\nUNITID,EFFYLEV,EFFYTOTLT\n\n\nInstitution Location\nHD2017, HD2018, HD2019, HD2020, HD2021, HD2022, HD2023\nUNITID,INSRNM,ADDR,CITY,STABBR,ZIP,FIPS\n\n\nInstitution is recognized as an HBCU\nHD2017, HD2018, HD2019, HD2020, HD2021, HD2022, HD2023\nUNITID, HBCU\n\n\nCarnegie classification\nHD2017, HD2018, HD2019, HD2020, HD2021, HD2022, HD2023\nUNITID, ICLEVEL,CONTROL,LANDGRANT\n\n\nEndowment size\nF1617_F1A,F1718_F1A, F1819_F1A,F1920_F1A,F2021_F1A,F2122_F1A,F2223_F1A\nUNITID,F1H01,F1H01\n\n\nlibrary budget\nAL2017, AL2018, AL2019,AL2020,AL2021,AL2022,AL2023\nUNITID, LEXPTOT\n\n\n\n\n\nDescription: This file contains the main information about the 12-month unduplicated headcount by race/ethnicity, gender and level of student\nContents:\n\nUNITID: The unique IPEDS identifier for each institution.\nEFFYLEV: level of study, 1-All students total, 2–undergraduate, 4-graduate\nEFYTOTLT: Grand total men and women enrolled for credit during the 12-month reporting period.\nyear: The year in which the enrollment size was recorded.\n\nNotes: This file does not contain the year variable, so I added it. finally, we only leave, EFFYLEV == 1, since 1 is all students level, including both undergraduate and graduate.\n\n\n\nDescription: This file contains information about the characteristic of the institutions.\nContents:\n\nUNITID:Unique identification number of the institution\nINSTNM: Institution (entity) name\nADDR: Street address or post office box\nCITY: City location of institution\nSTABBR: State abbreviation\nHBCU: Historically Black College or University\nCONTROL: Public, CONTROL==1; Private not-for-profit, CONTROL=2, private for-profit, CONTROL=3; not available, CONTROL=-3\nICLEVEL: 4-year or higher (4 year), ICLEVEL=1; 2-but-less-than 4-year (2 year),ICLEVEL=2; or less than 2-year, ICLEVEL=3; not available, ICLEVEL=-3\nLANDGRNT: 1, land grant, 2, not a land grant\nyear: The year variable that I added\n\n\n\n\nDescription: This file contains information about endowment assets of public universities.\nContents\n\nUNITID:Unique identification number of the institution\nF1H01: Value of endowment assets at the beginning of the fiscal year\nF1H02: Value of endowment assets at the end of the fiscal year\nyear: The year in which the endowment size was recorded.\n\n\n\n\nDescription: This file contains information about the library expenditure data.\nContents\n\nUNITID:Unique identification number of the institution\nLEXPTOT: Total expenditures (salaries/wages, benefits, materials/services, and operations/maintenance)\nyear: The year in which the endowment size was recorded."
  },
  {
    "objectID": "appendices/app_ipeds.html#how-the-files-are-related",
    "href": "appendices/app_ipeds.html#how-the-files-are-related",
    "title": "1 IPEDS Data Explanation",
    "section": "",
    "text": "Use UNITID and year variable to joint those dataset."
  },
  {
    "objectID": "appendices/app_ipeds.html#msi-data",
    "href": "appendices/app_ipeds.html#msi-data",
    "title": "Citation Database Assessment",
    "section": "2 MSI data",
    "text": "2 MSI data\nDescription: The MSI data prior to 2021 (including 2021) are sourced from The Minority Serving Institutions (MSI) Data Project, developed by Mike Hoa Nguyen et al. (2023), at this website: https://nyu.app.box.com/s/kf6ihe5wcm8gc4pkgcbvv5dfzjy2w6rm. This Project merges the U.S. Department of Education’s annual MSI eligibility matrics for 2017, 2018, 2019, 2020, and 2021 with IPEDS data.\nAfter 2021, MSI data are obtained from the Rutgers Graduate School of Education.\n\n2022: https://cmsi.gse.rutgers.edu/sites/default/files/2022%20MSI%20List.pdf\n2023: https://cmsi.gse.rutgers.edu/sites/default/files/2023%20CMSI%20MSI%20List.pdf\n\n\n2.1 1. 2017-2021 MSI data\nDescription: This file contains information about the eligibility of institution by programs .\nContents\n\nUNITID:Unique identification number of the institution\nelig_atleastone: Eligible or potentially eligible for at least one program\nyear: The year in which the endowment size was recorded.\n\n\n\n2.2 1. 2022-2023\nDescription: I first use Python to convert the PDF file to excel, in “MSI_after2020.py”.\nContents\n\nUNITID:Unique identification number of the institution\nelig_atleastone_after21: Eligible in the current year.\nyear: The year in which the endowment size was recorded."
  },
  {
    "objectID": "appendices/app_ipeds.html#how-the-files-are-related-1",
    "href": "appendices/app_ipeds.html#how-the-files-are-related-1",
    "title": "Citation Database Assessment",
    "section": "3 How the Files are Related",
    "text": "3 How the Files are Related\nThen all msi data can be joint with the IPEDS data by UNITID and year."
  },
  {
    "objectID": "appendices/app_ipeds.html#ipeds",
    "href": "appendices/app_ipeds.html#ipeds",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The Integrated Postsecondary Education Data System (IPEDS) is a national dataset maintained by the National Center for Education Statistics (NCES). It collects data from all U.S. institutions participating in federal financial aid programs. In this project, IPEDS data is used to analyze institutional characteristics (e.g., size, classification, location, and financial indicators) in relation to research publications identified in Scopus, OpenAlex, and Dimensions.\nThe raw IPEDS data was obtained from the IPEDS website: IPEDS Data Center.\n\n\n\nThe IPEDS dataset provides institutional characteristics that are used in:\n\nDisambiguating institution names in citation databases (Scopus, OpenAlex, Dimensions)\nComparing research output across different types of institutions\nAssessing dataset usage by institution type (e.g., HBCUs, land-grant universities, private vs. public institutions)\n\nBy integrating IPEDS institutional data with the publication datasets generated from each citation database, we can evaluate how research output varies across institution types and funding levels.\n\n\n\n\n\n\nCategory\nFile Path & Link\n\n\n\n\nProcessed IPEDS Dataset\ncompare_scopus_openalex/resources/IPEDS/IPEDS.csv\n\n\nRaw IPEDS Data\ncompare_scopus_openalex/resources/raw_data_IPEDS/\n\n\nData Processing Code\ncompare_scopus_openalex/resources/documentation/IPEDSdata.rmd\n\n\nData Documentation\ncompare_scopus_openalex/resources/documentation/IPEDS_Data.md\n\n\n\n\n\n\nThe IPEDS dataset integrates multiple tables across different survey years (2017-2023) to capture important institutional characteristics.\n\n\n\n\n\n\n\n\nVariable\nIPEDS Table (Years)\nDescription\n\n\n\n\nInstitution Size\nEFFY2017 - EFFY2023\nTotal student enrollment by level (undergrad/graduate)\n\n\nInstitution Location\nHD2017 - HD2023\nInstitution name, address, city, state, ZIP\n\n\nHBCU Status\nHD2017 - HD2023\nIndicator if institution is a Historically Black College or University (HBCU)\n\n\nCarnegie Classification\nHD2017 - HD2023\nInstitution type (e.g., public/private, 2-year/4-year)\n\n\nEndowment Size\nF1A2017 - F1A2023\nValue of endowment assets at the start and end of the fiscal year\n\n\nLibrary Budget\nAL2017 - AL2023\nTotal library expenditures (salaries, materials, operations)\n\n\n\n\n\n\n\n\n\nDescription: Contains 12-month unduplicated headcount enrollment by race/ethnicicy, gender, and student level.\nContents:\n\nUNITID: Unique institution identifier\nEFFYLEV: Level of study (1 = All students, 2 = Undergrad, 4 = Graduate)\nEFYTOTLT: Total students enrolled during the 12-month period\nyear: Manually added variable to track enrollment over time\n\nProcessing Notes:\n\nOnly includes EFFYLEV = 1 (All students level).\nYear variable was manually added since it was not originally present.\n\n\n\n\n\n\nDescription: Provides details on institution location, classification, and designation (e.g., HBCU, land-grant).\nContents:\n\nUNITID: Unique institution identifier\nINSTNM: Institution name\nADDR, CITY, STABBR, ZIP: Location information\nHBCU: 1 = HBCU, 0 = Non-HBCU\nCONTROL: 1 = Public, 2 = Private Nonprofit, 3 = Private For-Profit\nICLEVEL: 1 = 4-year, 2 = 2-year, 3 = Less than 2-year\nLANDGRNT: 1 = Land Grant, 2 = Not Land Grant\nyear: Added variable for time tracking\n\n\n\n\n\n\nDescription: Reports institutional endowment assets of public universities at the beginning and end of the fiscal year.\nContents:\n\nUNITID: Unique institution identifier\nF1H01: Endowment assets at start of fiscal year\nF1H02: Endowment assets at end of fiscal year\nyear: Fiscal year\n\n\n\n\n\n\nDescription: Tracks total library budgets, including salaries, benefits, and operations.\nContents:\n\nUNITID: Unique institution identifier\nLEXPTOT: Total library expenditures\nyear: Reporting year\n\n\n\n\n\n\n\nVariable Name Changes and Formatting:\n\nUNITID and year serve as the primary keys to merge datasets for analysis.\nyear was added to datasets that lacked it.\n\nHandling Missing Data and Filters:\n\nNon-relevant columns were removed.\nDatasets were filtered to retain only institutions with complete enrollment and classification data.\n\nMerging Strategy:\n\nDatasets can be joined using UNITID and year as unique identifiers.\nInstitutions missing UNITID were excluded.\n\n\n\n\n\nThe IPEDS dataset can be linked with the MSI dataset using the UNITID and year variables. This allows for:\n\nIdentifying MSI institutions within IPEDS to analyze institutional characteristics.\nComparing institutional characteristics of MSI and non-MSI institutions, such as enrollment size, Carnegie classification, and financial indicators.\nAssessing research output and dataset usage by institution type across Scopus, OpenAlex, and Dimensions.\nExamining trends over time in MSI status and institutional characteristics.",
    "crumbs": [
      "Reference Files: Dimensions",
      "Data Schema",
      "IPEDS"
    ]
  },
  {
    "objectID": "appendices/app_msi.html",
    "href": "appendices/app_msi.html",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "",
    "text": "The Minority-Serving Institutions (MSI) Data captures institutions eligible for federal MSI programs, as determined by the U.S. Department of Education and other sources. In this project, MSI data is used to analyze institutional characteristics in relation to research publications identified in Scopus, OpenAlex, and Dimensions.\nThis dataset provides information on institutions eligible or potentially eligible for at least one MSI designation and includes data from 2017 to 2023. The MSI dataset integrates information from multiple sources and is merged with IPEDS data to track research output across different institution types.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#msi",
    "href": "appendices/app_msi.html#msi",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The Minority-Serving Institutions (MSI) Data captures institutions eligible for federal MSI programs, as determined by the U.S. Department of Education and other sources. In this project, MSI data is used to analyze institutional characteristics in relation to research publications identified in Scopus, OpenAlex, and Dimensions.\nThis dataset provides information on institutions eligible or potentially eligible for at least one MSI designation and includes data from 2017 to 2023. The MSI dataset integrates information from multiple sources and is merged with IPEDS data to track research output across different institution types.\n\n\n\nThe MSI dataset are used to:\n\nIdentify MSI-designated institutions within Scopus, OpenAlex, and Dimensions.\nCompare research output across MSI and non-MSI institutions.\nAnalyze dataset usage at institutions serving underrepresented student populations.\n\nBy integrating MSI eligibility data with publication datasets, we can assess whether MSI institutions are underrepresented in academic research databases.\n\n\n\n\n\n\nCategory\nFile Path & Link\n\n\n\n\nProcessed MSI Dataset\ncompare_scopus_openalex/resources/MSI/MSI.csv\n\n\nRaw MSI Data\ncompare_scopus_openalex/resources/raw_data_MSI/\n\n\nData Processing Code\ncompare_scopus_openalex/resources/documentation/MSIdata.rmd\n\n\nData Documentation\ncompare_scopus_openalex/resources/documentation/MSI_Data.md\n\n\nPython Script for PDF Processing\ncompare_scopus_openalex/resources/documentation/MSI_after2020.py\n\n\n\n\n\n\nThe MSI dataset combines data from multiple sources:\n\n\n\nYears\nSource\nLink\n\n\n\n\n2017-2021\nMSI Data Project (Nguyen et al., 2023)1\nLink\n\n\n2022\nRutgers CMSI2\nPDF\n\n\n2023\nRutgers CMSI3\nPDF\n\n\n\n\n\n\n\n\n\nDescription: Contains MSI eligibility and designation data for institutions from 2017 to 2021.\nContents:\n\nUNITID: Unique institution identifier\nelig_atleastone: Institution is eligible or potentially eligible for at least one MSI program\nyear: Year of MSI designation\n\n\n\n\n\n\nDescription: MSI eligibility data for 2022 and 2023, extracted from Rutgers CMSI PDFs.\nContents:\n\nUNITID: Unique institution identifier\nelig_atleastone_after21: Institution eligible in the respective year.\nyear: Year of MSI designation\n\nProcessing Notes:\n\nPython was used to convert PDFs to Excel (MSI_after2020.py).\nData was cleaned and standardized to match the 2017-2021 format.\n\n\n\n\n\n\n\nVariable Name Changes and Formatting:\n\nyear was added to track MSI status over time.\nUNITID is the primary key to merge MSI and IPEDS datasets.\n\nHandling Missing Data and Filters:\n\nInstitutions without UNITID were excluded.\nNon-relevant columns were removed.\n\nMerging Strategy:\n\nThe 2017-2021 MSI data was directly compatible with IPEDS.\nThe 2022-2023 MSI data required additional formatting before merging.\nThe final MSI dataset is merged with IPEDS using UNITID and year.\n\n\n\n\n\nThe MSI dataset can be linked with IPEDS data using the UNITID and year variables. This allows for:\n\nTracking institutional MSI status over time.\nComparing MSI and non-MSI institutions within Scopus, OpenAlex, and Dimensions.\nAssessing dataset usage patterns across different institution types.",
    "crumbs": [
      "Reference Files: Dimensions",
      "Data Schema",
      "MSI"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#how-the-files-are-related",
    "href": "appendices/app_msi.html#how-the-files-are-related",
    "title": "Citation Database Assessment",
    "section": "2 How the Files are Related",
    "text": "2 How the Files are Related\nThen all msi data can be joint with the IPEDS data by UNITID and year."
  },
  {
    "objectID": "appendices/app_msi.html#footnotes",
    "href": "appendices/app_msi.html#footnotes",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 2017-2021 MSI data was sourced from the Minority Serving Institutions (MSI) Data Project by Nguyen et al. (2023), which merges the U.S. Department of Education’s MSI eligibility metrics (2017-2021) with IPEDS data.↩︎\nThe 2022-2023 MSI data was obtained from the Rutgers Graduate School of Education, which maintains annual MSI eligibility lists.↩︎\nThe 2022-2023 MSI data was obtained from the Rutgers Graduate School of Education, which maintains annual MSI eligibility lists.↩︎",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "appendices.html#footnotes",
    "href": "appendices.html#footnotes",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe 2017-2021 MSI data was sourced from the Minority Serving Institutions (MSI) Data Project by Nguyen et al. (2023), which merges the U.S. Department of Education’s MSI eligibility metrics (2017-2021) with IPEDS data.↩︎\nThe 2022-2023 MSI data was obtained from the Rutgers Graduate School of Education, which maintains annual MSI eligibility lists.↩︎\nThe 2022-2023 MSI data was obtained from the Rutgers Graduate School of Education, which maintains annual MSI eligibility lists.↩︎",
    "crumbs": [
      "Appendices"
    ]
  },
  {
    "objectID": "workflow/03scopus.html",
    "href": "workflow/03scopus.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 1:\n\n\n\nTable 1: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 2: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 3:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/03scopus.html#scopus",
    "href": "workflow/03scopus.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 1:\n\n\n\nTable 1: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 2: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 3:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/03scopus.html#footnotes",
    "href": "workflow/03scopus.html#footnotes",
    "title": "Citation Database Assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "workflow/02openalex.html",
    "href": "workflow/02openalex.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "workflow/02openalex.html#openalex",
    "href": "workflow/02openalex.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "workflow/02dimensions.html",
    "href": "workflow/02dimensions.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "workflow/02dimensions.html#dimensions",
    "href": "workflow/02dimensions.html#dimensions",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "workflow/02scopus.html",
    "href": "workflow/02scopus.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\nReturn to Project Workflow",
    "crumbs": [
      "Project Workflow",
      "Scopus"
    ]
  },
  {
    "objectID": "workflow/02scopus.html#scopus",
    "href": "workflow/02scopus.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\nReturn to Project Workflow",
    "crumbs": [
      "Project Workflow",
      "Scopus"
    ]
  },
  {
    "objectID": "workflow/03openalex.html",
    "href": "workflow/03openalex.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "workflow/03openalex.html#openalex",
    "href": "workflow/03openalex.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction."
  },
  {
    "objectID": "workflow/04openalex.html",
    "href": "workflow/04openalex.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in OpenAlex, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. The OpenAlex journal-level dataset follows a similar approach to Scopus but differs in data availability and structure.\nOpenAlex links journals to their respective publications using linkage files, which connect publication records to dataset mentions. Unlike Scopus, OpenAlex assigns a single dataset identifier per dataset, while Scopus may have multiple identifiers for the same dataset depending on how it is referenced.\nAdditionally, OpenAlex formats journal and publication identifiers differently:\n\nISSNs appear without dashes\nDOIs exclude the URL portion, storing only the core identifier\n\nTo ensure consistency with Scopus, we clean and standardize the publication-level dataset, aligning DOIs and ISSNs to a common format.\nCAL TO DESCRIBE THIS PROCESS\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/04openalex.html#openalex",
    "href": "workflow/04openalex.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in OpenAlex, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. The OpenAlex journal-level dataset follows a similar approach to Scopus but differs in data availability and structure.\nOpenAlex links journals to their respective publications using linkage files, which connect publication records to dataset mentions. Unlike Scopus, OpenAlex assigns a single dataset identifier per dataset, while Scopus may have multiple identifiers for the same dataset depending on how it is referenced.\nAdditionally, OpenAlex formats journal and publication identifiers differently:\n\nISSNs appear without dashes\nDOIs exclude the URL portion, storing only the core identifier\n\nTo ensure consistency with Scopus, we clean and standardize the publication-level dataset, aligning DOIs and ISSNs to a common format.\nCAL TO DESCRIBE THIS PROCESS\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/04scopus.html",
    "href": "workflow/04scopus.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/04scopus.html#scopus",
    "href": "workflow/04scopus.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON\n\nReturn to Project Workflow"
  },
  {
    "objectID": "workflow/05openalex.html",
    "href": "workflow/05openalex.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Return to Project Workflow"
  },
  {
    "objectID": "workflow/05openalex.html#openalex",
    "href": "workflow/05openalex.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Return to Project Workflow"
  },
  {
    "objectID": "workflow/05scopus.html",
    "href": "workflow/05scopus.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Return to Project Workflow"
  },
  {
    "objectID": "workflow/05scopus.html#scopus",
    "href": "workflow/05scopus.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Return to Project Workflow"
  },
  {
    "objectID": "home.html",
    "href": "home.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "What Is the Issue?\nFederal agencies like the US Department of Agriculture (USDA) track how their datasets are referenced in research papers and disseminate data usage statistics through platforms like DemocratizingData.ai and NASS’s 5’s Data Usage Dashboard. The process of identifying dataset mentions in academic research output requires the use of citation databases. However, different databases curate content (i.e., research output) in different ways - some focus on peer-reviewed journals while others include preprints and technical reports. Tracking dataset usage requires developing methods that scan publication text for dataset mentions. The accuracy of dataset tracking depends on the scope of research output we can access and analyze. Not to mention, dataset tracking requires reliable citation data from citation databases.\nThis report presents a methodology for identifying dataset mentions in research publications across various citation databases. In doing so, we compare publication coverage, as well as authorship and institutional representation, across Scopus, OpenAlex, and Dimensions as primary sources.\nThe purpose here is to establish a consistent set of statistics for effectively comparing results and evaluating differences in dataset tracking across citation databases. This allows for clearer insights into how publication scope and indexing strategies influence dataset usage statistics.\n\n\nWhat Did the Study Find?\nMain Finding: Accurate dataset tracking relies heavily on how publications are indexed across citation databases. For two citation databases – Scopus and OpenAlex – carefully constructed seed corpora were needed to track dataset mentions.\nSpecific Results from Database Comparison:\n\nAfter deduplication, the number of distinct authors decreased by XX% in Scopus and XX% in OpenAlex, indicating significant duplicate entries in the raw data\nInstitutional coverage was broader in XX, with XX% more institutions represented compared to XX\nAnalysis revealed XX major themes in USDA dataset usage, with ?? and ?? being the most prominent\nMinority-Serving Institutions (MSIs) represented only XX% of institutional users, highlighting opportunities for broader engagement\n\n\n\nHow Was the Study Conducted?\nThe three citation databases we are comparing are Elsevier’s Scopus, OurResearch’s OpenAlex, and Digital Science’s Dimensions.ai. Scopus charges for access to its citation database. It focuses on peer-reviewed literature and provides metadata about authors, institutions, and citations for academic journals. OpenAlex, an open-source platform, offers free metadata access. It covers both traditional academic publications and other research outputs like preprints and technical reports. Dimensions, developed by Digital Science, offers a hybrid model that provides both free and subscription-based access to its citation database. Unlike Scopus, which primarily indexes peer-reviewed journal articles, and OpenAlex, which emphasizes open-access content, Dimensions aggregates a broad spectrum of research outputs, including journal articles, books, clinical trials, patents, datasets, and policy documents. It integrates citation data with funding information, making it a useful tool for assessing the impact of research beyond traditional academic publishing.\nOur methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. We developed procedures for:\n\nIdentifying publication coverage across citation databases\nCross-referencing publications between datasets\nDeduplicating publication records\nStandardizing author and institution names\nAnalyzing research themes and institutional representation\n\nThe methodology produced these reusable components:\n\nCode repository for data cleaning and standardization\nCleaned author tables with disambiguated names and institutional affiliations\nStandardized institution tables using IPEDS identifiers\nCrosswalk table structure linking Scopus and OpenAlex publication records, authors, and institutions\nData schema visualization [Last updated: in progress]\n\nThe methods described can be applied to evaluate other citation databases such as Web of Science, Crossref, and Microsoft Academic, to name a few.",
    "crumbs": [
      "Report Summary"
    ]
  },
  {
    "objectID": "workflow.html",
    "href": "workflow.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The project workflow outlines the steps involved to evaluate how different citation databases track USDA dataset mentions in research papers. In searching for dataset mentions, the goal is to identify a set of publications that can be compared across the citation database test cases.\n\nThe process of deriving the list of publications from a citation database consists of four steps. Each step produces an output which is used as the input for the following step:\n\n\n\nIdentify USDA datasets that will be searched for and tracked.\nCollect official dataset names along with common abbreviations, acronyms, and alternative references used.\n\nResult: A structured list of dataset names and aliases.\n\n\n\n\nConduct searches across citation databases using multiple methods:\n\nFull-Text (String) Search: Scan entire articles for relevant dataset names.\nReference Search: Identify dataset citations within publication references.\nMachine Learning Models: Apply Kaggle competition models trained to detect dataset mentions.\n\n\nNote: In cases where full-text search is not supported by the citation database API (e.g., Scopus), an initial seed corpus of publications was collected separately to train machine learning models. Refer to “Creating a Seed Corpus” for more details.\nResult: Publication dataset for each data asset across each citation database.\n\n\n\n\nPre-process and clean publication metadata generated from each citation database.\nStandardize journal, institution, and author names.\nDeduplicate records.\n\nResult: Cleaned publication metadata, removed of duplicates, inconsistencies, and missing information.\n\n\n\n\nCompare dataset coverage across Scopus, OpenAlex, and Dimensions.\nApply fuzzy matching techniques to identify overlapping and unique dataset mentions.\nAnalyze differences in journal coverage, citation patterns, and author affiliations.\n\nResult: A set of statistics used to evaluate dataset tracking accuracy.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#sec-data",
    "href": "workflow.html#sec-data",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to compile a structured list of dataset names and their commonly used variations.\n\n\n\n\n\n\n\nNote\n\n\n\nThe data assets featured here consist of those collected by the USDA, primarily from the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). These data assets are widely used in agricultural economics and food systems research.\n\n\n\n\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to database curated by Cornell University. These reports are part of the USDA’s efforts to track data usage across various research applications. However, the names of these reports were highly generic, making it difficult to precisely identify them in citation databases. Examples include reports titled “Agricultural Prices” and “Farm Labor,” which lack specificity when compared to more structured dataset identifiers.\nData Processing and Standardization\nTo improve identification and searchability, the input was analyzed and transformed into a structured list that included:\n\nInternational Standard Serial Numbers (ISSNs): Each of the 21 reports was assigned an ISSN, where available, to provide a standardized identifier.\nAlias Creation: Generic report names were appended with the term report to better distinguish them from other similarly named publications in research literature.\nExpanded Search Terms: Additional variations of dataset names were included to account for different citation styles and possible ways authors reference these reports.\n\nThe final dataset classification involved:\n\nMain Data Asset (Parent Record): The original 21 reports, each representing a distinct dataset.\nAliases: ISSNs and URLs served as aliases to improve retrieval accuracy.\nSearch Term Expansion: Combining report names with different citation formats led to a total of 64 search terms (21 parent records + 43 aliases).\n\nThis standardization process improved the efficiency of identifying NASS datasets across publications indexed by citation databases such as Scopus, OpenAlex, and Dimensions.\n\n\n\nThe process of identifying ERS data assets occurred in two phases: (1) an initial dataset compilation, and (2) a refinement process incorporating feedback from a team of agricultural economists at Colorado State University (CSU). This process was meant to yield a list of data assets was both comprehensive and relevant to the research community tracking USDA dataset usage.\nPhase 1: Initial Compilation of ERS Data Assets\nIn October 2023, an initial list of 2,103 ERS records was compiled. These records included dataset names and, in some cases, associated aliases. The list was then reviewed by Professor Julia Lane, who identified and removed 144 records that were not suitable for machine learning-based dataset tracking.\nReasons for Exclusion\n\nRecords were too generic – Terms such as “Milk, Cotton, and CSV Format of National Data” were too broad to be meaningfully identified in citation databases.\nRecords were too specific – Entries such as “Table 15—Agricultural Chemical Input” and “Southeast: 1982-91, 1992-97” were references within broader reports rather than standalone data assets.\n\nAfter these exclusions, the remaining 1,959 records represented the initial list of ERS data assets.\nPhase 2: Refinement with CSU Team\nA team of agricultural economists at CSU were consulted to refine the list so that it accurately captured key USDA datasets that may have been overlooked in the initial process. This involved:\n\nReviewing dataset usage in prior USDA research – Identifying which datasets were frequently cited.\nCross-checking with known data users – Ensuring that key datasets used by agricultural economists were included.\nExpanding alias definitions – Recognizing dataset acronyms and alternative naming conventions.\n\nAs part of this process, an additional set of assets was incorporated, including datasets that had been previously identified in the Year 1 USDA project. Notably, datasets such as the Census of Agriculture and the Agricultural Resource Management Survey (ARMS) were added, along with key acronyms like FoodAPS. This phase contributed:\n\n12 new parent records\n8 additional alias records\nTotal: 20 new search terms\n\nFinal Data Asset Identification\nUnlike NASS data assets, which had ISSNs and DOIs, ERS datasets were primarily linked through URLs. The final structured dataset included:\n\n1,959 parent records (main ERS datasets)\n1,959 alias records (URLs serving as dataset identifiers)\n20 additional records from the CSU consultation\nTotal: 3,918 search terms\n\nThrough this two-phase process, the list of ERS data assets evolved from an initial broad set of records into a refined, structured collection of datasets that could be effectively tracked across citation databases.\n\n\n\nThe data assets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. The final set of data assets, their producing agencies, and descriptions are presented in Table 1.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases.\n\n\n\n\n\n\n\n\nTo provide a comprehensive reference for dataset tracking, the following appendix includes a detailed list of data assets and their corresponding aliases, collectively referred to as dyads. Each dyad represents a dataset-name and alias pair used in citation database searches, allowing for more precise identification of dataset mentions in research publications. These aliases include acronyms, alternate spellings, dataset variations, and associated URLs, ensuring broad coverage across different citation practices. The dyad list serves as the foundation for dataset extraction and disambiguation across Scopus, OpenAlex, and Dimensions. A complete reference of these dataset aliases is included in Appendix XX.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#sec-data-idn",
    "href": "workflow.html#sec-data-idn",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to build a dataset of publications that reference the dataset name aliases for the USDA data assets (Table 1) across Scopus, OpenAlex, and Dimensions.\n\nTo generate this dataset, the process requires:\n\nDataset name aliases (from Step 1)\nSearch routines tailored to each citation database to extract relevant publications\n\nSearch routines, described below, guide this step, as dataset mentions are often inconsistent across publications—appearing in titles, abstracts, full text, or reference lists. Scopus uses a structured seed corpus to refine searches, while OpenAlex and Dimensions rely on direct queries across their full publication records. The outputs of this step are three publication-level datasets, one for each citation database, which will be further analyzed in subsequent steps.\n\nScopusOpenAlexDimensions\n\n\n\n\nThe search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 2:\n\n\n\nTable 2: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 3: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 4:\n\n\n\nTable 4: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\nReturn to Project Workflow\n\n\n\n\n\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to the Appendix for additional details on file construction.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#sec-seed-corpus",
    "href": "workflow.html#sec-seed-corpus",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "After defining the data assets and aliases in Step 1 (Section 1.1), the next step is to identify where these datasets are referenced in research publications. Unlike OpenAlex and Dimensions, which allow for direct full-text searches, Scopus requires a structured seed corpus to establish a more targeted search space.\nFor Scopus, this seed corpus approach was necessary to balance recall (capturing relevant mentions) and precision (minimizing false positives). The process involved:\n\nRestricted search strategies using reference lists and available full-text searches\nMachine learning-assisted review to refine dataset mentions\nManual refinements to resolve ambiguities, consolidate duplicate aliases, and incorporate missing terms.\n\nThis structured search space helped mitigate the constraints of Scopus’s API and ensured that dataset mentions were captured as comprehensively as possible before proceeding to the data identification step (Section 1.2).\n\nScopusOpenAlexDimensions\n\n\n\n\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 5: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\nReturn to Project Workflow\n\n\n\n\nThe seed corpus approach was only applied to Scopus.\n\n\nThe seed corpus approach was only applied to Scopus.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#sec-disambiguation",
    "href": "workflow.html#sec-disambiguation",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to standardize and resolve inconsistencies in publication records by disambiguating journal names, author affiliations, and institutions across the three databases.\n\nTo compare publication coverage across citation databases, we first identify all journals that contain publications using each dataset in Scopus, OpenAlex, and Dimensions. Table 1 displays a list of the datasets for which we evaluate coverage.\nWe subset the publication dataset from Step 2 by filtering for dataset mentions. For example, if a publication references Ag Census, it is included in the Ag Census sub-dataset; otherwise, it is excluded. This process identifies dataset-specific publication patterns in Scopus, OpenAlex, and Dimensions.\nOur approach follows a hierarchical approach to understand how USDA data assets appear in these citation databases.\n\nJournal Level – Identifies journals publishing research using USDA datasets. A journal is included if at least one of its publications references the dataset, but this does not indicate overall dataset prevalence within that journal.\nPublication Level – Examines individual publications within these journals to assess how often and in what context USDA datasets appear.\nAuthor Level – Tracks authors of these publications, analyzing institutional affiliations and research networks to understand dataset reach.\nInstitution Level – Maps dataset usage across institutions to identify geographic and organizational research patterns.\n\nThis structured approach standardizes dataset mention analysis across databases, allowing for direct comparisons of coverage and research impact.\n\n\n\nTo illustrate the data cleaning and disambiguation process, we use the Census of Agriculture as a case study to systematically compare coverage, overlap, and differences between the three citation databases. The Census of Agriculture (also referred to as “Ag Census”) is widely used in agricultural and economic research, making it an ideal dataset for assessing database differences.\n\n\n\nScopusOpenAlexDimensions\n\n\n\n\nTo analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON\n\nReturn to Project Workflow\n\n\n\n\n\n\nTo analyze journal coverage in OpenAlex, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. The OpenAlex journal-level dataset follows a similar approach to Scopus but differs in data availability and structure.\nOpenAlex links journals to their respective publications using linkage files, which connect publication records to dataset mentions. Unlike Scopus, OpenAlex assigns a single dataset identifier per dataset, while Scopus may have multiple identifiers for the same dataset depending on how it is referenced.\nAdditionally, OpenAlex formats journal and publication identifiers differently:\n\nISSNs appear without dashes\nDOIs exclude the URL portion, storing only the core identifier\n\nTo ensure consistency with Scopus, we clean and standardize the publication-level dataset, aligning DOIs and ISSNs to a common format.\nCAL TO DESCRIBE THIS PROCESS\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON\n\nReturn to Project Workflow\n\n\n\n\n\n\n\n\n\n\n\nThis section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset\nStep 4 produces two publication-level datasets: one of all academic papers released through Scopus that use Ag Census data and a similar one for OpenAlex.\nThere are 4712 unique publications reported in Scopus and 1266 in OpenAlex. These data are collapsed into a journal-level dataset based on the International Standard Serial Number (ISSN) that is unique to each academic journal.\n\nScopusOpenAlexDimensions",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#sec-matching",
    "href": "workflow.html#sec-matching",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to develop statistics that measure dataset tracking accuracy.\n\n\n\n\nContinuing with our case study, we use the datasets produced in Step 4 to produce counts of the number of journals with Ag Census publications that:\n\nonly appear in Scopus,\nonly appear in OpenAlex, or\nappear in both.\n\nFor journals that contain Ag Census data in both citation databases, we summarize the coverage of publications that appear in both Scopus and OpenAlex.\nThen, we investigate discrepancies based on factors like missing identifiers, mismatched journal information (ISSNs), and additional publications accessed through OpenAlex’s API.\nAdd here: What are the steps in producing Table AA\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow\n\n\n\n\n\n\n\n\n\n\nThis section presents results after matching (which type varies – deterministic vs fuzzy)\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Matching Methods\n\n\n\n\n\n\nRule-based matching for exact matches\nProbabilistic matching for handling variations\nMachine learning methods for complex cases\n\n\n\n\nTable 6: Summary of Methods\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nConsiderations\nExample\nPros\nCons\n\n\n\n\nSearching for dataset names within Scopus\n\n\n\n\n\n\nSearching for dataset names within OpenAlex\n“Location” field set to “journal”\n\n\n\n\n\nDisambiguation of authors\n\n\n\n\n\n\nDisambiguation of institutions\n\n\n\n\n\n\nStandardization of institutions\n\n\n\n\n\n\nSearching based on the frequency of dataset appearance in journals\n\n\n\n\n\n\nMORE . . .\n\n\n\n\n\n\nFiltering on keywords to determine themes",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#technical-recommendations",
    "href": "workflow.html#technical-recommendations",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.1 Technical Recommendations",
    "text": "2.1 Technical Recommendations",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#expanding-dataset-usage",
    "href": "workflow.html#expanding-dataset-usage",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "2.2 Expanding Dataset Usage",
    "text": "2.2 Expanding Dataset Usage\nThis report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.\nGiven the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#footnotes",
    "href": "workflow.html#footnotes",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "More Coming Soon",
    "crumbs": [
      "Findings"
    ]
  },
  {
    "objectID": "report.html#sec-matching",
    "href": "report.html#sec-matching",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to develop statistics that measure dataset tracking accuracy.\n\n\n\n\nContinuing with our case study, we use the datasets produced in Step 4 to produce counts of the number of journals with Ag Census publications that:\n\nonly appear in Scopus,\nonly appear in OpenAlex, or\nappear in both.\n\nFor journals that contain Ag Census data in both citation databases, we summarize the coverage of publications that appear in both Scopus and OpenAlex.\nThen, we investigate discrepancies based on factors like missing identifiers, mismatched journal information (ISSNs), and additional publications accessed through OpenAlex’s API.\nAdd here: What are the steps in producing Table AA\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow\n\n\n\n\n\n\n\n\n\n\n\n\n\nReturn to Project Workflow\n\n\n\n\n\n\n\n\n\n\nThis section presents results after matching (which type varies – deterministic vs fuzzy)\n\nScopusOpenAlexDimensions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Matching Methods\n\n\n\n\n\n\nRule-based matching for exact matches\nProbabilistic matching for handling variations\nMachine learning methods for complex cases\n\n\n\n\nTable 1: Summary of Methods\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nConsiderations\nExample\nPros\nCons\n\n\n\n\nSearching for dataset names within Scopus\n\n\n\n\n\n\nSearching for dataset names within OpenAlex\n“Location” field set to “journal”\n\n\n\n\n\nDisambiguation of authors\n\n\n\n\n\n\nDisambiguation of institutions\n\n\n\n\n\n\nStandardization of institutions\n\n\n\n\n\n\nSearching based on the frequency of dataset appearance in journals\n\n\n\n\n\n\nMORE . . .\n\n\n\n\n\n\nFiltering on keywords to determine themes",
    "crumbs": [
      "Findings"
    ]
  },
  {
    "objectID": "report.html#technical-recommendations",
    "href": "report.html#technical-recommendations",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "1.1 Technical Recommendations",
    "text": "1.1 Technical Recommendations",
    "crumbs": [
      "Findings"
    ]
  },
  {
    "objectID": "report.html#expanding-dataset-usage",
    "href": "report.html#expanding-dataset-usage",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "1.2 Expanding Dataset Usage",
    "text": "1.2 Expanding Dataset Usage\nThis report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.\nGiven the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database.",
    "crumbs": [
      "Findings"
    ]
  },
  {
    "objectID": "terminology.html",
    "href": "terminology.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Terminology\nCitation databases form the foundation of modern research tracking and analysis. Digital repositories, like the test cases featured in this report, systematically catalog scholarly publications and their references to each other (De Bellis, 2009). Citation databases differ in their approaches to curating and maintaining this information. Some focus exclusively on peer-reviewed journal articles with strict inclusion criteria, while others index a broader range of research outputs including preprints, technical reports, and conference proceedings (Martín-Martín et al., 2021; Mongeon & Paul-Hus, 2016). These curation approaches affect how comprehensively each database captures research impact (Visser et al., 2021).\nUnderstanding how these databases work requires familiarity with bibliometrics - the statistical analysis of published works and their impact (Broadus, 1987). Bibliometric analysis examines patterns in publication, citation networks, and research influence (Hood & Wilson, 2001). The field emerged from early citation indices, which mapped relationships between papers through their references (Garfield, 1955).\nFor tracking USDA dataset usage, these concepts directly apply. Accurate tracking of dataset usage in scientific literature serves multiple purposes. For federal agencies like the USDA, it helps monitor the return on public data investments, find gaps in dataset use, plan future data collection, and support evidence-based policy decisions. This tracking requires reliable citation data from citation databases. Unlike standard citations, researchers often reference datasets within the text of their publications rather than citing them formally. This makes tracking dataset usage more complex.\nTo solve this tracking challenge, methods have been developed that scan publication text for dataset mentions (Lane et al., 2022). The scope and accuracy of our dataset tracking depends on what publications we can access and analyze. Because different databases curate content in different ways, it creates variation in what dataset mentions they capture and their frequency. Variations in content across sources affect our ability to accurately track dataset impact and adoption. The DemocratizingData.ai platform, for example, uses bibliometric data to monitor these dataset usage patterns, helping USDA understand how its data supports research. By comparing how different citation databases track this information, we can better understand their strengths and limitations for monitoring research impact.\n\n\n\n\n\n\n\n\n1 References\n\nBroadus, R. N. (1987). Toward a definition of “bibliometrics.” Scientometrics, 12, 373–379. https://doi.org/10.1007/bf02016680\n\n\nDe Bellis, N. (2009). Bibliometrics and citation analysis: From the science citation index to cybermetrics. Scarecrow Press. https://doi.org/10.1002/asi.21181\n\n\nGarfield, E. (1955). Citation indexes for science: A new dimension in documentation through association of ideas. Science, 122(3159), 108–111. https://doi.org/10.1126/science.122.3159.108\n\n\nHood, W. W., & Wilson, C. S. (2001). The literature of bibliometrics, scientometrics, and informetrics. Scientometrics, 52, 291–314. https://doi.org/10.1023/A:1017919924342\n\n\nLane, J., Gimeno, E., Levitskaya, E., Zhang, Z., & Zigoni, A. (2022). Data inventories for the modern age? Using data science to open government data. Harvard Data Science Review, 4(2). https://doi.org/10.1162/99608f92.8a3f2336\n\n\nMartín-Martín, A., Thelwall, M., Orduna-Malea, E., & Delgado López-Cózar, E. (2021). Google scholar, microsoft academic, scopus, dimensions, web of science, and OpenCitations’ COCI: A multidisciplinary comparison of coverage via citations. Scientometrics, 126(1), 871–906. https://doi.org/10.1007/s11192-020-03690-4\n\n\nMongeon, P., & Paul-Hus, A. (2016). The journal coverage of web of science and scopus: A comparative analysis. Scientometrics, 106, 213–228. https://doi.org/10.1007/s11192-015-1765-5\n\n\nVisser, M., Van Eck, N. J., & Waltman, L. (2021). Large-scale comparison of bibliographic data sources: Scopus, web of science, dimensions, crossref, and microsoft academic. Quantitative Science Studies, 2(1), 20–41. https://doi.org/10.1162/qss_a_00112",
    "crumbs": [
      "Terminology"
    ]
  },
  {
    "objectID": "conclusions.html",
    "href": "conclusions.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "This report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.\nGiven the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database.",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "conclusions.html#expanding-dataset-usage",
    "href": "conclusions.html#expanding-dataset-usage",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Expanding Dataset Usage",
    "text": "Expanding Dataset Usage\nThis report analyzes USDA dataset usage patterns across both platforms and recommends specific strategies for expanding dataset use in underrepresented research communities.\nGiven the small percentage of MSI’s represented in our institutional analysis, it is evident that user engagement is central to increasing usage rates of the datasets, regardless of citation database.",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "appendices/app_crosswalk_data_id.html",
    "href": "appendices/app_crosswalk_data_id.html",
    "title": "Data Schema",
    "section": "",
    "text": "1 Data Schema\n\n\n2 Crosswalk Files"
  },
  {
    "objectID": "appendices/app_crosswalk.html",
    "href": "appendices/app_crosswalk.html",
    "title": "Data Schemas and File Inventory",
    "section": "",
    "text": "Scopus Schema\n\n\nOpenAlex Schema\n\n\nDimensions Schema\nNote: All schemas were built using DBDiagram.io.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory"
    ]
  },
  {
    "objectID": "workflow/step03/clean_pub_data.html",
    "href": "workflow/step03/clean_pub_data.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to standardize and resolve inconsistencies in publication records by disambiguating journal names, author affiliations, and institutions across the three databases.\n\nTo compare publication coverage across citation databases, we first identify all journals that contain publications using each dataset in Scopus, OpenAlex, and Dimensions. Refer to “Define Data Assets” for the list of the datasets for which we evaluate coverage.\nWe subset the publication dataset from Step 2 by filtering for dataset mentions. For example, if a publication references Ag Census, it is included in the Ag Census sub-dataset; otherwise, it is excluded. This process identifies dataset-specific publication patterns in Scopus, OpenAlex, and Dimensions.\nOur approach follows a hierarchical approach to understand how USDA data assets appear in these citation databases.\n\nJournal Level – Identifies journals publishing research using USDA datasets. A journal is included if at least one of its publications references the dataset, but this does not indicate overall dataset prevalence within that journal.\nPublication Level – Examines individual publications within these journals to assess how often and in what context USDA datasets appear.\nAuthor Level – Tracks authors of these publications, analyzing institutional affiliations and research networks to understand dataset reach.\nInstitution Level – Maps dataset usage across institutions to identify geographic and organizational research patterns.\n\nThis structured approach standardizes dataset mention analysis across databases, allowing for direct comparisons of coverage and research impact.\n\n\n\nTo illustrate the data cleaning and disambiguation process, we use the Census of Agriculture as a case study to systematically compare coverage, overlap, and differences between the three citation databases. The Census of Agriculture (also referred to as “Ag Census”) is widely used in agricultural and economic research, making it an ideal dataset for assessing database differences.\n\n\n\nScopus\n\n\n\n\nTo analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON\n\n\n\n\n\n\n\n\n\nThis section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset\nStep 4 produces two publication-level datasets: one of all academic papers released through Scopus that use Ag Census data and a similar one for OpenAlex.\nThere are 4712 unique publications reported in Scopus and 1266 in OpenAlex. These data are collapsed into a journal-level dataset based on the International Standard Serial Number (ISSN) that is unique to each academic journal.",
    "crumbs": [
      "Project Workflow",
      "Step 03: Clean Publication Data"
    ]
  },
  {
    "objectID": "workflow/step03/clean_pub_data.html#sec-disambiguation",
    "href": "workflow/step03/clean_pub_data.html#sec-disambiguation",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to standardize and resolve inconsistencies in publication records by disambiguating journal names, author affiliations, and institutions across the three databases.\n\nTo compare publication coverage across citation databases, we first identify all journals that contain publications using each dataset in Scopus, OpenAlex, and Dimensions. Refer to “Define Data Assets” for the list of the datasets for which we evaluate coverage.\nWe subset the publication dataset from Step 2 by filtering for dataset mentions. For example, if a publication references Ag Census, it is included in the Ag Census sub-dataset; otherwise, it is excluded. This process identifies dataset-specific publication patterns in Scopus, OpenAlex, and Dimensions.\nOur approach follows a hierarchical approach to understand how USDA data assets appear in these citation databases.\n\nJournal Level – Identifies journals publishing research using USDA datasets. A journal is included if at least one of its publications references the dataset, but this does not indicate overall dataset prevalence within that journal.\nPublication Level – Examines individual publications within these journals to assess how often and in what context USDA datasets appear.\nAuthor Level – Tracks authors of these publications, analyzing institutional affiliations and research networks to understand dataset reach.\nInstitution Level – Maps dataset usage across institutions to identify geographic and organizational research patterns.\n\nThis structured approach standardizes dataset mention analysis across databases, allowing for direct comparisons of coverage and research impact.\n\n\n\nTo illustrate the data cleaning and disambiguation process, we use the Census of Agriculture as a case study to systematically compare coverage, overlap, and differences between the three citation databases. The Census of Agriculture (also referred to as “Ag Census”) is widely used in agricultural and economic research, making it an ideal dataset for assessing database differences.\n\n\n\nScopus\n\n\n\n\nTo analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON\n\n\n\n\n\n\n\n\n\nThis section presents overall statistics for Scopus and OA. Each subsection will have results reported for each dataset\nStep 4 produces two publication-level datasets: one of all academic papers released through Scopus that use Ag Census data and a similar one for OpenAlex.\nThere are 4712 unique publications reported in Scopus and 1266 in OpenAlex. These data are collapsed into a journal-level dataset based on the International Standard Serial Number (ISSN) that is unique to each academic journal.",
    "crumbs": [
      "Project Workflow",
      "Step 03: Clean Publication Data"
    ]
  },
  {
    "objectID": "workflow/step02_02/create_seed_corpus.html",
    "href": "workflow/step02_02/create_seed_corpus.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "After defining the data assets and aliases in Step 1, the next step is to identify where these datasets are referenced in research publications. Unlike OpenAlex and Dimensions, which allow for direct full-text searches, Scopus requires a structured seed corpus to establish a more targeted search space.\nFor Scopus, this seed corpus approach was necessary to balance recall (capturing relevant mentions) and precision (minimizing false positives). The process involved:\n\nRestricted search strategies using reference lists and available full-text searches\nMachine learning-assisted review to refine dataset mentions\nManual refinements to resolve ambiguities, consolidate duplicate aliases, and incorporate missing terms.\n\nThis structured search space helped mitigate the constraints of Scopus’s API and ensured that dataset mentions were captured as comprehensively as possible before proceeding to the data identification step.\n\nScopusOpenAlexIntroductionDescription of the ProblemCreating the Seed CorpusJournalsAuthorsGenerating the Search CorpusCurrent Limitations and ConsiderationsNext StepsReferencesConclusionDimensions\n\n\n\n\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\n\n\n\n\n\n\nThis report describes the process of generating a Seed Corpus and Search Corpus in the context of the Democratizing Data project, specifically using OpenAlex as the publication catalog. The goal is to facilitate more accurate identification of USDA dataset mentions in scholarly publications by using local full-text searches.\n\n\nPublications identified by dataset mention searches using Scopus were cross-verified in OpenAlex by searching their DOIs. These publications were confirmed to be open-access and available for full-text searches in OpenAlex. However, upon closer investigation, two primary issues were identified with OpenAlex’s full-text indexing methods:\n\nPDF vs. NGRAMS Indexing Methods:\n\nPDF Method: OpenAlex receives the publication’s full text in PDF format and indexes the content directly.\nNGRAMS Method: OpenAlex receives from the author or publisher a preprocessed set of words or phrases (ngrams) extracted from the publication’s full text.\n\nSpecific Issues:\n\nPDF Method Issue: Although undocumented, we observed that the text from the references section of the publications was not indexed by OpenAlex. This occurs because OpenAlex processes the references section specifically to create pointers to other OpenAlex works being referenced. While this approach functions well for publications referencing other scholarly publications, it fails to identify dataset mentions in the references.\nNGRAMS Method Issue: The provided set of ngrams might not include all relevant words or phrases required for dataset identification. For example, if searching for a specific alias such as “USDA Census,” the provided ngrams might not contain the exact phrase or all necessary words, causing missed dataset mentions.\n\n\nThese limitations with OpenAlex’s indexing methods highlighted the need to create dedicated seed and search corpora for accurate dataset mention identification.\n\n\nThe seed corpus generation aims to create an effective subset of publications available in OpenAlex to download locally and subsequently run text searches for dataset aliases and flagged terms.\n\n\nTo define the seed corpus, several criteria were established based on topics, publication type, publication year, language, and open-access availability. Below are detailed descriptions of the chosen criteria and associated publication counts.\n\n\n\nWe identified relevant topics based on their frequency and relevance. Below are the top 100 topics by publication count:\n\n\n\n\n\n\n\n\n\nTopic ID\nTopic Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nT11610\nImpact of Food Insecurity on Health Outcomes\n313\n78661\n\n\nT11610\nFood Security and Health in Diverse Populations\n236\n78661\n\n\nT10010\nGlobal Trends in Obesity and Overweight Research\n149\n111686\n\n\nT11066\nComparative Analysis of Organic Agricultural Practices\n141\n41275\n\n\nT12253\nUrban Agriculture and Community Development\n140\n27383\n\n\nT10010\nObesity, Physical Activity, Diet\n123\n111686\n\n\nT10367\nAgricultural Innovation and Livelihood Diversification\n110\n49818\n\n\nT11066\nOrganic Food and Agriculture\n106\n41275\n\n\nT11464\nImpact of Homelessness on Health and Well-being\n101\n101019\n\n\nT12253\nUrban Agriculture and Sustainability\n82\n27383\n\n\nT12033\nEuropean Agricultural Policy and Reform\n77\n88980\n\n\nT10367\nAgricultural Innovations and Practices\n76\n49818\n\n\nT11464\nHomelessness and Social Issues\n74\n101019\n\n\nT10841\nDiscrete Choice Models in Economics and Health Care\n72\n66757\n\n\nT10596\nMaternal and Child Nutrition in Developing Countries\n71\n118727\n\n\nT11898\nImpacts of Food Prices on Consumption and Poverty\n70\n29110\n\n\nT11259\nSustainable Diets and Environmental Impact\n65\n45082\n\n\nT12033\nAgricultural Economics and Policy\n60\n88980\n\n\nT10841\nEconomic and Environmental Valuation\n54\n66757\n\n\nT10439\nAdaptation to Climate Change in Agriculture\n50\n27311\n\n\nT10235\nImpact of Social Factors on Health Outcomes\n49\n86076\n\n\nT10866\nRole of Mediterranean Diet in Health Outcomes\n48\n76894\n\n\nT10596\nChild Nutrition and Water Access\n45\n118727\n\n\nT11259\nAgriculture Sustainability and Environmental Impact\n44\n45082\n\n\nT10330\nHydrological Modeling and Water Resource Management\n43\n132216\n\n\nT11886\nRisk Management and Vulnerability in Agriculture\n43\n44755\n\n\nT11898\nEconomics of Agriculture and Food Markets\n43\n29110\n\n\nT11311\nSoil and Water Nutrient Dynamics\n42\n52847\n\n\nT11311\nBiogeochemical Cycling of Nutrients in Aquatic Ecosystems\n42\n52847\n\n\nT10226\nGlobal Analysis of Ecosystem Services and Land Use\n40\n84104\n\n\nT10969\nOptimal Operation of Water Resources Systems\n39\n97570\n\n\nT12732\nImpact of Farming on Health and Safety\n33\n29731\n\n\nT10235\nHealth disparities and outcomes\n32\n86076\n\n\nT10226\nLand Use and Ecosystem Services\n31\n84104\n\n\nT11753\nForest Management and Policy\n31\n75196\n\n\nT10969\nWater resources management and optimization\n31\n97570\n\n\nT12098\nRural development and sustainability\n30\n62114\n\n\nT12724\nIntegrated Management of Water, Energy, and Food Resources\n30\n40148\n\n\nT11886\nAgricultural risk and resilience\n30\n44755\n\n\nT11753\nClimate Change Impacts on Forest Carbon Sequestration\n29\n75196\n\n\nT11711\nImpacts of COVID-19 on Global Economy and Markets\n29\n69059\n\n\nT10111\nRemote Sensing in Vegetation Monitoring and Phenology\n28\n56452\n\n\nT11404\nDeficit Irrigation for Agricultural Water Management\n27\n49715\n\n\nT10439\nClimate change impacts on agriculture\n27\n27311\n\n\nT11862\nAgroecology and Global Food Systems\n26\n34753\n\n\nT12583\nFood Waste Management and Reduction\n26\n27144\n\n\nT10004\nSoil Carbon Dynamics and Nutrient Cycling in Ecosystems\n26\n101907\n\n\nT10330\nHydrology and Watershed Management Studies\n26\n132216\n\n\nT10556\nGlobal Cancer Incidence and Mortality Patterns\n25\n64063\n\n\nT12098\nRural Development and Change in Agricultural Landscapes\n24\n62114\n\n\nT10111\nRemote Sensing in Agriculture\n24\n56452\n\n\nT10556\nGlobal Cancer Incidence and Screening\n24\n64063\n\n\nT10866\nNutritional Studies and Diet\n22\n76894\n\n\nT11560\nDynamics of Livestock Disease Transmission and Control\n22\n68578\n\n\nT10266\nGlobal Forest Drought Response and Climate Change\n22\n73291\n\n\nT12904\nAgricultural Education and School Gardening Research\n21\n110210\n\n\nT12003\nDevelopment and Impacts of Bioenergy Crops\n21\n36853\n\n\nT10298\nInfluence of Built Environment on Active Travel\n21\n86890\n\n\nT10029\nClimate Change and Variability Research\n20\n113541\n\n\nT11711\nCOVID-19 Pandemic Impacts\n20\n69059\n\n\nT10266\nPlant Water Relations and Carbon Dynamics\n20\n73291\n\n\nT11544\nGender Inequality and Labor Force Dynamics\n19\n98755\n\n\nT13388\nFactors Affecting Sagebrush Ecosystems and Wildlife Conservation\n19\n58614\n\n\nT12904\nDiverse Educational Innovations Studies\n18\n110210\n\n\nT12773\nWater Quality and Hydrogeology Research\n18\n50724\n\n\nT10435\nEnvironmental Impact and Sustainability\n18\n55580\n\n\nT11862\nAgriculture, Land Use, Rural Development\n18\n34753\n\n\nT10435\nLife Cycle Assessment and Environmental Impact Analysis\n18\n55580\n\n\nT13393\nFeeding Disorders in Children with Autism Spectrum Disorders\n17\n50595\n\n\nT12724\nWater-Energy-Food Nexus Studies\n17\n40148\n\n\nT11404\nIrrigation Practices and Water Management\n17\n49715\n\n\nT11560\nAnimal Disease Management and Epidemiology\n17\n68578\n\n\nT13393\nChild Nutrition and Feeding Issues\n16\n50595\n\n\nT11544\nGender, Labor, and Family Dynamics\n16\n98755\n\n\nT10298\nUrban Transport and Accessibility\n15\n86890\n\n\nT11789\nLand Tenure and Property Rights in Agriculture\n15\n46627\n\n\nT10391\nEconomics of Health Care Systems and Policies\n15\n260472\n\n\nT10692\nImpact of Urban Green Space on Public Health\n15\n40686\n\n\nT10889\nSoil Erosion and Agricultural Sustainability\n15\n72441\n\n\nT10004\nSoil Carbon and Nitrogen Dynamics\n14\n101907\n\n\nT10446\nIncome, Poverty, and Inequality\n14\n62906\n\n\nT12057\nImpact of Ultra-Processed Foods on Health\n14\n28199\n\n\nT12873\nImpact of Nutrition and Eating Habits on Health\n14\n43157\n\n\nT11645\nEffects of Residential Segregation on Communities and Individuals\n14\n50639\n\n\nT12583\nFood Waste Reduction and Sustainability\n13\n27144\n\n\nT12310\nFactors Affecting Maize Yield and Lodging Resistance\n13\n105863\n\n\nT10889\nSoil erosion and sediment transport\n13\n72441\n\n\nT10487\nImpact of Pollinator Decline on Ecosystems and Agriculture\n13\n218697\n\n\nT10576\nOpioid Epidemic in the United States\n13\n50143\n\n\nT11186\nGlobal Drought Monitoring and Assessment\n13\n35695\n\n\nT10552\nGlobal Trends in Colorectal Cancer Research\n13\n70491\n\n\nT11925\nFood Tourism and Gastronomy Research\n13\n95356\n\n\nT12399\nFactors Influencing Wine Tourism and Consumer Behavior\n13\n50383\n\n\nT12003\nBioenergy crop production and management\n13\n36853\n\n\nT13388\nRangeland and Wildlife Management\n12\n58614\n\n\nT10410\nModeling the Dynamics of COVID-19 Pandemic\n12\n67192\n\n\nT10190\nHealth Effects of Air Pollution\n12\n125501\n\n\nT12733\nBluetongue Virus and Culicoides-Borne Diseases in Europe\n12\n34477\n\n\nT11546\nPlant Physiology and Cultivation Studies\n12\n189058\n\n\nT11552\nGovernance of Global Value Chains and Production Networks\n12\n46357\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 1,192,809.\n\n\n\nWe selected the top journals to further refine our corpus. The following table lists the top 100 journals by publication count:\n\n\n\n\n\n\n\n\n\nJournal ID\nJournal Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nS2764628096\nJournal of Agriculture Food Systems and Community Development\n57\n825\n\n\nS115427279\nPublic Health Nutrition\n51\n3282\n\n\nS206696595\nJournal of Nutrition Education and Behavior\n41\n3509\n\n\nS15239247\nInternational Journal of Environmental Research and Public Health\n39\n59130\n\n\nS4210201861\nApplied Economic Perspectives and Policy\n39\n647\n\n\nS10134376\nSustainability\n35\n87533\n\n\nS5832799\nJournal of Soil and Water Conservation\n34\n556\n\n\nS2739393555\nJournal of Agricultural and Applied Economics\n34\n329\n\n\nS202381698\nPLoS ONE\n30\n143568\n\n\nS124372222\nRenewable Agriculture and Food Systems\n30\n426\n\n\nS91754907\nAmerican Journal of Agricultural Economics\n28\n876\n\n\nS200437886\nBMC Public Health\n28\n18120\n\n\nS18733340\nJournal of the Academy of Nutrition and Dietetics\n27\n5301\n\n\nS78512408\nAgriculture and Human Values\n27\n938\n\n\nS2764593300\nAgricultural and Resource Economics Review\n25\n247\n\n\nS110785341\nNutrients\n25\n30911\n\n\nS4210212157\nFrontiers in Sustainable Food Systems\n23\n3776\n\n\nS69340840\nThe Journal of Rural Health\n20\n749\n\n\nS63571384\nFood Policy\n20\n1069\n\n\nS19383905\nAgricultural Finance Review\n18\n327\n\n\nS4210234824\nEDIS\n18\n3714\n\n\nS119228529\nJournal of Hunger & Environmental Nutrition\n17\n467\n\n\nS204691207\nHortTechnology\n14\n847\n\n\nS4210212179\nJournal of Extension\n14\n1004\n\n\nS43295729\nRemote Sensing\n14\n33899\n\n\nS2738397068\nLand\n14\n9774\n\n\nS80485027\nLand Use Policy\n14\n4559\n\n\nS4210217848\nJAMA Network Open\n13\n12933\n\n\nS139338987\nEnvironmental Research Letters\n13\n6399\n\n\nS2595931848\nFrontiers in Public Health\n12\n19316\n\n\nS122347013\nAmerican Journal of Obstetrics and Gynecology\n12\n15259\n\n\nS204847658\nWater Resources Research\n12\n5305\n\n\nS73449225\nFood Security\n12\n899\n\n\nS4210219560\nCurrent Developments in Nutrition\n12\n10807\n\n\nS183652945\nHortScience\n11\n2415\n\n\nS196734849\nScientific Reports\n11\n198095\n\n\nS139950591\nAgronomy Journal\n10\n2675\n\n\nS86852077\nThe Science of The Total Environment\n10\n56249\n\n\nS37976914\nJAWRA Journal of the American Water Resources Association\n9\n852\n\n\nS2764587901\nJournal of Applied Communications\n9\n250\n\n\nS2607323502\nScientific Data\n9\n5287\n\n\nS44455300\nJournal of Environmental Management\n9\n17835\n\n\nS4210220469\nJournal of Applied Farm Economics\n8\n39\n\n\nS141808269\nRemote Sensing of Environment\n8\n4135\n\n\nS129060628\nDiabetes\n8\n17439\n\n\nS2475403985\nPreventing Chronic Disease\n8\n1068\n\n\nS156283932\nCalifornia Agriculture\n8\n233\n\n\nS157560195\nAgricultural Systems\n8\n1722\n\n\nS136211407\nEcological Economics\n8\n2684\n\n\nS23642417\nSociety & Natural Resources\n8\n792\n\n\nS2574783\nGynecologic Oncology\n8\n8756\n\n\nS149285975\nLand Economics\n8\n399\n\n\nS2594976040\nFrontiers in Veterinary Science\n7\n9896\n\n\nS8391440\nCancer Epidemiology Biomarkers & Prevention\n7\n6099\n\n\nS6596815\nRural Sociology\n7\n467\n\n\nS4210180312\nJournal of the Agricultural and Applied Economics Association\n7\n161\n\n\nS4210202585\nAgriculture\n7\n9931\n\n\nS79054089\nBMJ Open\n7\n31973\n\n\nS2764832999\nScientific investigations report\n7\n1270\n\n\nS180723199\nAgribusiness\n7\n583\n\n\nS154775064\nAgricultural Economics\n7\n729\n\n\nS2738534743\nJournal of Nutritional Science\n6\n659\n\n\nS2754843627\nCancer Medicine\n6\n7317\n\n\nS2228914\nHealth Services Research\n6\n1855\n\n\nS2764680059\nStatistical Journal of the IAOS\n6\n852\n\n\nS76844451\nAnnual Review of Resource Economics\n6\n206\n\n\nS207068962\nCommunity Development\n6\n516\n\n\nS148307540\nEcology and Society\n6\n1281\n\n\nS4210194219\nAntarctica A Keystone in a Changing World\n6\n1110\n\n\nS4210186936\nJournal of Agricultural Science\n6\n2412\n\n\nS12132826\nThe International Food and Agribusiness Management Review\n6\n464\n\n\nS4210197466\nAgroecology and Sustainable Food Systems\n6\n645\n\n\nS204799461\nClimatic Change\n6\n1992\n\n\nS139838620\nObstetrics and Gynecology\n6\n8132\n\n\nS2596909297\nFrontiers in Nutrition\n6\n9700\n\n\nS168049282\nAmerican Journal of Public Health\n6\n4777\n\n\nS106822843\nSocial Science & Medicine\n5\n5847\n\n\nS134216166\nWater\n5\n25819\n\n\nS28036099\nJournal of Rural Studies\n5\n2034\n\n\nS2737313858\nAgricultural & Environmental Letters\n5\n262\n\n\nS32361082\nEuropean Review of Agricultural Economics\n5\n362\n\n\nS178566096\nPreventive Veterinary Medicine\n5\n1907\n\n\nS150168663\nCancer Causes & Control\n5\n1136\n\n\nS106908163\nNeuro-Oncology\n5\n18986\n\n\nS178182516\nJournal of Agromedicine\n5\n565\n\n\nS116775814\nComputers and Electronics in Agriculture\n5\n5965\n\n\nS176659572\nHealth & Social Care in the Community\n5\n2064\n\n\nS2764613780\nJournal of Agricultural Safety and Health\n5\n138\n\n\nS99400149\nJournal of Health Care for the Poor and Underserved\n5\n1197\n\n\nS42419699\nPrecision Agriculture\n5\n1130\n\n\nS130750583\nGlobal Environmental Change\n5\n1092\n\n\nS2492648963\nTransactions of the ASABE\n5\n894\n\n\nS135458494\nPlant Disease\n5\n8264\n\n\nS72684844\nJournal of Animal Science\n5\n15499\n\n\nS173554290\nJournal of Community Health\n5\n1126\n\n\nS28349394\nJournal of Dairy Science\n5\n8060\n\n\nS88153332\nJournal of Nutrition\n5\n3469\n\n\nS199825796\nApplied Engineering in Agriculture\n5\n722\n\n\nS95823145\nForest Policy and Economics\n5\n1509\n\n\nS104641133\nAgricultural Water Management\n5\n4298\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 770,522.\n\n\nAuthors with American affiliations were selected to enhance corpus relevance:\n\n\n\n\n\n\n\n\n\nAuthor ID\nAuthor Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nA5016803484\nHeather A. Eicher‐Miller\n15\n140\n\n\nA5024975191\nEdward A. Frongillo\n13\n351\n\n\nA5055158106\nBecca B.R. Jablonski\n12\n60\n\n\nA5047780964\nMeredith T. Niles\n11\n200\n\n\nA5015017711\nJeffrey K. O’Hara\n10\n27\n\n\nA5062679478\nJ. Gordon Arbuckle\n10\n68\n\n\nA5068812455\nCindy W. Leung\n10\n170\n\n\nA5076121862\nSheri D. Weiser\n10\n241\n\n\nA5081656928\nWhitney E. Zahnd\n9\n147\n\n\nA5008463933\nCatherine Brinkley\n8\n34\n\n\nA5027684365\nDayton M. Lambert\n8\n110\n\n\nA5002438645\nPhyllis C. Tien\n8\n244\n\n\nA5081012770\nLinda J. Young\n8\n51\n\n\nA5030548116\nMichele Ver Ploeg\n8\n33\n\n\nA5035584432\nAngela D. Liese\n8\n172\n\n\nA5032940306\nLisa Harnack\n7\n89\n\n\nA5008296893\nEryka Wentz\n7\n33\n\n\nA5006129622\nCarmen Byker Shanks\n7\n103\n\n\nA5053170901\nAni L. Katchova\n7\n62\n\n\nA5024127854\nEduardo Villamor\n7\n84\n\n\nA5060802257\nTracey E. Wilson\n7\n102\n\n\nA5050792105\nJennifer L. Moss\n7\n90\n\n\nA5040727809\nGeorge B. Frisvold\n7\n66\n\n\nA5056021318\nNathan Hendricks\n7\n320\n\n\nA5034750133\nLila A. Sheira\n7\n61\n\n\nA5044317355\nDaniel Merenstein\n7\n113\n\n\nA5002732604\nJulia A. Wolfson\n7\n137\n\n\nA5015455112\nHikaru Hanawa Peterson\n7\n56\n\n\nA5024248662\nAdebola Adedimeji\n7\n137\n\n\nA5038610136\nChristopher N. Boyer\n7\n115\n\n\nA5101813658\nChristian J. Peters\n7\n32\n\n\nA5035164673\nStephan J. Goetz\n6\n90\n\n\nA5029397288\nAmy L. Yaroch\n6\n113\n\n\nA5022651324\nSeth A. Berkowitz\n6\n158\n\n\nA5083470674\nMardge H. Cohen\n6\n205\n\n\nA5070284513\nTimothy S. Griffin\n6\n59\n\n\nA5026810637\nJoleen C. Hadrich\n6\n30\n\n\nA5013419936\nNigel Key\n6\n25\n\n\nA5062332393\nAlessandro Bonanno\n6\n61\n\n\nA5012666568\nHilary K. Seligman\n6\n123\n\n\nA5071854708\nBurton C. English\n6\n68\n\n\nA5069981543\nMegan Konar\n6\n86\n\n\nA5083406390\nZach Conrad\n6\n123\n\n\nA5074296013\nSuat Irmak\n6\n151\n\n\nA5079036202\nJames A. Larson\n6\n49\n\n\nA5038417176\nAdaora A. Adimora\n6\n334\n\n\nA5045489628\nSelena Ahmed\n6\n89\n\n\nA5057302432\nAlan W. Hodges\n6\n73\n\n\nA5091590760\nCraig Gundersen\n6\n69\n\n\nA5089578074\nParke Wilde\n6\n107\n\n\nA5063008522\nA. D. Kendall\n6\n119\n\n\nA5100771544\nHanqin Tian\n6\n434\n\n\nA5072286156\nD. W. Hyndman\n6\n130\n\n\nA5052456209\nKartika Palar\n6\n59\n\n\nA5042679164\nJeffrey Gillespie\n6\n30\n\n\nA5091103546\nKimberly L. Jensen\n6\n65\n\n\nA5014800024\nKartik K. Venkatesh\n5\n244\n\n\nA5003088939\nFrances Hardin‐Fanning\n5\n32\n\n\nA5028409673\nLauri M. Baker\n5\n60\n\n\nA5087431618\nGabrielle Roesch‐McNally\n5\n36\n\n\nA5112481717\nJianhong E. Mu\n5\n20\n\n\nA5067158518\nLisa R. Metsch\n5\n133\n\n\nA5051019392\nDawn Thilmany McFadden\n5\n20\n\n\nA5065308164\nEdward C. Jaenicke\n5\n59\n\n\nA5035062421\nKatherine Dentzman\n5\n35\n\n\nA5011693138\nRyan S. Miller\n5\n163\n\n\nA5019910416\nHolly Gibbs\n5\n69\n\n\nA5014991206\nMargarita Velandia\n5\n29\n\n\nA5081521085\nMark Lubell\n5\n80\n\n\nA5007931812\nTyler J. Lark\n5\n77\n\n\nA5078358162\nJanet M. Turan\n5\n189\n\n\nA5089582462\nLynn M. Yee\n5\n426\n\n\nA5040186224\nNathanael M. Thompson\n5\n32\n\n\nA5070695418\nIghovwerha Ofotokun\n5\n105\n\n\nA5018179894\nAmir M. Rahmani\n5\n264\n\n\nA5057015263\nDawn Thilmany\n5\n34\n\n\nA5038972534\nJyotsna S. Jagai\n5\n48\n\n\nA5003676504\nLandon Marston\n5\n84\n\n\nA5079390198\nChen Zhen\n5\n48\n\n\nA5043968039\nW. David Mulkey\n5\n7\n\n\nA5072793478\nClayton Hallman\n5\n11\n\n\nA5016915956\nJohn Tyndall\n5\n35\n\n\nA5044462604\nJohn M. Antle\n5\n49\n\n\nA5105267439\nColleen T. Webb\n5\n44\n\n\nA5016997673\nMiguel I. Gómez\n5\n138\n\n\nA5006518901\nAndrea Leschewski\n5\n18\n\n\nA5022734861\nAllison Bauman\n5\n32\n\n\nA5010819579\nLisa Chase\n5\n34\n\n\nA5044003446\nW. Jay Christian\n5\n50\n\n\nA5022220353\nBailey Houghtaling\n5\n72\n\n\nA5066919611\nRick Welsh\n5\n20\n\n\nA5053832932\nEric M. Clark\n4\n41\n\n\nA5002482027\nBenjamin M. Gramig\n4\n46\n\n\nA5029506929\nCourtney D. Lynch\n4\n84\n\n\nA5031318120\nJessica Rudnick\n4\n15\n\n\nA5046729938\nSteven R. Browning\n4\n23\n\n\nA5052396793\nLindsay M. Beck‐Johnson\n4\n13\n\n\nA5027592346\nDennis P. Swaney\n4\n26\n\n\nA5042166415\nDavid H. Fleisher\n4\n69\n\n\nA5008867112\nKatie Portacci\n4\n14\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 3,714.\n\n\nUpon applying the agreed filters, the final seed corpus resulted in 1,774,245 unique publications. An initial Python script was developed to collect full texts of these publications.\n\n\n\n\n\nMetric\nResult\n\n\n\n\nTotal publications attempted\n2,774\n\n\nSuccessfully downloaded full-texts\n974\n\n\nSuccess Rate\n35%\n\n\nEstimated full texts (projected)\n625,000 out of 1,774,245\n\n\nTotal estimated processing time\n~124 days\n\n\n\nThe relatively low success rate indicates significant challenges in accessing full texts, primarily due to missing or inaccessible OA URLs.\n\n\n\n\nOnly 35% success rate in downloading full texts.\nThe existing process is slow, computationally intensive, and likely to require improvements or distributed computing.\nComparison with OpenAlex’s built-in full-text search shows it might be sufficient in certain cases, potentially reducing the necessity of local processing.\n\n\n\n\nImplement distributed processing to accelerate corpus generation.\nAssess the adequacy of OpenAlex’s built-in full-text search for practical usage scenarios.\nBalance the need for accuracy with available resources (time and cost).\n\n\n\n\nOpenAlex API documentation: OpenAlex Works API\nOA filtered results: OpenAlex Filtered Corpus\nOpenAlex: https://docs.openalex.org\n\n\n\nCreating a locally processed seed corpus from OpenAlex significantly improves dataset mention accuracy but poses considerable resource demands. While local full-text processing enhances specificity, careful consideration is required regarding when OpenAlex’s native search capabilities are sufficient.\n\nReferences: - OpenAlex API Documentation: https://docs.openalex.org - Democratizing Data project repository and guidelines (internal documentation, 2025). - USDA Dataset Project Documentation (Internal Document, 2025).\n\n\nThe seed corpus approach was only applied to Scopus.",
    "crumbs": [
      "Project Workflow",
      "Creating a Seed Corpus"
    ]
  },
  {
    "objectID": "workflow/step02_02/create_seed_corpus.html#sec-seed-corpus",
    "href": "workflow/step02_02/create_seed_corpus.html#sec-seed-corpus",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "After defining the data assets and aliases in Step 1, the next step is to identify where these datasets are referenced in research publications. Unlike OpenAlex and Dimensions, which allow for direct full-text searches, Scopus requires a structured seed corpus to establish a more targeted search space.\nFor Scopus, this seed corpus approach was necessary to balance recall (capturing relevant mentions) and precision (minimizing false positives). The process involved:\n\nRestricted search strategies using reference lists and available full-text searches\nMachine learning-assisted review to refine dataset mentions\nManual refinements to resolve ambiguities, consolidate duplicate aliases, and incorporate missing terms.\n\nThis structured search space helped mitigate the constraints of Scopus’s API and ensured that dataset mentions were captured as comprehensively as possible before proceeding to the data identification step.\n\nScopusOpenAlexIntroductionDescription of the ProblemCreating the Seed CorpusJournalsAuthorsGenerating the Search CorpusCurrent Limitations and ConsiderationsNext StepsReferencesConclusionDimensions\n\n\n\n\nThere are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later.\n\n\n\n\n\n\n\n\nThis report describes the process of generating a Seed Corpus and Search Corpus in the context of the Democratizing Data project, specifically using OpenAlex as the publication catalog. The goal is to facilitate more accurate identification of USDA dataset mentions in scholarly publications by using local full-text searches.\n\n\nPublications identified by dataset mention searches using Scopus were cross-verified in OpenAlex by searching their DOIs. These publications were confirmed to be open-access and available for full-text searches in OpenAlex. However, upon closer investigation, two primary issues were identified with OpenAlex’s full-text indexing methods:\n\nPDF vs. NGRAMS Indexing Methods:\n\nPDF Method: OpenAlex receives the publication’s full text in PDF format and indexes the content directly.\nNGRAMS Method: OpenAlex receives from the author or publisher a preprocessed set of words or phrases (ngrams) extracted from the publication’s full text.\n\nSpecific Issues:\n\nPDF Method Issue: Although undocumented, we observed that the text from the references section of the publications was not indexed by OpenAlex. This occurs because OpenAlex processes the references section specifically to create pointers to other OpenAlex works being referenced. While this approach functions well for publications referencing other scholarly publications, it fails to identify dataset mentions in the references.\nNGRAMS Method Issue: The provided set of ngrams might not include all relevant words or phrases required for dataset identification. For example, if searching for a specific alias such as “USDA Census,” the provided ngrams might not contain the exact phrase or all necessary words, causing missed dataset mentions.\n\n\nThese limitations with OpenAlex’s indexing methods highlighted the need to create dedicated seed and search corpora for accurate dataset mention identification.\n\n\nThe seed corpus generation aims to create an effective subset of publications available in OpenAlex to download locally and subsequently run text searches for dataset aliases and flagged terms.\n\n\nTo define the seed corpus, several criteria were established based on topics, publication type, publication year, language, and open-access availability. Below are detailed descriptions of the chosen criteria and associated publication counts.\n\n\n\nWe identified relevant topics based on their frequency and relevance. Below are the top 100 topics by publication count:\n\n\n\n\n\n\n\n\n\nTopic ID\nTopic Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nT11610\nImpact of Food Insecurity on Health Outcomes\n313\n78661\n\n\nT11610\nFood Security and Health in Diverse Populations\n236\n78661\n\n\nT10010\nGlobal Trends in Obesity and Overweight Research\n149\n111686\n\n\nT11066\nComparative Analysis of Organic Agricultural Practices\n141\n41275\n\n\nT12253\nUrban Agriculture and Community Development\n140\n27383\n\n\nT10010\nObesity, Physical Activity, Diet\n123\n111686\n\n\nT10367\nAgricultural Innovation and Livelihood Diversification\n110\n49818\n\n\nT11066\nOrganic Food and Agriculture\n106\n41275\n\n\nT11464\nImpact of Homelessness on Health and Well-being\n101\n101019\n\n\nT12253\nUrban Agriculture and Sustainability\n82\n27383\n\n\nT12033\nEuropean Agricultural Policy and Reform\n77\n88980\n\n\nT10367\nAgricultural Innovations and Practices\n76\n49818\n\n\nT11464\nHomelessness and Social Issues\n74\n101019\n\n\nT10841\nDiscrete Choice Models in Economics and Health Care\n72\n66757\n\n\nT10596\nMaternal and Child Nutrition in Developing Countries\n71\n118727\n\n\nT11898\nImpacts of Food Prices on Consumption and Poverty\n70\n29110\n\n\nT11259\nSustainable Diets and Environmental Impact\n65\n45082\n\n\nT12033\nAgricultural Economics and Policy\n60\n88980\n\n\nT10841\nEconomic and Environmental Valuation\n54\n66757\n\n\nT10439\nAdaptation to Climate Change in Agriculture\n50\n27311\n\n\nT10235\nImpact of Social Factors on Health Outcomes\n49\n86076\n\n\nT10866\nRole of Mediterranean Diet in Health Outcomes\n48\n76894\n\n\nT10596\nChild Nutrition and Water Access\n45\n118727\n\n\nT11259\nAgriculture Sustainability and Environmental Impact\n44\n45082\n\n\nT10330\nHydrological Modeling and Water Resource Management\n43\n132216\n\n\nT11886\nRisk Management and Vulnerability in Agriculture\n43\n44755\n\n\nT11898\nEconomics of Agriculture and Food Markets\n43\n29110\n\n\nT11311\nSoil and Water Nutrient Dynamics\n42\n52847\n\n\nT11311\nBiogeochemical Cycling of Nutrients in Aquatic Ecosystems\n42\n52847\n\n\nT10226\nGlobal Analysis of Ecosystem Services and Land Use\n40\n84104\n\n\nT10969\nOptimal Operation of Water Resources Systems\n39\n97570\n\n\nT12732\nImpact of Farming on Health and Safety\n33\n29731\n\n\nT10235\nHealth disparities and outcomes\n32\n86076\n\n\nT10226\nLand Use and Ecosystem Services\n31\n84104\n\n\nT11753\nForest Management and Policy\n31\n75196\n\n\nT10969\nWater resources management and optimization\n31\n97570\n\n\nT12098\nRural development and sustainability\n30\n62114\n\n\nT12724\nIntegrated Management of Water, Energy, and Food Resources\n30\n40148\n\n\nT11886\nAgricultural risk and resilience\n30\n44755\n\n\nT11753\nClimate Change Impacts on Forest Carbon Sequestration\n29\n75196\n\n\nT11711\nImpacts of COVID-19 on Global Economy and Markets\n29\n69059\n\n\nT10111\nRemote Sensing in Vegetation Monitoring and Phenology\n28\n56452\n\n\nT11404\nDeficit Irrigation for Agricultural Water Management\n27\n49715\n\n\nT10439\nClimate change impacts on agriculture\n27\n27311\n\n\nT11862\nAgroecology and Global Food Systems\n26\n34753\n\n\nT12583\nFood Waste Management and Reduction\n26\n27144\n\n\nT10004\nSoil Carbon Dynamics and Nutrient Cycling in Ecosystems\n26\n101907\n\n\nT10330\nHydrology and Watershed Management Studies\n26\n132216\n\n\nT10556\nGlobal Cancer Incidence and Mortality Patterns\n25\n64063\n\n\nT12098\nRural Development and Change in Agricultural Landscapes\n24\n62114\n\n\nT10111\nRemote Sensing in Agriculture\n24\n56452\n\n\nT10556\nGlobal Cancer Incidence and Screening\n24\n64063\n\n\nT10866\nNutritional Studies and Diet\n22\n76894\n\n\nT11560\nDynamics of Livestock Disease Transmission and Control\n22\n68578\n\n\nT10266\nGlobal Forest Drought Response and Climate Change\n22\n73291\n\n\nT12904\nAgricultural Education and School Gardening Research\n21\n110210\n\n\nT12003\nDevelopment and Impacts of Bioenergy Crops\n21\n36853\n\n\nT10298\nInfluence of Built Environment on Active Travel\n21\n86890\n\n\nT10029\nClimate Change and Variability Research\n20\n113541\n\n\nT11711\nCOVID-19 Pandemic Impacts\n20\n69059\n\n\nT10266\nPlant Water Relations and Carbon Dynamics\n20\n73291\n\n\nT11544\nGender Inequality and Labor Force Dynamics\n19\n98755\n\n\nT13388\nFactors Affecting Sagebrush Ecosystems and Wildlife Conservation\n19\n58614\n\n\nT12904\nDiverse Educational Innovations Studies\n18\n110210\n\n\nT12773\nWater Quality and Hydrogeology Research\n18\n50724\n\n\nT10435\nEnvironmental Impact and Sustainability\n18\n55580\n\n\nT11862\nAgriculture, Land Use, Rural Development\n18\n34753\n\n\nT10435\nLife Cycle Assessment and Environmental Impact Analysis\n18\n55580\n\n\nT13393\nFeeding Disorders in Children with Autism Spectrum Disorders\n17\n50595\n\n\nT12724\nWater-Energy-Food Nexus Studies\n17\n40148\n\n\nT11404\nIrrigation Practices and Water Management\n17\n49715\n\n\nT11560\nAnimal Disease Management and Epidemiology\n17\n68578\n\n\nT13393\nChild Nutrition and Feeding Issues\n16\n50595\n\n\nT11544\nGender, Labor, and Family Dynamics\n16\n98755\n\n\nT10298\nUrban Transport and Accessibility\n15\n86890\n\n\nT11789\nLand Tenure and Property Rights in Agriculture\n15\n46627\n\n\nT10391\nEconomics of Health Care Systems and Policies\n15\n260472\n\n\nT10692\nImpact of Urban Green Space on Public Health\n15\n40686\n\n\nT10889\nSoil Erosion and Agricultural Sustainability\n15\n72441\n\n\nT10004\nSoil Carbon and Nitrogen Dynamics\n14\n101907\n\n\nT10446\nIncome, Poverty, and Inequality\n14\n62906\n\n\nT12057\nImpact of Ultra-Processed Foods on Health\n14\n28199\n\n\nT12873\nImpact of Nutrition and Eating Habits on Health\n14\n43157\n\n\nT11645\nEffects of Residential Segregation on Communities and Individuals\n14\n50639\n\n\nT12583\nFood Waste Reduction and Sustainability\n13\n27144\n\n\nT12310\nFactors Affecting Maize Yield and Lodging Resistance\n13\n105863\n\n\nT10889\nSoil erosion and sediment transport\n13\n72441\n\n\nT10487\nImpact of Pollinator Decline on Ecosystems and Agriculture\n13\n218697\n\n\nT10576\nOpioid Epidemic in the United States\n13\n50143\n\n\nT11186\nGlobal Drought Monitoring and Assessment\n13\n35695\n\n\nT10552\nGlobal Trends in Colorectal Cancer Research\n13\n70491\n\n\nT11925\nFood Tourism and Gastronomy Research\n13\n95356\n\n\nT12399\nFactors Influencing Wine Tourism and Consumer Behavior\n13\n50383\n\n\nT12003\nBioenergy crop production and management\n13\n36853\n\n\nT13388\nRangeland and Wildlife Management\n12\n58614\n\n\nT10410\nModeling the Dynamics of COVID-19 Pandemic\n12\n67192\n\n\nT10190\nHealth Effects of Air Pollution\n12\n125501\n\n\nT12733\nBluetongue Virus and Culicoides-Borne Diseases in Europe\n12\n34477\n\n\nT11546\nPlant Physiology and Cultivation Studies\n12\n189058\n\n\nT11552\nGovernance of Global Value Chains and Production Networks\n12\n46357\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 1,192,809.\n\n\n\nWe selected the top journals to further refine our corpus. The following table lists the top 100 journals by publication count:\n\n\n\n\n\n\n\n\n\nJournal ID\nJournal Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nS2764628096\nJournal of Agriculture Food Systems and Community Development\n57\n825\n\n\nS115427279\nPublic Health Nutrition\n51\n3282\n\n\nS206696595\nJournal of Nutrition Education and Behavior\n41\n3509\n\n\nS15239247\nInternational Journal of Environmental Research and Public Health\n39\n59130\n\n\nS4210201861\nApplied Economic Perspectives and Policy\n39\n647\n\n\nS10134376\nSustainability\n35\n87533\n\n\nS5832799\nJournal of Soil and Water Conservation\n34\n556\n\n\nS2739393555\nJournal of Agricultural and Applied Economics\n34\n329\n\n\nS202381698\nPLoS ONE\n30\n143568\n\n\nS124372222\nRenewable Agriculture and Food Systems\n30\n426\n\n\nS91754907\nAmerican Journal of Agricultural Economics\n28\n876\n\n\nS200437886\nBMC Public Health\n28\n18120\n\n\nS18733340\nJournal of the Academy of Nutrition and Dietetics\n27\n5301\n\n\nS78512408\nAgriculture and Human Values\n27\n938\n\n\nS2764593300\nAgricultural and Resource Economics Review\n25\n247\n\n\nS110785341\nNutrients\n25\n30911\n\n\nS4210212157\nFrontiers in Sustainable Food Systems\n23\n3776\n\n\nS69340840\nThe Journal of Rural Health\n20\n749\n\n\nS63571384\nFood Policy\n20\n1069\n\n\nS19383905\nAgricultural Finance Review\n18\n327\n\n\nS4210234824\nEDIS\n18\n3714\n\n\nS119228529\nJournal of Hunger & Environmental Nutrition\n17\n467\n\n\nS204691207\nHortTechnology\n14\n847\n\n\nS4210212179\nJournal of Extension\n14\n1004\n\n\nS43295729\nRemote Sensing\n14\n33899\n\n\nS2738397068\nLand\n14\n9774\n\n\nS80485027\nLand Use Policy\n14\n4559\n\n\nS4210217848\nJAMA Network Open\n13\n12933\n\n\nS139338987\nEnvironmental Research Letters\n13\n6399\n\n\nS2595931848\nFrontiers in Public Health\n12\n19316\n\n\nS122347013\nAmerican Journal of Obstetrics and Gynecology\n12\n15259\n\n\nS204847658\nWater Resources Research\n12\n5305\n\n\nS73449225\nFood Security\n12\n899\n\n\nS4210219560\nCurrent Developments in Nutrition\n12\n10807\n\n\nS183652945\nHortScience\n11\n2415\n\n\nS196734849\nScientific Reports\n11\n198095\n\n\nS139950591\nAgronomy Journal\n10\n2675\n\n\nS86852077\nThe Science of The Total Environment\n10\n56249\n\n\nS37976914\nJAWRA Journal of the American Water Resources Association\n9\n852\n\n\nS2764587901\nJournal of Applied Communications\n9\n250\n\n\nS2607323502\nScientific Data\n9\n5287\n\n\nS44455300\nJournal of Environmental Management\n9\n17835\n\n\nS4210220469\nJournal of Applied Farm Economics\n8\n39\n\n\nS141808269\nRemote Sensing of Environment\n8\n4135\n\n\nS129060628\nDiabetes\n8\n17439\n\n\nS2475403985\nPreventing Chronic Disease\n8\n1068\n\n\nS156283932\nCalifornia Agriculture\n8\n233\n\n\nS157560195\nAgricultural Systems\n8\n1722\n\n\nS136211407\nEcological Economics\n8\n2684\n\n\nS23642417\nSociety & Natural Resources\n8\n792\n\n\nS2574783\nGynecologic Oncology\n8\n8756\n\n\nS149285975\nLand Economics\n8\n399\n\n\nS2594976040\nFrontiers in Veterinary Science\n7\n9896\n\n\nS8391440\nCancer Epidemiology Biomarkers & Prevention\n7\n6099\n\n\nS6596815\nRural Sociology\n7\n467\n\n\nS4210180312\nJournal of the Agricultural and Applied Economics Association\n7\n161\n\n\nS4210202585\nAgriculture\n7\n9931\n\n\nS79054089\nBMJ Open\n7\n31973\n\n\nS2764832999\nScientific investigations report\n7\n1270\n\n\nS180723199\nAgribusiness\n7\n583\n\n\nS154775064\nAgricultural Economics\n7\n729\n\n\nS2738534743\nJournal of Nutritional Science\n6\n659\n\n\nS2754843627\nCancer Medicine\n6\n7317\n\n\nS2228914\nHealth Services Research\n6\n1855\n\n\nS2764680059\nStatistical Journal of the IAOS\n6\n852\n\n\nS76844451\nAnnual Review of Resource Economics\n6\n206\n\n\nS207068962\nCommunity Development\n6\n516\n\n\nS148307540\nEcology and Society\n6\n1281\n\n\nS4210194219\nAntarctica A Keystone in a Changing World\n6\n1110\n\n\nS4210186936\nJournal of Agricultural Science\n6\n2412\n\n\nS12132826\nThe International Food and Agribusiness Management Review\n6\n464\n\n\nS4210197466\nAgroecology and Sustainable Food Systems\n6\n645\n\n\nS204799461\nClimatic Change\n6\n1992\n\n\nS139838620\nObstetrics and Gynecology\n6\n8132\n\n\nS2596909297\nFrontiers in Nutrition\n6\n9700\n\n\nS168049282\nAmerican Journal of Public Health\n6\n4777\n\n\nS106822843\nSocial Science & Medicine\n5\n5847\n\n\nS134216166\nWater\n5\n25819\n\n\nS28036099\nJournal of Rural Studies\n5\n2034\n\n\nS2737313858\nAgricultural & Environmental Letters\n5\n262\n\n\nS32361082\nEuropean Review of Agricultural Economics\n5\n362\n\n\nS178566096\nPreventive Veterinary Medicine\n5\n1907\n\n\nS150168663\nCancer Causes & Control\n5\n1136\n\n\nS106908163\nNeuro-Oncology\n5\n18986\n\n\nS178182516\nJournal of Agromedicine\n5\n565\n\n\nS116775814\nComputers and Electronics in Agriculture\n5\n5965\n\n\nS176659572\nHealth & Social Care in the Community\n5\n2064\n\n\nS2764613780\nJournal of Agricultural Safety and Health\n5\n138\n\n\nS99400149\nJournal of Health Care for the Poor and Underserved\n5\n1197\n\n\nS42419699\nPrecision Agriculture\n5\n1130\n\n\nS130750583\nGlobal Environmental Change\n5\n1092\n\n\nS2492648963\nTransactions of the ASABE\n5\n894\n\n\nS135458494\nPlant Disease\n5\n8264\n\n\nS72684844\nJournal of Animal Science\n5\n15499\n\n\nS173554290\nJournal of Community Health\n5\n1126\n\n\nS28349394\nJournal of Dairy Science\n5\n8060\n\n\nS88153332\nJournal of Nutrition\n5\n3469\n\n\nS199825796\nApplied Engineering in Agriculture\n5\n722\n\n\nS95823145\nForest Policy and Economics\n5\n1509\n\n\nS104641133\nAgricultural Water Management\n5\n4298\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 770,522.\n\n\nAuthors with American affiliations were selected to enhance corpus relevance:\n\n\n\n\n\n\n\n\n\nAuthor ID\nAuthor Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nA5016803484\nHeather A. Eicher‐Miller\n15\n140\n\n\nA5024975191\nEdward A. Frongillo\n13\n351\n\n\nA5055158106\nBecca B.R. Jablonski\n12\n60\n\n\nA5047780964\nMeredith T. Niles\n11\n200\n\n\nA5015017711\nJeffrey K. O’Hara\n10\n27\n\n\nA5062679478\nJ. Gordon Arbuckle\n10\n68\n\n\nA5068812455\nCindy W. Leung\n10\n170\n\n\nA5076121862\nSheri D. Weiser\n10\n241\n\n\nA5081656928\nWhitney E. Zahnd\n9\n147\n\n\nA5008463933\nCatherine Brinkley\n8\n34\n\n\nA5027684365\nDayton M. Lambert\n8\n110\n\n\nA5002438645\nPhyllis C. Tien\n8\n244\n\n\nA5081012770\nLinda J. Young\n8\n51\n\n\nA5030548116\nMichele Ver Ploeg\n8\n33\n\n\nA5035584432\nAngela D. Liese\n8\n172\n\n\nA5032940306\nLisa Harnack\n7\n89\n\n\nA5008296893\nEryka Wentz\n7\n33\n\n\nA5006129622\nCarmen Byker Shanks\n7\n103\n\n\nA5053170901\nAni L. Katchova\n7\n62\n\n\nA5024127854\nEduardo Villamor\n7\n84\n\n\nA5060802257\nTracey E. Wilson\n7\n102\n\n\nA5050792105\nJennifer L. Moss\n7\n90\n\n\nA5040727809\nGeorge B. Frisvold\n7\n66\n\n\nA5056021318\nNathan Hendricks\n7\n320\n\n\nA5034750133\nLila A. Sheira\n7\n61\n\n\nA5044317355\nDaniel Merenstein\n7\n113\n\n\nA5002732604\nJulia A. Wolfson\n7\n137\n\n\nA5015455112\nHikaru Hanawa Peterson\n7\n56\n\n\nA5024248662\nAdebola Adedimeji\n7\n137\n\n\nA5038610136\nChristopher N. Boyer\n7\n115\n\n\nA5101813658\nChristian J. Peters\n7\n32\n\n\nA5035164673\nStephan J. Goetz\n6\n90\n\n\nA5029397288\nAmy L. Yaroch\n6\n113\n\n\nA5022651324\nSeth A. Berkowitz\n6\n158\n\n\nA5083470674\nMardge H. Cohen\n6\n205\n\n\nA5070284513\nTimothy S. Griffin\n6\n59\n\n\nA5026810637\nJoleen C. Hadrich\n6\n30\n\n\nA5013419936\nNigel Key\n6\n25\n\n\nA5062332393\nAlessandro Bonanno\n6\n61\n\n\nA5012666568\nHilary K. Seligman\n6\n123\n\n\nA5071854708\nBurton C. English\n6\n68\n\n\nA5069981543\nMegan Konar\n6\n86\n\n\nA5083406390\nZach Conrad\n6\n123\n\n\nA5074296013\nSuat Irmak\n6\n151\n\n\nA5079036202\nJames A. Larson\n6\n49\n\n\nA5038417176\nAdaora A. Adimora\n6\n334\n\n\nA5045489628\nSelena Ahmed\n6\n89\n\n\nA5057302432\nAlan W. Hodges\n6\n73\n\n\nA5091590760\nCraig Gundersen\n6\n69\n\n\nA5089578074\nParke Wilde\n6\n107\n\n\nA5063008522\nA. D. Kendall\n6\n119\n\n\nA5100771544\nHanqin Tian\n6\n434\n\n\nA5072286156\nD. W. Hyndman\n6\n130\n\n\nA5052456209\nKartika Palar\n6\n59\n\n\nA5042679164\nJeffrey Gillespie\n6\n30\n\n\nA5091103546\nKimberly L. Jensen\n6\n65\n\n\nA5014800024\nKartik K. Venkatesh\n5\n244\n\n\nA5003088939\nFrances Hardin‐Fanning\n5\n32\n\n\nA5028409673\nLauri M. Baker\n5\n60\n\n\nA5087431618\nGabrielle Roesch‐McNally\n5\n36\n\n\nA5112481717\nJianhong E. Mu\n5\n20\n\n\nA5067158518\nLisa R. Metsch\n5\n133\n\n\nA5051019392\nDawn Thilmany McFadden\n5\n20\n\n\nA5065308164\nEdward C. Jaenicke\n5\n59\n\n\nA5035062421\nKatherine Dentzman\n5\n35\n\n\nA5011693138\nRyan S. Miller\n5\n163\n\n\nA5019910416\nHolly Gibbs\n5\n69\n\n\nA5014991206\nMargarita Velandia\n5\n29\n\n\nA5081521085\nMark Lubell\n5\n80\n\n\nA5007931812\nTyler J. Lark\n5\n77\n\n\nA5078358162\nJanet M. Turan\n5\n189\n\n\nA5089582462\nLynn M. Yee\n5\n426\n\n\nA5040186224\nNathanael M. Thompson\n5\n32\n\n\nA5070695418\nIghovwerha Ofotokun\n5\n105\n\n\nA5018179894\nAmir M. Rahmani\n5\n264\n\n\nA5057015263\nDawn Thilmany\n5\n34\n\n\nA5038972534\nJyotsna S. Jagai\n5\n48\n\n\nA5003676504\nLandon Marston\n5\n84\n\n\nA5079390198\nChen Zhen\n5\n48\n\n\nA5043968039\nW. David Mulkey\n5\n7\n\n\nA5072793478\nClayton Hallman\n5\n11\n\n\nA5016915956\nJohn Tyndall\n5\n35\n\n\nA5044462604\nJohn M. Antle\n5\n49\n\n\nA5105267439\nColleen T. Webb\n5\n44\n\n\nA5016997673\nMiguel I. Gómez\n5\n138\n\n\nA5006518901\nAndrea Leschewski\n5\n18\n\n\nA5022734861\nAllison Bauman\n5\n32\n\n\nA5010819579\nLisa Chase\n5\n34\n\n\nA5044003446\nW. Jay Christian\n5\n50\n\n\nA5022220353\nBailey Houghtaling\n5\n72\n\n\nA5066919611\nRick Welsh\n5\n20\n\n\nA5053832932\nEric M. Clark\n4\n41\n\n\nA5002482027\nBenjamin M. Gramig\n4\n46\n\n\nA5029506929\nCourtney D. Lynch\n4\n84\n\n\nA5031318120\nJessica Rudnick\n4\n15\n\n\nA5046729938\nSteven R. Browning\n4\n23\n\n\nA5052396793\nLindsay M. Beck‐Johnson\n4\n13\n\n\nA5027592346\nDennis P. Swaney\n4\n26\n\n\nA5042166415\nDavid H. Fleisher\n4\n69\n\n\nA5008867112\nKatie Portacci\n4\n14\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 3,714.\n\n\nUpon applying the agreed filters, the final seed corpus resulted in 1,774,245 unique publications. An initial Python script was developed to collect full texts of these publications.\n\n\n\n\n\nMetric\nResult\n\n\n\n\nTotal publications attempted\n2,774\n\n\nSuccessfully downloaded full-texts\n974\n\n\nSuccess Rate\n35%\n\n\nEstimated full texts (projected)\n625,000 out of 1,774,245\n\n\nTotal estimated processing time\n~124 days\n\n\n\nThe relatively low success rate indicates significant challenges in accessing full texts, primarily due to missing or inaccessible OA URLs.\n\n\n\n\nOnly 35% success rate in downloading full texts.\nThe existing process is slow, computationally intensive, and likely to require improvements or distributed computing.\nComparison with OpenAlex’s built-in full-text search shows it might be sufficient in certain cases, potentially reducing the necessity of local processing.\n\n\n\n\nImplement distributed processing to accelerate corpus generation.\nAssess the adequacy of OpenAlex’s built-in full-text search for practical usage scenarios.\nBalance the need for accuracy with available resources (time and cost).\n\n\n\n\nOpenAlex API documentation: OpenAlex Works API\nOA filtered results: OpenAlex Filtered Corpus\nOpenAlex: https://docs.openalex.org\n\n\n\nCreating a locally processed seed corpus from OpenAlex significantly improves dataset mention accuracy but poses considerable resource demands. While local full-text processing enhances specificity, careful consideration is required regarding when OpenAlex’s native search capabilities are sufficient.\n\nReferences: - OpenAlex API Documentation: https://docs.openalex.org - Democratizing Data project repository and guidelines (internal documentation, 2025). - USDA Dataset Project Documentation (Internal Document, 2025).\n\n\nThe seed corpus approach was only applied to Scopus.",
    "crumbs": [
      "Project Workflow",
      "Creating a Seed Corpus"
    ]
  },
  {
    "objectID": "workflow/step02_02/02openalex.html",
    "href": "workflow/step02_02/02openalex.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "This report describes the process of generating a Seed Corpus and Search Corpus in the context of the Democratizing Data project, specifically using OpenAlex as the publication catalog. The goal is to facilitate more accurate identification of USDA dataset mentions in scholarly publications by using local full-text searches.\n\n\n\nPublications identified by dataset mention searches using Scopus were cross-verified in OpenAlex by searching their DOIs. These publications were confirmed to be open-access and available for full-text searches in OpenAlex. However, upon closer investigation, two primary issues were identified with OpenAlex’s full-text indexing methods:\n\nPDF vs. NGRAMS Indexing Methods:\n\nPDF Method: OpenAlex receives the publication’s full text in PDF format and indexes the content directly.\nNGRAMS Method: OpenAlex receives from the author or publisher a preprocessed set of words or phrases (ngrams) extracted from the publication’s full text.\n\nSpecific Issues:\n\nPDF Method Issue: Although undocumented, we observed that the text from the references section of the publications was not indexed by OpenAlex. This occurs because OpenAlex processes the references section specifically to create pointers to other OpenAlex works being referenced. While this approach functions well for publications referencing other scholarly publications, it fails to identify dataset mentions in the references.\nNGRAMS Method Issue: The provided set of ngrams might not include all relevant words or phrases required for dataset identification. For example, if searching for a specific alias such as “USDA Census,” the provided ngrams might not contain the exact phrase or all necessary words, causing missed dataset mentions.\n\n\nThese limitations with OpenAlex’s indexing methods highlighted the need to create dedicated seed and search corpora for accurate dataset mention identification.\n\n\n\nThe seed corpus generation aims to create an effective subset of publications available in OpenAlex to download locally and subsequently run text searches for dataset aliases and flagged terms.\n\n\n\nTo define the seed corpus, several criteria were established based on topics, publication type, publication year, language, and open-access availability. Below are detailed descriptions of the chosen criteria and associated publication counts.\n\n\n\nWe identified relevant topics based on their frequency and relevance. Below are the top 100 topics by publication count:\n\n\n\n\n\n\n\n\n\nTopic ID\nTopic Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nT11610\nImpact of Food Insecurity on Health Outcomes\n313\n78661\n\n\nT11610\nFood Security and Health in Diverse Populations\n236\n78661\n\n\nT10010\nGlobal Trends in Obesity and Overweight Research\n149\n111686\n\n\nT11066\nComparative Analysis of Organic Agricultural Practices\n141\n41275\n\n\nT12253\nUrban Agriculture and Community Development\n140\n27383\n\n\nT10010\nObesity, Physical Activity, Diet\n123\n111686\n\n\nT10367\nAgricultural Innovation and Livelihood Diversification\n110\n49818\n\n\nT11066\nOrganic Food and Agriculture\n106\n41275\n\n\nT11464\nImpact of Homelessness on Health and Well-being\n101\n101019\n\n\nT12253\nUrban Agriculture and Sustainability\n82\n27383\n\n\nT12033\nEuropean Agricultural Policy and Reform\n77\n88980\n\n\nT10367\nAgricultural Innovations and Practices\n76\n49818\n\n\nT11464\nHomelessness and Social Issues\n74\n101019\n\n\nT10841\nDiscrete Choice Models in Economics and Health Care\n72\n66757\n\n\nT10596\nMaternal and Child Nutrition in Developing Countries\n71\n118727\n\n\nT11898\nImpacts of Food Prices on Consumption and Poverty\n70\n29110\n\n\nT11259\nSustainable Diets and Environmental Impact\n65\n45082\n\n\nT12033\nAgricultural Economics and Policy\n60\n88980\n\n\nT10841\nEconomic and Environmental Valuation\n54\n66757\n\n\nT10439\nAdaptation to Climate Change in Agriculture\n50\n27311\n\n\nT10235\nImpact of Social Factors on Health Outcomes\n49\n86076\n\n\nT10866\nRole of Mediterranean Diet in Health Outcomes\n48\n76894\n\n\nT10596\nChild Nutrition and Water Access\n45\n118727\n\n\nT11259\nAgriculture Sustainability and Environmental Impact\n44\n45082\n\n\nT10330\nHydrological Modeling and Water Resource Management\n43\n132216\n\n\nT11886\nRisk Management and Vulnerability in Agriculture\n43\n44755\n\n\nT11898\nEconomics of Agriculture and Food Markets\n43\n29110\n\n\nT11311\nSoil and Water Nutrient Dynamics\n42\n52847\n\n\nT11311\nBiogeochemical Cycling of Nutrients in Aquatic Ecosystems\n42\n52847\n\n\nT10226\nGlobal Analysis of Ecosystem Services and Land Use\n40\n84104\n\n\nT10969\nOptimal Operation of Water Resources Systems\n39\n97570\n\n\nT12732\nImpact of Farming on Health and Safety\n33\n29731\n\n\nT10235\nHealth disparities and outcomes\n32\n86076\n\n\nT10226\nLand Use and Ecosystem Services\n31\n84104\n\n\nT11753\nForest Management and Policy\n31\n75196\n\n\nT10969\nWater resources management and optimization\n31\n97570\n\n\nT12098\nRural development and sustainability\n30\n62114\n\n\nT12724\nIntegrated Management of Water, Energy, and Food Resources\n30\n40148\n\n\nT11886\nAgricultural risk and resilience\n30\n44755\n\n\nT11753\nClimate Change Impacts on Forest Carbon Sequestration\n29\n75196\n\n\nT11711\nImpacts of COVID-19 on Global Economy and Markets\n29\n69059\n\n\nT10111\nRemote Sensing in Vegetation Monitoring and Phenology\n28\n56452\n\n\nT11404\nDeficit Irrigation for Agricultural Water Management\n27\n49715\n\n\nT10439\nClimate change impacts on agriculture\n27\n27311\n\n\nT11862\nAgroecology and Global Food Systems\n26\n34753\n\n\nT12583\nFood Waste Management and Reduction\n26\n27144\n\n\nT10004\nSoil Carbon Dynamics and Nutrient Cycling in Ecosystems\n26\n101907\n\n\nT10330\nHydrology and Watershed Management Studies\n26\n132216\n\n\nT10556\nGlobal Cancer Incidence and Mortality Patterns\n25\n64063\n\n\nT12098\nRural Development and Change in Agricultural Landscapes\n24\n62114\n\n\nT10111\nRemote Sensing in Agriculture\n24\n56452\n\n\nT10556\nGlobal Cancer Incidence and Screening\n24\n64063\n\n\nT10866\nNutritional Studies and Diet\n22\n76894\n\n\nT11560\nDynamics of Livestock Disease Transmission and Control\n22\n68578\n\n\nT10266\nGlobal Forest Drought Response and Climate Change\n22\n73291\n\n\nT12904\nAgricultural Education and School Gardening Research\n21\n110210\n\n\nT12003\nDevelopment and Impacts of Bioenergy Crops\n21\n36853\n\n\nT10298\nInfluence of Built Environment on Active Travel\n21\n86890\n\n\nT10029\nClimate Change and Variability Research\n20\n113541\n\n\nT11711\nCOVID-19 Pandemic Impacts\n20\n69059\n\n\nT10266\nPlant Water Relations and Carbon Dynamics\n20\n73291\n\n\nT11544\nGender Inequality and Labor Force Dynamics\n19\n98755\n\n\nT13388\nFactors Affecting Sagebrush Ecosystems and Wildlife Conservation\n19\n58614\n\n\nT12904\nDiverse Educational Innovations Studies\n18\n110210\n\n\nT12773\nWater Quality and Hydrogeology Research\n18\n50724\n\n\nT10435\nEnvironmental Impact and Sustainability\n18\n55580\n\n\nT11862\nAgriculture, Land Use, Rural Development\n18\n34753\n\n\nT10435\nLife Cycle Assessment and Environmental Impact Analysis\n18\n55580\n\n\nT13393\nFeeding Disorders in Children with Autism Spectrum Disorders\n17\n50595\n\n\nT12724\nWater-Energy-Food Nexus Studies\n17\n40148\n\n\nT11404\nIrrigation Practices and Water Management\n17\n49715\n\n\nT11560\nAnimal Disease Management and Epidemiology\n17\n68578\n\n\nT13393\nChild Nutrition and Feeding Issues\n16\n50595\n\n\nT11544\nGender, Labor, and Family Dynamics\n16\n98755\n\n\nT10298\nUrban Transport and Accessibility\n15\n86890\n\n\nT11789\nLand Tenure and Property Rights in Agriculture\n15\n46627\n\n\nT10391\nEconomics of Health Care Systems and Policies\n15\n260472\n\n\nT10692\nImpact of Urban Green Space on Public Health\n15\n40686\n\n\nT10889\nSoil Erosion and Agricultural Sustainability\n15\n72441\n\n\nT10004\nSoil Carbon and Nitrogen Dynamics\n14\n101907\n\n\nT10446\nIncome, Poverty, and Inequality\n14\n62906\n\n\nT12057\nImpact of Ultra-Processed Foods on Health\n14\n28199\n\n\nT12873\nImpact of Nutrition and Eating Habits on Health\n14\n43157\n\n\nT11645\nEffects of Residential Segregation on Communities and Individuals\n14\n50639\n\n\nT12583\nFood Waste Reduction and Sustainability\n13\n27144\n\n\nT12310\nFactors Affecting Maize Yield and Lodging Resistance\n13\n105863\n\n\nT10889\nSoil erosion and sediment transport\n13\n72441\n\n\nT10487\nImpact of Pollinator Decline on Ecosystems and Agriculture\n13\n218697\n\n\nT10576\nOpioid Epidemic in the United States\n13\n50143\n\n\nT11186\nGlobal Drought Monitoring and Assessment\n13\n35695\n\n\nT10552\nGlobal Trends in Colorectal Cancer Research\n13\n70491\n\n\nT11925\nFood Tourism and Gastronomy Research\n13\n95356\n\n\nT12399\nFactors Influencing Wine Tourism and Consumer Behavior\n13\n50383\n\n\nT12003\nBioenergy crop production and management\n13\n36853\n\n\nT13388\nRangeland and Wildlife Management\n12\n58614\n\n\nT10410\nModeling the Dynamics of COVID-19 Pandemic\n12\n67192\n\n\nT10190\nHealth Effects of Air Pollution\n12\n125501\n\n\nT12733\nBluetongue Virus and Culicoides-Borne Diseases in Europe\n12\n34477\n\n\nT11546\nPlant Physiology and Cultivation Studies\n12\n189058\n\n\nT11552\nGovernance of Global Value Chains and Production Networks\n12\n46357\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 1,192,809.\n\n\n\nWe selected the top journals to further refine our corpus. The following table lists the top 100 journals by publication count:\n\n\n\n\n\n\n\n\n\nJournal ID\nJournal Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nS2764628096\nJournal of Agriculture Food Systems and Community Development\n57\n825\n\n\nS115427279\nPublic Health Nutrition\n51\n3282\n\n\nS206696595\nJournal of Nutrition Education and Behavior\n41\n3509\n\n\nS15239247\nInternational Journal of Environmental Research and Public Health\n39\n59130\n\n\nS4210201861\nApplied Economic Perspectives and Policy\n39\n647\n\n\nS10134376\nSustainability\n35\n87533\n\n\nS5832799\nJournal of Soil and Water Conservation\n34\n556\n\n\nS2739393555\nJournal of Agricultural and Applied Economics\n34\n329\n\n\nS202381698\nPLoS ONE\n30\n143568\n\n\nS124372222\nRenewable Agriculture and Food Systems\n30\n426\n\n\nS91754907\nAmerican Journal of Agricultural Economics\n28\n876\n\n\nS200437886\nBMC Public Health\n28\n18120\n\n\nS18733340\nJournal of the Academy of Nutrition and Dietetics\n27\n5301\n\n\nS78512408\nAgriculture and Human Values\n27\n938\n\n\nS2764593300\nAgricultural and Resource Economics Review\n25\n247\n\n\nS110785341\nNutrients\n25\n30911\n\n\nS4210212157\nFrontiers in Sustainable Food Systems\n23\n3776\n\n\nS69340840\nThe Journal of Rural Health\n20\n749\n\n\nS63571384\nFood Policy\n20\n1069\n\n\nS19383905\nAgricultural Finance Review\n18\n327\n\n\nS4210234824\nEDIS\n18\n3714\n\n\nS119228529\nJournal of Hunger & Environmental Nutrition\n17\n467\n\n\nS204691207\nHortTechnology\n14\n847\n\n\nS4210212179\nJournal of Extension\n14\n1004\n\n\nS43295729\nRemote Sensing\n14\n33899\n\n\nS2738397068\nLand\n14\n9774\n\n\nS80485027\nLand Use Policy\n14\n4559\n\n\nS4210217848\nJAMA Network Open\n13\n12933\n\n\nS139338987\nEnvironmental Research Letters\n13\n6399\n\n\nS2595931848\nFrontiers in Public Health\n12\n19316\n\n\nS122347013\nAmerican Journal of Obstetrics and Gynecology\n12\n15259\n\n\nS204847658\nWater Resources Research\n12\n5305\n\n\nS73449225\nFood Security\n12\n899\n\n\nS4210219560\nCurrent Developments in Nutrition\n12\n10807\n\n\nS183652945\nHortScience\n11\n2415\n\n\nS196734849\nScientific Reports\n11\n198095\n\n\nS139950591\nAgronomy Journal\n10\n2675\n\n\nS86852077\nThe Science of The Total Environment\n10\n56249\n\n\nS37976914\nJAWRA Journal of the American Water Resources Association\n9\n852\n\n\nS2764587901\nJournal of Applied Communications\n9\n250\n\n\nS2607323502\nScientific Data\n9\n5287\n\n\nS44455300\nJournal of Environmental Management\n9\n17835\n\n\nS4210220469\nJournal of Applied Farm Economics\n8\n39\n\n\nS141808269\nRemote Sensing of Environment\n8\n4135\n\n\nS129060628\nDiabetes\n8\n17439\n\n\nS2475403985\nPreventing Chronic Disease\n8\n1068\n\n\nS156283932\nCalifornia Agriculture\n8\n233\n\n\nS157560195\nAgricultural Systems\n8\n1722\n\n\nS136211407\nEcological Economics\n8\n2684\n\n\nS23642417\nSociety & Natural Resources\n8\n792\n\n\nS2574783\nGynecologic Oncology\n8\n8756\n\n\nS149285975\nLand Economics\n8\n399\n\n\nS2594976040\nFrontiers in Veterinary Science\n7\n9896\n\n\nS8391440\nCancer Epidemiology Biomarkers & Prevention\n7\n6099\n\n\nS6596815\nRural Sociology\n7\n467\n\n\nS4210180312\nJournal of the Agricultural and Applied Economics Association\n7\n161\n\n\nS4210202585\nAgriculture\n7\n9931\n\n\nS79054089\nBMJ Open\n7\n31973\n\n\nS2764832999\nScientific investigations report\n7\n1270\n\n\nS180723199\nAgribusiness\n7\n583\n\n\nS154775064\nAgricultural Economics\n7\n729\n\n\nS2738534743\nJournal of Nutritional Science\n6\n659\n\n\nS2754843627\nCancer Medicine\n6\n7317\n\n\nS2228914\nHealth Services Research\n6\n1855\n\n\nS2764680059\nStatistical Journal of the IAOS\n6\n852\n\n\nS76844451\nAnnual Review of Resource Economics\n6\n206\n\n\nS207068962\nCommunity Development\n6\n516\n\n\nS148307540\nEcology and Society\n6\n1281\n\n\nS4210194219\nAntarctica A Keystone in a Changing World\n6\n1110\n\n\nS4210186936\nJournal of Agricultural Science\n6\n2412\n\n\nS12132826\nThe International Food and Agribusiness Management Review\n6\n464\n\n\nS4210197466\nAgroecology and Sustainable Food Systems\n6\n645\n\n\nS204799461\nClimatic Change\n6\n1992\n\n\nS139838620\nObstetrics and Gynecology\n6\n8132\n\n\nS2596909297\nFrontiers in Nutrition\n6\n9700\n\n\nS168049282\nAmerican Journal of Public Health\n6\n4777\n\n\nS106822843\nSocial Science & Medicine\n5\n5847\n\n\nS134216166\nWater\n5\n25819\n\n\nS28036099\nJournal of Rural Studies\n5\n2034\n\n\nS2737313858\nAgricultural & Environmental Letters\n5\n262\n\n\nS32361082\nEuropean Review of Agricultural Economics\n5\n362\n\n\nS178566096\nPreventive Veterinary Medicine\n5\n1907\n\n\nS150168663\nCancer Causes & Control\n5\n1136\n\n\nS106908163\nNeuro-Oncology\n5\n18986\n\n\nS178182516\nJournal of Agromedicine\n5\n565\n\n\nS116775814\nComputers and Electronics in Agriculture\n5\n5965\n\n\nS176659572\nHealth & Social Care in the Community\n5\n2064\n\n\nS2764613780\nJournal of Agricultural Safety and Health\n5\n138\n\n\nS99400149\nJournal of Health Care for the Poor and Underserved\n5\n1197\n\n\nS42419699\nPrecision Agriculture\n5\n1130\n\n\nS130750583\nGlobal Environmental Change\n5\n1092\n\n\nS2492648963\nTransactions of the ASABE\n5\n894\n\n\nS135458494\nPlant Disease\n5\n8264\n\n\nS72684844\nJournal of Animal Science\n5\n15499\n\n\nS173554290\nJournal of Community Health\n5\n1126\n\n\nS28349394\nJournal of Dairy Science\n5\n8060\n\n\nS88153332\nJournal of Nutrition\n5\n3469\n\n\nS199825796\nApplied Engineering in Agriculture\n5\n722\n\n\nS95823145\nForest Policy and Economics\n5\n1509\n\n\nS104641133\nAgricultural Water Management\n5\n4298\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 770,522.\n\n\n\nAuthors with American affiliations were selected to enhance corpus relevance:\n\n\n\n\n\n\n\n\n\nAuthor ID\nAuthor Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nA5016803484\nHeather A. Eicher‐Miller\n15\n140\n\n\nA5024975191\nEdward A. Frongillo\n13\n351\n\n\nA5055158106\nBecca B.R. Jablonski\n12\n60\n\n\nA5047780964\nMeredith T. Niles\n11\n200\n\n\nA5015017711\nJeffrey K. O’Hara\n10\n27\n\n\nA5062679478\nJ. Gordon Arbuckle\n10\n68\n\n\nA5068812455\nCindy W. Leung\n10\n170\n\n\nA5076121862\nSheri D. Weiser\n10\n241\n\n\nA5081656928\nWhitney E. Zahnd\n9\n147\n\n\nA5008463933\nCatherine Brinkley\n8\n34\n\n\nA5027684365\nDayton M. Lambert\n8\n110\n\n\nA5002438645\nPhyllis C. Tien\n8\n244\n\n\nA5081012770\nLinda J. Young\n8\n51\n\n\nA5030548116\nMichele Ver Ploeg\n8\n33\n\n\nA5035584432\nAngela D. Liese\n8\n172\n\n\nA5032940306\nLisa Harnack\n7\n89\n\n\nA5008296893\nEryka Wentz\n7\n33\n\n\nA5006129622\nCarmen Byker Shanks\n7\n103\n\n\nA5053170901\nAni L. Katchova\n7\n62\n\n\nA5024127854\nEduardo Villamor\n7\n84\n\n\nA5060802257\nTracey E. Wilson\n7\n102\n\n\nA5050792105\nJennifer L. Moss\n7\n90\n\n\nA5040727809\nGeorge B. Frisvold\n7\n66\n\n\nA5056021318\nNathan Hendricks\n7\n320\n\n\nA5034750133\nLila A. Sheira\n7\n61\n\n\nA5044317355\nDaniel Merenstein\n7\n113\n\n\nA5002732604\nJulia A. Wolfson\n7\n137\n\n\nA5015455112\nHikaru Hanawa Peterson\n7\n56\n\n\nA5024248662\nAdebola Adedimeji\n7\n137\n\n\nA5038610136\nChristopher N. Boyer\n7\n115\n\n\nA5101813658\nChristian J. Peters\n7\n32\n\n\nA5035164673\nStephan J. Goetz\n6\n90\n\n\nA5029397288\nAmy L. Yaroch\n6\n113\n\n\nA5022651324\nSeth A. Berkowitz\n6\n158\n\n\nA5083470674\nMardge H. Cohen\n6\n205\n\n\nA5070284513\nTimothy S. Griffin\n6\n59\n\n\nA5026810637\nJoleen C. Hadrich\n6\n30\n\n\nA5013419936\nNigel Key\n6\n25\n\n\nA5062332393\nAlessandro Bonanno\n6\n61\n\n\nA5012666568\nHilary K. Seligman\n6\n123\n\n\nA5071854708\nBurton C. English\n6\n68\n\n\nA5069981543\nMegan Konar\n6\n86\n\n\nA5083406390\nZach Conrad\n6\n123\n\n\nA5074296013\nSuat Irmak\n6\n151\n\n\nA5079036202\nJames A. Larson\n6\n49\n\n\nA5038417176\nAdaora A. Adimora\n6\n334\n\n\nA5045489628\nSelena Ahmed\n6\n89\n\n\nA5057302432\nAlan W. Hodges\n6\n73\n\n\nA5091590760\nCraig Gundersen\n6\n69\n\n\nA5089578074\nParke Wilde\n6\n107\n\n\nA5063008522\nA. D. Kendall\n6\n119\n\n\nA5100771544\nHanqin Tian\n6\n434\n\n\nA5072286156\nD. W. Hyndman\n6\n130\n\n\nA5052456209\nKartika Palar\n6\n59\n\n\nA5042679164\nJeffrey Gillespie\n6\n30\n\n\nA5091103546\nKimberly L. Jensen\n6\n65\n\n\nA5014800024\nKartik K. Venkatesh\n5\n244\n\n\nA5003088939\nFrances Hardin‐Fanning\n5\n32\n\n\nA5028409673\nLauri M. Baker\n5\n60\n\n\nA5087431618\nGabrielle Roesch‐McNally\n5\n36\n\n\nA5112481717\nJianhong E. Mu\n5\n20\n\n\nA5067158518\nLisa R. Metsch\n5\n133\n\n\nA5051019392\nDawn Thilmany McFadden\n5\n20\n\n\nA5065308164\nEdward C. Jaenicke\n5\n59\n\n\nA5035062421\nKatherine Dentzman\n5\n35\n\n\nA5011693138\nRyan S. Miller\n5\n163\n\n\nA5019910416\nHolly Gibbs\n5\n69\n\n\nA5014991206\nMargarita Velandia\n5\n29\n\n\nA5081521085\nMark Lubell\n5\n80\n\n\nA5007931812\nTyler J. Lark\n5\n77\n\n\nA5078358162\nJanet M. Turan\n5\n189\n\n\nA5089582462\nLynn M. Yee\n5\n426\n\n\nA5040186224\nNathanael M. Thompson\n5\n32\n\n\nA5070695418\nIghovwerha Ofotokun\n5\n105\n\n\nA5018179894\nAmir M. Rahmani\n5\n264\n\n\nA5057015263\nDawn Thilmany\n5\n34\n\n\nA5038972534\nJyotsna S. Jagai\n5\n48\n\n\nA5003676504\nLandon Marston\n5\n84\n\n\nA5079390198\nChen Zhen\n5\n48\n\n\nA5043968039\nW. David Mulkey\n5\n7\n\n\nA5072793478\nClayton Hallman\n5\n11\n\n\nA5016915956\nJohn Tyndall\n5\n35\n\n\nA5044462604\nJohn M. Antle\n5\n49\n\n\nA5105267439\nColleen T. Webb\n5\n44\n\n\nA5016997673\nMiguel I. Gómez\n5\n138\n\n\nA5006518901\nAndrea Leschewski\n5\n18\n\n\nA5022734861\nAllison Bauman\n5\n32\n\n\nA5010819579\nLisa Chase\n5\n34\n\n\nA5044003446\nW. Jay Christian\n5\n50\n\n\nA5022220353\nBailey Houghtaling\n5\n72\n\n\nA5066919611\nRick Welsh\n5\n20\n\n\nA5053832932\nEric M. Clark\n4\n41\n\n\nA5002482027\nBenjamin M. Gramig\n4\n46\n\n\nA5029506929\nCourtney D. Lynch\n4\n84\n\n\nA5031318120\nJessica Rudnick\n4\n15\n\n\nA5046729938\nSteven R. Browning\n4\n23\n\n\nA5052396793\nLindsay M. Beck‐Johnson\n4\n13\n\n\nA5027592346\nDennis P. Swaney\n4\n26\n\n\nA5042166415\nDavid H. Fleisher\n4\n69\n\n\nA5008867112\nKatie Portacci\n4\n14\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 3,714.\n\n\n\nUpon applying the agreed filters, the final seed corpus resulted in 1,774,245 unique publications. An initial Python script was developed to collect full texts of these publications.\n\n\n\n\n\n\nMetric\nResult\n\n\n\n\nTotal publications attempted\n2,774\n\n\nSuccessfully downloaded full-texts\n974\n\n\nSuccess Rate\n35%\n\n\nEstimated full texts (projected)\n625,000 out of 1,774,245\n\n\nTotal estimated processing time\n~124 days\n\n\n\nThe relatively low success rate indicates significant challenges in accessing full texts, primarily due to missing or inaccessible OA URLs.\n\n\n\n\nOnly 35% success rate in downloading full texts.\nThe existing process is slow, computationally intensive, and likely to require improvements or distributed computing.\nComparison with OpenAlex’s built-in full-text search shows it might be sufficient in certain cases, potentially reducing the necessity of local processing.\n\n\n\n\n\nImplement distributed processing to accelerate corpus generation.\nAssess the adequacy of OpenAlex’s built-in full-text search for practical usage scenarios.\nBalance the need for accuracy with available resources (time and cost).\n\n\n\n\n\nOpenAlex API documentation: OpenAlex Works API\nOA filtered results: OpenAlex Filtered Corpus\nOpenAlex: https://docs.openalex.org\n\n\n\n\nCreating a locally processed seed corpus from OpenAlex significantly improves dataset mention accuracy but poses considerable resource demands. While local full-text processing enhances specificity, careful consideration is required regarding when OpenAlex’s native search capabilities are sufficient.\n\nReferences: - OpenAlex API Documentation: https://docs.openalex.org - Democratizing Data project repository and guidelines (internal documentation, 2025). - USDA Dataset Project Documentation (Internal Document, 2025)."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#openalex",
    "href": "workflow/step02_02/02openalex.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "This report describes the process of generating a Seed Corpus and Search Corpus in the context of the Democratizing Data project, specifically using OpenAlex as the publication catalog. The goal is to facilitate more accurate identification of USDA dataset mentions in scholarly publications by using local full-text searches.\n\n\n\nPublications identified by dataset mention searches using Scopus were cross-verified in OpenAlex by searching their DOIs. These publications were confirmed to be open-access and available for full-text searches in OpenAlex. However, upon closer investigation, two primary issues were identified with OpenAlex’s full-text indexing methods:\n\nPDF vs. NGRAMS Indexing Methods:\n\nPDF Method: OpenAlex receives the publication’s full text in PDF format and indexes the content directly.\nNGRAMS Method: OpenAlex receives from the author or publisher a preprocessed set of words or phrases (ngrams) extracted from the publication’s full text.\n\nSpecific Issues:\n\nPDF Method Issue: Although undocumented, we observed that the text from the references section of the publications was not indexed by OpenAlex. This occurs because OpenAlex processes the references section specifically to create pointers to other OpenAlex works being referenced. While this approach functions well for publications referencing other scholarly publications, it fails to identify dataset mentions in the references.\nNGRAMS Method Issue: The provided set of ngrams might not include all relevant words or phrases required for dataset identification. For example, if searching for a specific alias such as “USDA Census,” the provided ngrams might not contain the exact phrase or all necessary words, causing missed dataset mentions.\n\n\nThese limitations with OpenAlex’s indexing methods highlighted the need to create dedicated seed and search corpora for accurate dataset mention identification.\n\n\n\nThe seed corpus generation aims to create an effective subset of publications available in OpenAlex to download locally and subsequently run text searches for dataset aliases and flagged terms.\n\n\n\nTo define the seed corpus, several criteria were established based on topics, publication type, publication year, language, and open-access availability. Below are detailed descriptions of the chosen criteria and associated publication counts.\n\n\n\nWe identified relevant topics based on their frequency and relevance. Below are the top 100 topics by publication count:\n\n\n\n\n\n\n\n\n\nTopic ID\nTopic Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nT11610\nImpact of Food Insecurity on Health Outcomes\n313\n78661\n\n\nT11610\nFood Security and Health in Diverse Populations\n236\n78661\n\n\nT10010\nGlobal Trends in Obesity and Overweight Research\n149\n111686\n\n\nT11066\nComparative Analysis of Organic Agricultural Practices\n141\n41275\n\n\nT12253\nUrban Agriculture and Community Development\n140\n27383\n\n\nT10010\nObesity, Physical Activity, Diet\n123\n111686\n\n\nT10367\nAgricultural Innovation and Livelihood Diversification\n110\n49818\n\n\nT11066\nOrganic Food and Agriculture\n106\n41275\n\n\nT11464\nImpact of Homelessness on Health and Well-being\n101\n101019\n\n\nT12253\nUrban Agriculture and Sustainability\n82\n27383\n\n\nT12033\nEuropean Agricultural Policy and Reform\n77\n88980\n\n\nT10367\nAgricultural Innovations and Practices\n76\n49818\n\n\nT11464\nHomelessness and Social Issues\n74\n101019\n\n\nT10841\nDiscrete Choice Models in Economics and Health Care\n72\n66757\n\n\nT10596\nMaternal and Child Nutrition in Developing Countries\n71\n118727\n\n\nT11898\nImpacts of Food Prices on Consumption and Poverty\n70\n29110\n\n\nT11259\nSustainable Diets and Environmental Impact\n65\n45082\n\n\nT12033\nAgricultural Economics and Policy\n60\n88980\n\n\nT10841\nEconomic and Environmental Valuation\n54\n66757\n\n\nT10439\nAdaptation to Climate Change in Agriculture\n50\n27311\n\n\nT10235\nImpact of Social Factors on Health Outcomes\n49\n86076\n\n\nT10866\nRole of Mediterranean Diet in Health Outcomes\n48\n76894\n\n\nT10596\nChild Nutrition and Water Access\n45\n118727\n\n\nT11259\nAgriculture Sustainability and Environmental Impact\n44\n45082\n\n\nT10330\nHydrological Modeling and Water Resource Management\n43\n132216\n\n\nT11886\nRisk Management and Vulnerability in Agriculture\n43\n44755\n\n\nT11898\nEconomics of Agriculture and Food Markets\n43\n29110\n\n\nT11311\nSoil and Water Nutrient Dynamics\n42\n52847\n\n\nT11311\nBiogeochemical Cycling of Nutrients in Aquatic Ecosystems\n42\n52847\n\n\nT10226\nGlobal Analysis of Ecosystem Services and Land Use\n40\n84104\n\n\nT10969\nOptimal Operation of Water Resources Systems\n39\n97570\n\n\nT12732\nImpact of Farming on Health and Safety\n33\n29731\n\n\nT10235\nHealth disparities and outcomes\n32\n86076\n\n\nT10226\nLand Use and Ecosystem Services\n31\n84104\n\n\nT11753\nForest Management and Policy\n31\n75196\n\n\nT10969\nWater resources management and optimization\n31\n97570\n\n\nT12098\nRural development and sustainability\n30\n62114\n\n\nT12724\nIntegrated Management of Water, Energy, and Food Resources\n30\n40148\n\n\nT11886\nAgricultural risk and resilience\n30\n44755\n\n\nT11753\nClimate Change Impacts on Forest Carbon Sequestration\n29\n75196\n\n\nT11711\nImpacts of COVID-19 on Global Economy and Markets\n29\n69059\n\n\nT10111\nRemote Sensing in Vegetation Monitoring and Phenology\n28\n56452\n\n\nT11404\nDeficit Irrigation for Agricultural Water Management\n27\n49715\n\n\nT10439\nClimate change impacts on agriculture\n27\n27311\n\n\nT11862\nAgroecology and Global Food Systems\n26\n34753\n\n\nT12583\nFood Waste Management and Reduction\n26\n27144\n\n\nT10004\nSoil Carbon Dynamics and Nutrient Cycling in Ecosystems\n26\n101907\n\n\nT10330\nHydrology and Watershed Management Studies\n26\n132216\n\n\nT10556\nGlobal Cancer Incidence and Mortality Patterns\n25\n64063\n\n\nT12098\nRural Development and Change in Agricultural Landscapes\n24\n62114\n\n\nT10111\nRemote Sensing in Agriculture\n24\n56452\n\n\nT10556\nGlobal Cancer Incidence and Screening\n24\n64063\n\n\nT10866\nNutritional Studies and Diet\n22\n76894\n\n\nT11560\nDynamics of Livestock Disease Transmission and Control\n22\n68578\n\n\nT10266\nGlobal Forest Drought Response and Climate Change\n22\n73291\n\n\nT12904\nAgricultural Education and School Gardening Research\n21\n110210\n\n\nT12003\nDevelopment and Impacts of Bioenergy Crops\n21\n36853\n\n\nT10298\nInfluence of Built Environment on Active Travel\n21\n86890\n\n\nT10029\nClimate Change and Variability Research\n20\n113541\n\n\nT11711\nCOVID-19 Pandemic Impacts\n20\n69059\n\n\nT10266\nPlant Water Relations and Carbon Dynamics\n20\n73291\n\n\nT11544\nGender Inequality and Labor Force Dynamics\n19\n98755\n\n\nT13388\nFactors Affecting Sagebrush Ecosystems and Wildlife Conservation\n19\n58614\n\n\nT12904\nDiverse Educational Innovations Studies\n18\n110210\n\n\nT12773\nWater Quality and Hydrogeology Research\n18\n50724\n\n\nT10435\nEnvironmental Impact and Sustainability\n18\n55580\n\n\nT11862\nAgriculture, Land Use, Rural Development\n18\n34753\n\n\nT10435\nLife Cycle Assessment and Environmental Impact Analysis\n18\n55580\n\n\nT13393\nFeeding Disorders in Children with Autism Spectrum Disorders\n17\n50595\n\n\nT12724\nWater-Energy-Food Nexus Studies\n17\n40148\n\n\nT11404\nIrrigation Practices and Water Management\n17\n49715\n\n\nT11560\nAnimal Disease Management and Epidemiology\n17\n68578\n\n\nT13393\nChild Nutrition and Feeding Issues\n16\n50595\n\n\nT11544\nGender, Labor, and Family Dynamics\n16\n98755\n\n\nT10298\nUrban Transport and Accessibility\n15\n86890\n\n\nT11789\nLand Tenure and Property Rights in Agriculture\n15\n46627\n\n\nT10391\nEconomics of Health Care Systems and Policies\n15\n260472\n\n\nT10692\nImpact of Urban Green Space on Public Health\n15\n40686\n\n\nT10889\nSoil Erosion and Agricultural Sustainability\n15\n72441\n\n\nT10004\nSoil Carbon and Nitrogen Dynamics\n14\n101907\n\n\nT10446\nIncome, Poverty, and Inequality\n14\n62906\n\n\nT12057\nImpact of Ultra-Processed Foods on Health\n14\n28199\n\n\nT12873\nImpact of Nutrition and Eating Habits on Health\n14\n43157\n\n\nT11645\nEffects of Residential Segregation on Communities and Individuals\n14\n50639\n\n\nT12583\nFood Waste Reduction and Sustainability\n13\n27144\n\n\nT12310\nFactors Affecting Maize Yield and Lodging Resistance\n13\n105863\n\n\nT10889\nSoil erosion and sediment transport\n13\n72441\n\n\nT10487\nImpact of Pollinator Decline on Ecosystems and Agriculture\n13\n218697\n\n\nT10576\nOpioid Epidemic in the United States\n13\n50143\n\n\nT11186\nGlobal Drought Monitoring and Assessment\n13\n35695\n\n\nT10552\nGlobal Trends in Colorectal Cancer Research\n13\n70491\n\n\nT11925\nFood Tourism and Gastronomy Research\n13\n95356\n\n\nT12399\nFactors Influencing Wine Tourism and Consumer Behavior\n13\n50383\n\n\nT12003\nBioenergy crop production and management\n13\n36853\n\n\nT13388\nRangeland and Wildlife Management\n12\n58614\n\n\nT10410\nModeling the Dynamics of COVID-19 Pandemic\n12\n67192\n\n\nT10190\nHealth Effects of Air Pollution\n12\n125501\n\n\nT12733\nBluetongue Virus and Culicoides-Borne Diseases in Europe\n12\n34477\n\n\nT11546\nPlant Physiology and Cultivation Studies\n12\n189058\n\n\nT11552\nGovernance of Global Value Chains and Production Networks\n12\n46357\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 1,192,809.\n\n\n\nWe selected the top journals to further refine our corpus. The following table lists the top 100 journals by publication count:\n\n\n\n\n\n\n\n\n\nJournal ID\nJournal Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nS2764628096\nJournal of Agriculture Food Systems and Community Development\n57\n825\n\n\nS115427279\nPublic Health Nutrition\n51\n3282\n\n\nS206696595\nJournal of Nutrition Education and Behavior\n41\n3509\n\n\nS15239247\nInternational Journal of Environmental Research and Public Health\n39\n59130\n\n\nS4210201861\nApplied Economic Perspectives and Policy\n39\n647\n\n\nS10134376\nSustainability\n35\n87533\n\n\nS5832799\nJournal of Soil and Water Conservation\n34\n556\n\n\nS2739393555\nJournal of Agricultural and Applied Economics\n34\n329\n\n\nS202381698\nPLoS ONE\n30\n143568\n\n\nS124372222\nRenewable Agriculture and Food Systems\n30\n426\n\n\nS91754907\nAmerican Journal of Agricultural Economics\n28\n876\n\n\nS200437886\nBMC Public Health\n28\n18120\n\n\nS18733340\nJournal of the Academy of Nutrition and Dietetics\n27\n5301\n\n\nS78512408\nAgriculture and Human Values\n27\n938\n\n\nS2764593300\nAgricultural and Resource Economics Review\n25\n247\n\n\nS110785341\nNutrients\n25\n30911\n\n\nS4210212157\nFrontiers in Sustainable Food Systems\n23\n3776\n\n\nS69340840\nThe Journal of Rural Health\n20\n749\n\n\nS63571384\nFood Policy\n20\n1069\n\n\nS19383905\nAgricultural Finance Review\n18\n327\n\n\nS4210234824\nEDIS\n18\n3714\n\n\nS119228529\nJournal of Hunger & Environmental Nutrition\n17\n467\n\n\nS204691207\nHortTechnology\n14\n847\n\n\nS4210212179\nJournal of Extension\n14\n1004\n\n\nS43295729\nRemote Sensing\n14\n33899\n\n\nS2738397068\nLand\n14\n9774\n\n\nS80485027\nLand Use Policy\n14\n4559\n\n\nS4210217848\nJAMA Network Open\n13\n12933\n\n\nS139338987\nEnvironmental Research Letters\n13\n6399\n\n\nS2595931848\nFrontiers in Public Health\n12\n19316\n\n\nS122347013\nAmerican Journal of Obstetrics and Gynecology\n12\n15259\n\n\nS204847658\nWater Resources Research\n12\n5305\n\n\nS73449225\nFood Security\n12\n899\n\n\nS4210219560\nCurrent Developments in Nutrition\n12\n10807\n\n\nS183652945\nHortScience\n11\n2415\n\n\nS196734849\nScientific Reports\n11\n198095\n\n\nS139950591\nAgronomy Journal\n10\n2675\n\n\nS86852077\nThe Science of The Total Environment\n10\n56249\n\n\nS37976914\nJAWRA Journal of the American Water Resources Association\n9\n852\n\n\nS2764587901\nJournal of Applied Communications\n9\n250\n\n\nS2607323502\nScientific Data\n9\n5287\n\n\nS44455300\nJournal of Environmental Management\n9\n17835\n\n\nS4210220469\nJournal of Applied Farm Economics\n8\n39\n\n\nS141808269\nRemote Sensing of Environment\n8\n4135\n\n\nS129060628\nDiabetes\n8\n17439\n\n\nS2475403985\nPreventing Chronic Disease\n8\n1068\n\n\nS156283932\nCalifornia Agriculture\n8\n233\n\n\nS157560195\nAgricultural Systems\n8\n1722\n\n\nS136211407\nEcological Economics\n8\n2684\n\n\nS23642417\nSociety & Natural Resources\n8\n792\n\n\nS2574783\nGynecologic Oncology\n8\n8756\n\n\nS149285975\nLand Economics\n8\n399\n\n\nS2594976040\nFrontiers in Veterinary Science\n7\n9896\n\n\nS8391440\nCancer Epidemiology Biomarkers & Prevention\n7\n6099\n\n\nS6596815\nRural Sociology\n7\n467\n\n\nS4210180312\nJournal of the Agricultural and Applied Economics Association\n7\n161\n\n\nS4210202585\nAgriculture\n7\n9931\n\n\nS79054089\nBMJ Open\n7\n31973\n\n\nS2764832999\nScientific investigations report\n7\n1270\n\n\nS180723199\nAgribusiness\n7\n583\n\n\nS154775064\nAgricultural Economics\n7\n729\n\n\nS2738534743\nJournal of Nutritional Science\n6\n659\n\n\nS2754843627\nCancer Medicine\n6\n7317\n\n\nS2228914\nHealth Services Research\n6\n1855\n\n\nS2764680059\nStatistical Journal of the IAOS\n6\n852\n\n\nS76844451\nAnnual Review of Resource Economics\n6\n206\n\n\nS207068962\nCommunity Development\n6\n516\n\n\nS148307540\nEcology and Society\n6\n1281\n\n\nS4210194219\nAntarctica A Keystone in a Changing World\n6\n1110\n\n\nS4210186936\nJournal of Agricultural Science\n6\n2412\n\n\nS12132826\nThe International Food and Agribusiness Management Review\n6\n464\n\n\nS4210197466\nAgroecology and Sustainable Food Systems\n6\n645\n\n\nS204799461\nClimatic Change\n6\n1992\n\n\nS139838620\nObstetrics and Gynecology\n6\n8132\n\n\nS2596909297\nFrontiers in Nutrition\n6\n9700\n\n\nS168049282\nAmerican Journal of Public Health\n6\n4777\n\n\nS106822843\nSocial Science & Medicine\n5\n5847\n\n\nS134216166\nWater\n5\n25819\n\n\nS28036099\nJournal of Rural Studies\n5\n2034\n\n\nS2737313858\nAgricultural & Environmental Letters\n5\n262\n\n\nS32361082\nEuropean Review of Agricultural Economics\n5\n362\n\n\nS178566096\nPreventive Veterinary Medicine\n5\n1907\n\n\nS150168663\nCancer Causes & Control\n5\n1136\n\n\nS106908163\nNeuro-Oncology\n5\n18986\n\n\nS178182516\nJournal of Agromedicine\n5\n565\n\n\nS116775814\nComputers and Electronics in Agriculture\n5\n5965\n\n\nS176659572\nHealth & Social Care in the Community\n5\n2064\n\n\nS2764613780\nJournal of Agricultural Safety and Health\n5\n138\n\n\nS99400149\nJournal of Health Care for the Poor and Underserved\n5\n1197\n\n\nS42419699\nPrecision Agriculture\n5\n1130\n\n\nS130750583\nGlobal Environmental Change\n5\n1092\n\n\nS2492648963\nTransactions of the ASABE\n5\n894\n\n\nS135458494\nPlant Disease\n5\n8264\n\n\nS72684844\nJournal of Animal Science\n5\n15499\n\n\nS173554290\nJournal of Community Health\n5\n1126\n\n\nS28349394\nJournal of Dairy Science\n5\n8060\n\n\nS88153332\nJournal of Nutrition\n5\n3469\n\n\nS199825796\nApplied Engineering in Agriculture\n5\n722\n\n\nS95823145\nForest Policy and Economics\n5\n1509\n\n\nS104641133\nAgricultural Water Management\n5\n4298\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 770,522.\n\n\n\nAuthors with American affiliations were selected to enhance corpus relevance:\n\n\n\n\n\n\n\n\n\nAuthor ID\nAuthor Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nA5016803484\nHeather A. Eicher‐Miller\n15\n140\n\n\nA5024975191\nEdward A. Frongillo\n13\n351\n\n\nA5055158106\nBecca B.R. Jablonski\n12\n60\n\n\nA5047780964\nMeredith T. Niles\n11\n200\n\n\nA5015017711\nJeffrey K. O’Hara\n10\n27\n\n\nA5062679478\nJ. Gordon Arbuckle\n10\n68\n\n\nA5068812455\nCindy W. Leung\n10\n170\n\n\nA5076121862\nSheri D. Weiser\n10\n241\n\n\nA5081656928\nWhitney E. Zahnd\n9\n147\n\n\nA5008463933\nCatherine Brinkley\n8\n34\n\n\nA5027684365\nDayton M. Lambert\n8\n110\n\n\nA5002438645\nPhyllis C. Tien\n8\n244\n\n\nA5081012770\nLinda J. Young\n8\n51\n\n\nA5030548116\nMichele Ver Ploeg\n8\n33\n\n\nA5035584432\nAngela D. Liese\n8\n172\n\n\nA5032940306\nLisa Harnack\n7\n89\n\n\nA5008296893\nEryka Wentz\n7\n33\n\n\nA5006129622\nCarmen Byker Shanks\n7\n103\n\n\nA5053170901\nAni L. Katchova\n7\n62\n\n\nA5024127854\nEduardo Villamor\n7\n84\n\n\nA5060802257\nTracey E. Wilson\n7\n102\n\n\nA5050792105\nJennifer L. Moss\n7\n90\n\n\nA5040727809\nGeorge B. Frisvold\n7\n66\n\n\nA5056021318\nNathan Hendricks\n7\n320\n\n\nA5034750133\nLila A. Sheira\n7\n61\n\n\nA5044317355\nDaniel Merenstein\n7\n113\n\n\nA5002732604\nJulia A. Wolfson\n7\n137\n\n\nA5015455112\nHikaru Hanawa Peterson\n7\n56\n\n\nA5024248662\nAdebola Adedimeji\n7\n137\n\n\nA5038610136\nChristopher N. Boyer\n7\n115\n\n\nA5101813658\nChristian J. Peters\n7\n32\n\n\nA5035164673\nStephan J. Goetz\n6\n90\n\n\nA5029397288\nAmy L. Yaroch\n6\n113\n\n\nA5022651324\nSeth A. Berkowitz\n6\n158\n\n\nA5083470674\nMardge H. Cohen\n6\n205\n\n\nA5070284513\nTimothy S. Griffin\n6\n59\n\n\nA5026810637\nJoleen C. Hadrich\n6\n30\n\n\nA5013419936\nNigel Key\n6\n25\n\n\nA5062332393\nAlessandro Bonanno\n6\n61\n\n\nA5012666568\nHilary K. Seligman\n6\n123\n\n\nA5071854708\nBurton C. English\n6\n68\n\n\nA5069981543\nMegan Konar\n6\n86\n\n\nA5083406390\nZach Conrad\n6\n123\n\n\nA5074296013\nSuat Irmak\n6\n151\n\n\nA5079036202\nJames A. Larson\n6\n49\n\n\nA5038417176\nAdaora A. Adimora\n6\n334\n\n\nA5045489628\nSelena Ahmed\n6\n89\n\n\nA5057302432\nAlan W. Hodges\n6\n73\n\n\nA5091590760\nCraig Gundersen\n6\n69\n\n\nA5089578074\nParke Wilde\n6\n107\n\n\nA5063008522\nA. D. Kendall\n6\n119\n\n\nA5100771544\nHanqin Tian\n6\n434\n\n\nA5072286156\nD. W. Hyndman\n6\n130\n\n\nA5052456209\nKartika Palar\n6\n59\n\n\nA5042679164\nJeffrey Gillespie\n6\n30\n\n\nA5091103546\nKimberly L. Jensen\n6\n65\n\n\nA5014800024\nKartik K. Venkatesh\n5\n244\n\n\nA5003088939\nFrances Hardin‐Fanning\n5\n32\n\n\nA5028409673\nLauri M. Baker\n5\n60\n\n\nA5087431618\nGabrielle Roesch‐McNally\n5\n36\n\n\nA5112481717\nJianhong E. Mu\n5\n20\n\n\nA5067158518\nLisa R. Metsch\n5\n133\n\n\nA5051019392\nDawn Thilmany McFadden\n5\n20\n\n\nA5065308164\nEdward C. Jaenicke\n5\n59\n\n\nA5035062421\nKatherine Dentzman\n5\n35\n\n\nA5011693138\nRyan S. Miller\n5\n163\n\n\nA5019910416\nHolly Gibbs\n5\n69\n\n\nA5014991206\nMargarita Velandia\n5\n29\n\n\nA5081521085\nMark Lubell\n5\n80\n\n\nA5007931812\nTyler J. Lark\n5\n77\n\n\nA5078358162\nJanet M. Turan\n5\n189\n\n\nA5089582462\nLynn M. Yee\n5\n426\n\n\nA5040186224\nNathanael M. Thompson\n5\n32\n\n\nA5070695418\nIghovwerha Ofotokun\n5\n105\n\n\nA5018179894\nAmir M. Rahmani\n5\n264\n\n\nA5057015263\nDawn Thilmany\n5\n34\n\n\nA5038972534\nJyotsna S. Jagai\n5\n48\n\n\nA5003676504\nLandon Marston\n5\n84\n\n\nA5079390198\nChen Zhen\n5\n48\n\n\nA5043968039\nW. David Mulkey\n5\n7\n\n\nA5072793478\nClayton Hallman\n5\n11\n\n\nA5016915956\nJohn Tyndall\n5\n35\n\n\nA5044462604\nJohn M. Antle\n5\n49\n\n\nA5105267439\nColleen T. Webb\n5\n44\n\n\nA5016997673\nMiguel I. Gómez\n5\n138\n\n\nA5006518901\nAndrea Leschewski\n5\n18\n\n\nA5022734861\nAllison Bauman\n5\n32\n\n\nA5010819579\nLisa Chase\n5\n34\n\n\nA5044003446\nW. Jay Christian\n5\n50\n\n\nA5022220353\nBailey Houghtaling\n5\n72\n\n\nA5066919611\nRick Welsh\n5\n20\n\n\nA5053832932\nEric M. Clark\n4\n41\n\n\nA5002482027\nBenjamin M. Gramig\n4\n46\n\n\nA5029506929\nCourtney D. Lynch\n4\n84\n\n\nA5031318120\nJessica Rudnick\n4\n15\n\n\nA5046729938\nSteven R. Browning\n4\n23\n\n\nA5052396793\nLindsay M. Beck‐Johnson\n4\n13\n\n\nA5027592346\nDennis P. Swaney\n4\n26\n\n\nA5042166415\nDavid H. Fleisher\n4\n69\n\n\nA5008867112\nKatie Portacci\n4\n14\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 3,714.\n\n\n\nUpon applying the agreed filters, the final seed corpus resulted in 1,774,245 unique publications. An initial Python script was developed to collect full texts of these publications.\n\n\n\n\n\n\nMetric\nResult\n\n\n\n\nTotal publications attempted\n2,774\n\n\nSuccessfully downloaded full-texts\n974\n\n\nSuccess Rate\n35%\n\n\nEstimated full texts (projected)\n625,000 out of 1,774,245\n\n\nTotal estimated processing time\n~124 days\n\n\n\nThe relatively low success rate indicates significant challenges in accessing full texts, primarily due to missing or inaccessible OA URLs.\n\n\n\n\nOnly 35% success rate in downloading full texts.\nThe existing process is slow, computationally intensive, and likely to require improvements or distributed computing.\nComparison with OpenAlex’s built-in full-text search shows it might be sufficient in certain cases, potentially reducing the necessity of local processing.\n\n\n\n\n\nImplement distributed processing to accelerate corpus generation.\nAssess the adequacy of OpenAlex’s built-in full-text search for practical usage scenarios.\nBalance the need for accuracy with available resources (time and cost).\n\n\n\n\n\nOpenAlex API documentation: OpenAlex Works API\nOA filtered results: OpenAlex Filtered Corpus\nOpenAlex: https://docs.openalex.org\n\n\n\n\nCreating a locally processed seed corpus from OpenAlex significantly improves dataset mention accuracy but poses considerable resource demands. While local full-text processing enhances specificity, careful consideration is required regarding when OpenAlex’s native search capabilities are sufficient.\n\nReferences: - OpenAlex API Documentation: https://docs.openalex.org - Democratizing Data project repository and guidelines (internal documentation, 2025). - USDA Dataset Project Documentation (Internal Document, 2025)."
  },
  {
    "objectID": "workflow/step02_01/extract_dataset_mentions.html",
    "href": "workflow/step02_01/extract_dataset_mentions.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to build a dataset of publications that reference the dataset name aliases for the USDA data assets across Scopus, OpenAlex, and Dimensions.\n\nTo generate this dataset, the process requires:\n\nDataset name aliases (from Step 1)\nSearch routines tailored to each citation database to extract relevant publications\n\nSearch routines, described below, guide this step, as dataset mentions are often inconsistent across publications—appearing in titles, abstracts, full text, or reference lists. Scopus uses a structured seed corpus to refine searches, while OpenAlex and Dimensions rely on direct queries across their full publication records. The outputs of this step are three publication-level datasets, one for each citation database, which will be further analyzed in subsequent steps.\n\nScopusOpenAlexDimensions\n\n\n\n\nThe search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 1:\n\n\n\nTable 1: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 2: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 3:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\n\n\n\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to this Appendix for additional details on file construction.\n\n\nComing Soon",
    "crumbs": [
      "Project Workflow",
      "Step 02: Extract Dataset Mentions"
    ]
  },
  {
    "objectID": "workflow/step02_01/extract_dataset_mentions.html#sec-data-idn",
    "href": "workflow/step02_01/extract_dataset_mentions.html#sec-data-idn",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to build a dataset of publications that reference the dataset name aliases for the USDA data assets across Scopus, OpenAlex, and Dimensions.\n\nTo generate this dataset, the process requires:\n\nDataset name aliases (from Step 1)\nSearch routines tailored to each citation database to extract relevant publications\n\nSearch routines, described below, guide this step, as dataset mentions are often inconsistent across publications—appearing in titles, abstracts, full text, or reference lists. Scopus uses a structured seed corpus to refine searches, while OpenAlex and Dimensions rely on direct queries across their full publication records. The outputs of this step are three publication-level datasets, one for each citation database, which will be further analyzed in subsequent steps.\n\nScopusOpenAlexDimensions\n\n\n\n\nThe search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 1:\n\n\n\nTable 1: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 2: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 3:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases.\n\n\n\n\n\nDescribe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to this Appendix for additional details on file construction.\n\n\nComing Soon",
    "crumbs": [
      "Project Workflow",
      "Step 02: Extract Dataset Mentions"
    ]
  },
  {
    "objectID": "workflow/step02_01/extract_dataset_mentions.html#footnotes",
    "href": "workflow/step02_01/extract_dataset_mentions.html#footnotes",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎",
    "crumbs": [
      "Project Workflow",
      "Step 02: Extract Dataset Mentions"
    ]
  },
  {
    "objectID": "workflow/step02_01/03openalex.html",
    "href": "workflow/step02_01/03openalex.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to this Appendix for additional details on file construction."
  },
  {
    "objectID": "workflow/step02_01/03openalex.html#openalex",
    "href": "workflow/step02_01/03openalex.html#openalex",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Describe Rafael’s methodology for searching for dataset names in OpenAlex articles and additional steps Cal did to pull data from the OpenAlex API\nTo collect publications mentioning the NASS Census of Agriculture from the OpenAlex Catalog, I conducted a string search using a predefined set of dataset aliases: “Census of Agriculture,” “USDA Census,” “NASS Census,” “Agricultural Census,” and “AG Census.” To minimize false positives, I applied several filters: the publications had to be in English, published between 2017 and 2024, and include at least one author affiliated with an American institution. Additionally, to ensure that the publications were indeed referring to the correct dataset, I required that they also contain specific flag terms within the full text body, such as “USDA,” “US Department of Agriculture,” “United States Department of Agriculture,” “NASS,” or “National Agricultural Statistics Service.”\nThis method closely mirrors the approach used in the USDA Briefing Book sent by Julia (Appendix 1: Data Search), where a similar string search was applied to the Scopus catalog. In the Scopus analysis, the string search was performed primarily on the references text body rather than the full text and was executed only within a seed corpus. In contrast, our search in OpenAlex was conducted across the entire OpenAlex database. Notably, the references string search in Scopus identified over 80% of the findings, as documented in the briefing book, highlighting the effectiveness of this approach.\nRefer to this Appendix for additional details on file construction."
  },
  {
    "objectID": "workflow/step01/define_data_assets.html",
    "href": "workflow/step01/define_data_assets.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to compile a structured list of dataset names and their commonly used variations.\n\n\n\n\n\n\n\nNote\n\n\n\nThe data assets featured here consist of those collected by the USDA, primarily from the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). These data assets are widely used in agricultural economics and food systems research.\n\n\n\n\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to database curated by Cornell University. These reports are part of the USDA’s efforts to track data usage across various research applications. However, the names of these reports were highly generic, making it difficult to precisely identify them in citation databases. Examples include reports titled “Agricultural Prices” and “Farm Labor,” which lack specificity when compared to more structured dataset identifiers.\nData Processing and Standardization\nTo improve identification and searchability, the input was analyzed and transformed into a structured list that included:\n\nInternational Standard Serial Numbers (ISSNs): Each of the 21 reports was assigned an ISSN, where available, to provide a standardized identifier.\nAlias Creation: Generic report names were appended with the term report to better distinguish them from other similarly named publications in research literature.\nExpanded Search Terms: Additional variations of dataset names were included to account for different citation styles and possible ways authors reference these reports.\n\nThe final dataset classification involved:\n\nMain Data Asset (Parent Record): The original 21 reports, each representing a distinct dataset.\nAliases: ISSNs and URLs served as aliases to improve retrieval accuracy.\nSearch Term Expansion: Combining report names with different citation formats led to a total of 64 search terms (21 parent records + 43 aliases).\n\nThis standardization process improved the efficiency of identifying NASS datasets across publications indexed by citation databases such as Scopus, OpenAlex, and Dimensions.\n\n\n\nThe process of identifying ERS data assets occurred in two phases: (1) an initial dataset compilation, and (2) a refinement process incorporating feedback from a team of agricultural economists at Colorado State University (CSU). This process was meant to yield a list of data assets was both comprehensive and relevant to the research community tracking USDA dataset usage.\nPhase 1: Initial Compilation of ERS Data Assets\nIn October 2023, an initial list of 2,103 ERS records was compiled. These records included dataset names and, in some cases, associated aliases. The list was then reviewed by Professor Julia Lane, who identified and removed 144 records that were not suitable for machine learning-based dataset tracking.\nReasons for Exclusion\n\nRecords were too generic – Terms such as “Milk, Cotton, and CSV Format of National Data” were too broad to be meaningfully identified in citation databases.\nRecords were too specific – Entries such as “Table 15—Agricultural Chemical Input” and “Southeast: 1982-91, 1992-97” were references within broader reports rather than standalone data assets.\n\nAfter these exclusions, the remaining 1,959 records represented the initial list of ERS data assets.\nPhase 2: Refinement with CSU Team\nA team of agricultural economists at CSU were consulted to refine the list so that it accurately captured key USDA datasets that may have been overlooked in the initial process. This involved:\n\nReviewing dataset usage in prior USDA research – Identifying which datasets were frequently cited.\nCross-checking with known data users – Ensuring that key datasets used by agricultural economists were included.\nExpanding alias definitions – Recognizing dataset acronyms and alternative naming conventions.\n\nAs part of this process, an additional set of assets was incorporated, including datasets that had been previously identified in the Year 1 USDA project. Notably, datasets such as the Census of Agriculture and the Agricultural Resource Management Survey (ARMS) were added, along with key acronyms like FoodAPS. This phase contributed:\n\n12 new parent records\n8 additional alias records\nTotal: 20 new search terms\n\nFinal Data Asset Identification\nUnlike NASS data assets, which had ISSNs and DOIs, ERS datasets were primarily linked through URLs. The final structured dataset included:\n\n1,959 parent records (main ERS datasets)\n1,959 alias records (URLs serving as dataset identifiers)\n20 additional records from the CSU consultation\nTotal: 3,918 search terms\n\nThrough this two-phase process, the list of ERS data assets evolved from an initial broad set of records into a refined, structured collection of datasets that could be effectively tracked across citation databases.\n\n\n\nThe data assets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. The final set of data assets, their producing agencies, and descriptions are presented in Table 1.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo provide a comprehensive reference for dataset tracking, this Appendix includes a detailed list of data assets and their corresponding aliases, collectively referred to as dyads. Each dyad represents a dataset-name and alias pair used in citation database searches, allowing for more precise identification of dataset mentions in research publications. These aliases include acronyms, alternate spellings, dataset variations, and associated URLs, ensuring broad coverage across different citation practices. The dyad list serves as the foundation for dataset extraction and disambiguation across Scopus, OpenAlex, and Dimensions.",
    "crumbs": [
      "Project Workflow",
      "Step 01: Define Data Assets"
    ]
  },
  {
    "objectID": "workflow/step01/define_data_assets.html#sec-data",
    "href": "workflow/step01/define_data_assets.html#sec-data",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to compile a structured list of dataset names and their commonly used variations.\n\n\n\n\n\n\n\nNote\n\n\n\nThe data assets featured here consist of those collected by the USDA, primarily from the Economic Research Service (ERS) and the National Agricultural Statistics Service (NASS). These data assets are widely used in agricultural economics and food systems research.\n\n\n\n\nIn late July 2023, an initial set of 21 reports were received containing a report name and a URL link to database curated by Cornell University. These reports are part of the USDA’s efforts to track data usage across various research applications. However, the names of these reports were highly generic, making it difficult to precisely identify them in citation databases. Examples include reports titled “Agricultural Prices” and “Farm Labor,” which lack specificity when compared to more structured dataset identifiers.\nData Processing and Standardization\nTo improve identification and searchability, the input was analyzed and transformed into a structured list that included:\n\nInternational Standard Serial Numbers (ISSNs): Each of the 21 reports was assigned an ISSN, where available, to provide a standardized identifier.\nAlias Creation: Generic report names were appended with the term report to better distinguish them from other similarly named publications in research literature.\nExpanded Search Terms: Additional variations of dataset names were included to account for different citation styles and possible ways authors reference these reports.\n\nThe final dataset classification involved:\n\nMain Data Asset (Parent Record): The original 21 reports, each representing a distinct dataset.\nAliases: ISSNs and URLs served as aliases to improve retrieval accuracy.\nSearch Term Expansion: Combining report names with different citation formats led to a total of 64 search terms (21 parent records + 43 aliases).\n\nThis standardization process improved the efficiency of identifying NASS datasets across publications indexed by citation databases such as Scopus, OpenAlex, and Dimensions.\n\n\n\nThe process of identifying ERS data assets occurred in two phases: (1) an initial dataset compilation, and (2) a refinement process incorporating feedback from a team of agricultural economists at Colorado State University (CSU). This process was meant to yield a list of data assets was both comprehensive and relevant to the research community tracking USDA dataset usage.\nPhase 1: Initial Compilation of ERS Data Assets\nIn October 2023, an initial list of 2,103 ERS records was compiled. These records included dataset names and, in some cases, associated aliases. The list was then reviewed by Professor Julia Lane, who identified and removed 144 records that were not suitable for machine learning-based dataset tracking.\nReasons for Exclusion\n\nRecords were too generic – Terms such as “Milk, Cotton, and CSV Format of National Data” were too broad to be meaningfully identified in citation databases.\nRecords were too specific – Entries such as “Table 15—Agricultural Chemical Input” and “Southeast: 1982-91, 1992-97” were references within broader reports rather than standalone data assets.\n\nAfter these exclusions, the remaining 1,959 records represented the initial list of ERS data assets.\nPhase 2: Refinement with CSU Team\nA team of agricultural economists at CSU were consulted to refine the list so that it accurately captured key USDA datasets that may have been overlooked in the initial process. This involved:\n\nReviewing dataset usage in prior USDA research – Identifying which datasets were frequently cited.\nCross-checking with known data users – Ensuring that key datasets used by agricultural economists were included.\nExpanding alias definitions – Recognizing dataset acronyms and alternative naming conventions.\n\nAs part of this process, an additional set of assets was incorporated, including datasets that had been previously identified in the Year 1 USDA project. Notably, datasets such as the Census of Agriculture and the Agricultural Resource Management Survey (ARMS) were added, along with key acronyms like FoodAPS. This phase contributed:\n\n12 new parent records\n8 additional alias records\nTotal: 20 new search terms\n\nFinal Data Asset Identification\nUnlike NASS data assets, which had ISSNs and DOIs, ERS datasets were primarily linked through URLs. The final structured dataset included:\n\n1,959 parent records (main ERS datasets)\n1,959 alias records (URLs serving as dataset identifiers)\n20 additional records from the CSU consultation\nTotal: 3,918 search terms\n\nThrough this two-phase process, the list of ERS data assets evolved from an initial broad set of records into a refined, structured collection of datasets that could be effectively tracked across citation databases.\n\n\n\nThe data assets represent those most frequently used in agricultural economics research, spanning topics from farm management to food security. The final set of data assets, their producing agencies, and descriptions are presented in Table 1.\n\n\n\nTable 1: List of USDA Data Assets\n\n\n\n\n\nDataset Name\nProduced By\nDescription\n\n\n\n\nCensus of Agriculture\nNASS\nConducted every five years, it provides comprehensive data on U.S. farms, ranches, and producers.\n\n\nAgricultural Resource Management Survey (ARMS)\nERS\nA USDA survey on farm financials, production practices, and resource use.\n\n\nFood Acquisition and Purchase Survey (FoodAPS)\nERS\nA nationally representative survey tracking U.S. household food purchases and acquisitions.\n\n\nCurrent Population Survey Food Security Supplement (CPS-FSS)\nERS\nAn annual supplement to the Current Population Survey (CPS) measuring U.S. household food security.\n\n\nFood Access Research Atlas (FARA)\nERS\nA USDA tool mapping food access based on store locations and socioeconomic data.\n\n\nRural-Urban Continuum Code (RUCC)\nERS\nA classification system distinguishing U.S. counties by rural and urban characteristics.\n\n\nHousehold Food Security Survey Module\nERS\nA USDA survey module used to assess food insecurity levels in households.\n\n\nLocal Food Marketing Practices Survey\nNASS\nA USDA survey on U.S. farms’ local food sales, direct-to-consumer marketing, and supply chains.\n\n\nFarm to School Census\nFNS\nA USDA survey tracking school food procurement and local farm partnerships.\n\n\nQuarterly Food at Home Price Database (QFAHPD)\nERS\nA database of U.S. retail food prices by product, region, and time.\n\n\nTenure Ownership and Transition of Agricultural Land (TOTAL)\nNASS\nA survey collecting data on farmland ownership, leasing, and transfer.\n\n\nTransition of Agricultural Land Survey\nNASS\nA component of TOTAL that examines farmland ownership changes and succession plans.\n\n\nInformation Resources, Inc. (IRI) InfoScan\nCircana (formerly IRI)\nA commercial scanner dataset tracking retail food and consumer goods purchases.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo provide a comprehensive reference for dataset tracking, this Appendix includes a detailed list of data assets and their corresponding aliases, collectively referred to as dyads. Each dyad represents a dataset-name and alias pair used in citation database searches, allowing for more precise identification of dataset mentions in research publications. These aliases include acronyms, alternate spellings, dataset variations, and associated URLs, ensuring broad coverage across different citation practices. The dyad list serves as the foundation for dataset extraction and disambiguation across Scopus, OpenAlex, and Dimensions.",
    "crumbs": [
      "Project Workflow",
      "Step 01: Define Data Assets"
    ]
  },
  {
    "objectID": "workflow/step02_01/03scopus.html",
    "href": "workflow/step02_01/03scopus.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 1:\n\n\n\nTable 1: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 2: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 3:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "workflow/step02_01/03scopus.html#scopus",
    "href": "workflow/step02_01/03scopus.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The search routines for Scopus were designed to systematically identify mentions of USDA data assets across a vast collection of academic publications. Multiple approaches were used to maximize dataset identification. These included (1) full-text searches, which leveraged Scopus’s licensed access to retrieve dataset mentions directly from publication text, (2) reference searches, which scanned citation lists for dataset appearances, and (3) machine learning models, which applied text-matching algorithms to improve accuracy.\n\n\n\n\n\n\nProcess of Running Search Routines\n\n\n\nThe process of running the search routines is to identify a candidate match with the list of data assets. The candidate match is effectively the “publication ID – dataset ID” combination and is referred to as a dyad. For any given publication, there may be multiple dyads.\nIdentifying references to datasets within scientific publications is inherently difficult for a number of reasons including:\n\nNo defined format for dataset references: Datasets are often not cited formally and rather are referred to using unpredictable textual context and formats.\nName disambiguation: Datasets can be referred to by their full name, acronym, and many other valid ways. For instance, the dataset “Rural-Urban Continuum Codes” may also be referred to as “Rural Urban Continuum Codes” or “RUCC” or by using a URL reference,\nConflicts with other terms and phrases: Contextual cues need to be used to ensure, for example, that a data asset such as “Feed Outlook” is indeed the relevant USDA reference.\nSimple spelling and other invalid references: Ideally, search algorithms need to allow for “fuzzy” matching to catch slightly misspelled or mis-named datasets.\n\nTo address this challenge, the project employed the top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative.\n\n\n\n\nScopus is a large, curated abstract and citation database of scientific literature including scientific journals, books, and conference proceedings. Around 11,000 new records are added each day from over 7,000 publishers worldwide. Elsevier is also licensed to access the full text of publications from many, although not all, of these publishers for internal analysis (the full text is not licensed for public use). Where the appropriate licenses do not exist, the records are excluded from the search. To provide some context in this respect, for calendar year 2022, Elsevier estimates that full text records exist for 91% of records published and captured in that year. With license restrictions also considered, the estimate is that it is possible to undertake full text searches on approximately 82% of the total records for that year.\nThe USDA full text search corpus was created using Scopus, with a publication year range of 2017 to 2023 inclusive and using the Topics, Journals and Top Authors.\nThe full text records associated with the USDA search corpus is shown in Table 1:\n\n\n\nTable 1: Full Text Records Associated with USDA Search Corpus\n\n\n\n\n\n\n\n\n\n\nNumber of Records\n\n\n\n\n2017-2023 Articles from Topics\n726,423\n\n\n2017-2023 Articles from Journals\n1,537,851\n\n\n2017-2023 Articles from Top Authors\n21,938\n\n\nDe-duplicated Articles from Above\n2,089,728\n\n\nDeduplicated articles where we have full text\n1,630,958\n\n\nDeduplicated articles where we have full text and are licensed to search\n1,450,086\n\n\n\n\n\n\n\n\n\nA search through the references list of Scopus records is also undertaken as a separate and distinct step from the full text search. The search corpus here is broader than for full text, as there are no license conditions restricting the search. In addition, because references contain highly structured data, it is feasible to search through all of Scopus, as the computational limitations of full-text search do not apply.\nBecause of this, all Scopus records within the publication date range are searched. For the USDA search period of 2017 to 2023, this amounted to 25,110,182 records.\nThe reference search employs an exact text string matching routine across the references of the identified Scopus records.\nBecause of the issues associated with generic terms, the same flags as applied in the Machine Learning step were also applied here.\n\n\n\nTable 2: Number of Records from Scopus References Search Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified in reference search\n25,588\n\n\nNumber of those publications that were unique to the reference search (i.e. not found by Kaggle models).\n22,818\n\n\nNumber of target data assets matched with the above publications\n34,526\n\n\n\n\n\n\n\n\n\nThe top three models from the 2021 Kaggle competition sponsored by the Coleridge Initiative differ in their approaches, strengths, and weaknesses, and the strategy was to use all three to generate results, aggregating and filtering the results to achieve a synergy that would outperform any of the models individually. The same Kaggle models that were used in support of the Year 1 USDA project were employed on the data assets available to this project.\nThe models are applied to the full text search corpus and generate a series of outputs identifying potential dataset matches. For two of the Kaggle models, the focus is on identifying general data assets so many matches will be generated that are not relevant to USDA and its target data assets. Thus, a further fuzzy text matching routine is applied to the Kaggle output to produce a subset of candidate matches (dyads) that are linked to the target data assets.\nAs well as producing metadata for the publications and associated dyads, the process records the Kaggle record that produced the dyad and the scores associated with the matching routines. In addition, for all returned records where publisher licensing allows, a snippet is produced. The snippet is a fragment of text that shows both the referenced dataset and the contextual text that surrounds it, to provide human validators sufficient context to enable them to determine the validity of that candidate reference. The machine learning phase of the project therefore aims to locate all mentions of the target data assets within the search corpus of full text publications and to provide the candidate matches along with their snippets of text to a database that can facilitate subsequent validation by subject matter experts.\nWith a focus on data assets rather than datasets and with some of the name aliases comprising short acronyms and/or very generic terms, there is a risk that high levels of false positives would be generated. For example, one of the search terms was “Crop Progress Report”. There are likely to many other countries beyond the US that all issue reports on Crop Progress. Hence, as well as searching for the terms, a set of flags/filters were also included thus ensuring dyads could be identified which also had the flagged terms. Typically, the filters chosen were linked either to focusing on the agency or to focusing on the research produced in the US. Specifically, for the full text search in the USDA project, the following terms were employed as filters:\n\nNASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service, Economic Research Service\n\nIn total, the use of flags was identified as being appropriate for 112 of the data assets.\nThe Kaggle routines were run in early December 2023 with the process completing on 14 December.\nA summary of some of the key results from the Full Text search is provided in Table 3:\n\n\n\nTable 3: Full Text Search Generated by Kaggle Routine\n\n\n\n\n\n\n\n\n\nProcess Step Outputs\nNumber of Records\n\n\n\n\nNumber of unique Scopus publications identified by the three Kaggle algorithms\n635,831\n\n\nNumber of unique publications identified after Fuzzy text matching to target data assets\n4,104\n\n\nNumber of target data assets matched in the above publications\n4,3921\n\n\nNumber of snippets generated\n14,3772\n\n\n\n\n\n\nPost Processing Adjustments – RUCC and QuickStat Increment\nNote that the RUCC and Quickstat increment was applied after the Kaggle routines were initially run. The process for running that increment involved two steps:\n\nA new search of the Scopus reference search corpus using the RUCC and Quickstat aliases.\nA fuzzy text search of the Kaggle output that had been generated using the RUCC and Quickstat aliases."
  },
  {
    "objectID": "workflow/step02_01/03scopus.html#footnotes",
    "href": "workflow/step02_01/03scopus.html#footnotes",
    "title": "Citation Database Assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExplanatory Note 1: A publication may contain references to more than one target data assets. It may also contain multiple references to the same target data asset. As an example, a publication may contain the following references to target assets (Data Asset A = 3 references, Data Asset B = 2 references, Data asset C = 4 reference then in this field three target data assets, the value included would be “3”.↩︎\nExplanatory Note 2: For the same publication as in Explanatory Note 1, the value here would be 9 provided the license for the publication allowed for snippet generation.↩︎"
  },
  {
    "objectID": "workflow/step02_02/02dimensions.html",
    "href": "workflow/step02_02/02dimensions.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "workflow/step02_02/02dimensions.html#dimensions",
    "href": "workflow/step02_02/02dimensions.html#dimensions",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "The seed corpus approach was only applied to Scopus."
  },
  {
    "objectID": "workflow/step02_02/02scopus.html",
    "href": "workflow/step02_02/02scopus.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later."
  },
  {
    "objectID": "workflow/step02_02/02scopus.html#scopus",
    "href": "workflow/step02_02/02scopus.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "There are several factors that motivate the use of a restricted search space. Although an increasing number of research publications cite datasets in a standard way, data citation has not been adopted as a uniform practice in the research community. Consequently, searches of publication metadata (e.g., the publication reference list) alone is not sufficient. A full text search is required to find all citations of the relevant data assets. However, full text searching is much more intensive in terms of computation, and more expensive in terms of the validation process. As a consequence, the search space is restricted to balance recall and precision.\nIncreasing recall means more of the data assets will be found and hence will provide a broader picture of the data asset’s impact. However, increasing recall comes at the expense of precision, i.e., it will result in more false positives. No matter how attractive achieving high levels of recall may be, that objective rapidly becomes prohibitive in terms of both cost (subject matter expert effort) and time.\nThese three inputs were then combined to provide one list of data assets that could be used in the subsequent process steps. The combined list comprised 2,006 parent records and a further 1,996 aliases (some of which were acronyms). There was therefore a total of 4,002 search terms.\n\n\n\nThe process of creating the search corpus, that is, the body of texts that will be searched, begins with the creation of a seed corpus. The purpose of the seed corpus is to define the parameters that can be used for creating the final search corpus. The seed corpus is, to a first approximation, created by text matching the name for each parent data asset and its aliases with:\n\nfull-text records in ScienceDirect which are within a specified range of publication years, and\nthe reference section for Scopus records that are within the specified range of publication years. For the USDA Year 2 project, the date range period was publications produced between 2017 and 2023 (inclusive).\n\nBecause some of the alias terms are very generic and/or otherwise could result in false positives, they are either excluded from the seed corpus process or only included where they are associated with a flag. In this respect,\n\n12 aliases from the total of 4,002 were excluded completely from both the seed corpus creation reference search and the ScienceDirect search.\n71 aliases were included in the search with a flag term i.e. they returned results only when associated with one or more flag terms. The flag terms were: NASS, USDA, US Department of Agriculture, United States Department of Agriculture, National Agricultural Statistics Service\n\nThe search through ScienceDirect and Scopus references resulted in a set of research publication records matched to the data asset names and aliases. Of the 3,990 (4,002 – 12) aliases included in the search, 328 were found in the references and 163 in ScienceDirect. Of course, there are overlaps between these two datasets and the number of unique data assets found within the seed corpus was 334.\nThe metadata associated with these publications provides insight into what types of research are leveraging these data assets. The “entities” used for this purpose were as follows:\n\nSciVal Topic – 2,699 unique topics in the seed corpus\nJournal – 2,650 unique journals in the seed corpus\nTop Authors – Authors are grouped by numbers of output in seed corpus and the top 1,000 are selected. In our sample 769 relevant authors were from USA.\n\nIt should be noted that Scopus Topics are intended to identify the subject area most likely to use these data assets. These topics are intended to uncover clusters of researchers likely to use similar resources, such as datasets, based on the citation links between their work. It should also be noted that in the Year 1 USDA project, the seed corpus was created just be reviewing list of target journals.\nAs well as recording the entities, the number of records associated with them that was found in the seed corpus is also collected. This provides the basis by which a filtering can be undertaken to focus down on those entities that should be used in search corpus creation.\nThe results of the seed corpus generation (i.e. the entities and record counts) were provided for review to Professor Julia Lane on 15 November 2023. Based on that review, the following table provides a summary of\n\ndecisions were taken with regards to the parameters to be used for creation of the search corpus, and\nthe implications of that decision on search corpus\n\n\n\n\nTable 1: Creating a Seed Corpus\n\n\n\n\n\n\n\n\n\n\nParameter\nSeed Corpus Detection\nConsequence / Implication for Seed Corpus\n\n\n\n\nSciVal Topics\nInclude those SciVal Topics where the article count in the Seed Corpus\nAll articles associated with 262 SciVal Topics\n\n\nJournals\nInclude those Journals where the article count in the Seed Corpus was 7 or more\nAll articles associated with 280 journals\n\n\nTop Authors\nInclude those with US affiliation\nAll articles associated with the US-affiliated 769 Top Authors\n\n\n\n\n\n\n\n\n\nFollowing the Machine Learning routines and during the review of the results, some ambiguities were found in the Data Asset list leading to a set of post-search refinements. In a search list of the scale being dealt with (over 4,000) records, finding these ambiguities was not unexpected.\nSpecifically, in the original list a small number of duplicate alias terms were noted and these were consolidated. A specific example was multiple entries for the data asset “Measuring Access to Food in Tanzania: A Food Basket Approach” and its aliases. Furthermore, it was noted a small number of aliases were attributed to the wrong parent. These were corrected. These changes resulted in a data asset, as at 22 December 2023, that had 3,991 parent and alias records.\nFinally, and again only following the original Kaggle search, it was noted that terms relating to Rural Urban Continuum Codes and Quick Stats had not featured in the original list. An additional eight terms were therefore added as an incremental addition to the list on 29 December 2023. These eight terms comprised two parent records with a further six associated aliases. A specific additional search was conducted for these eight records as explained later."
  },
  {
    "objectID": "workflow/step03/04scopus.html",
    "href": "workflow/step03/04scopus.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON"
  },
  {
    "objectID": "workflow/step03/04scopus.html#scopus",
    "href": "workflow/step03/04scopus.html#scopus",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "To analyze journal coverage in Scopus, we generate a dataset containing all unique journals that include at least one publication referencing Ag Census data. This dataset is built from an initial publication-level dataset, which captures individual research articles mentioning Ag Census.\nWe construct the publication-level dataset for only Ag Census mentions using the following metadata from the publication-level data:\n\nPublication identifier (DOI)\nJournal name\nPublisher\nISSN (International Standard Serial Number, a unique journal identifier)\nDataset alias (alternate names used to reference Ag Census)\nDyad (dataset mention pair)\n\nThis data structure follows the format outlined in the data schema (Figure XX).\n\n\n\n\n\n\nCrosswalk of Dataset Identifiers between Scopus and OpenAlex\n\n\n\n\n\nScopus assigns multiple identifiers to the same dataset depending on how it is reported, rather than a single, standardized identifier. Therefore, the authors create a crosswalk between Scopus and OpenAlex so that each dataset can have one common identifier.\nLink to crosswalk file\n\n\n\nAfter assembling the publication-level dataset, the final step in preparing the Scopus journal-level dataset is to aggregate publications at the journal level based on their ISSN.\n\n\n\nCOMING SOON\n\n\n\nCOMING SOON"
  },
  {
    "objectID": "workflow/results/compare_pub_data.html",
    "href": "workflow/results/compare_pub_data.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to develop statistics that measure dataset tracking accuracy.\n\n\n\n\nContinuing with our case study, we use the datasets produced in Step 4 to produce counts of the number of journals with Ag Census publications that:\n\nonly appear in Scopus,\nonly appear in OpenAlex, or\nappear in both.\n\nFor journals that contain Ag Census data in both citation databases, we summarize the coverage of publications that appear in both Scopus and OpenAlex.\nThen, we investigate discrepancies based on factors like missing identifiers, mismatched journal information (ISSNs), and additional publications accessed through OpenAlex’s API.\nAdd here: What are the steps in producing Table AA\n\nScopusOpenAlex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis section presents results after matching (which type varies – deterministic vs fuzzy)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Matching Methods\n\n\n\n\n\n\nRule-based matching for exact matches\nProbabilistic matching for handling variations\nMachine learning methods for complex cases\n\n\n\n\nTable 1: Summary of Methods\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nConsiderations\nExample\nPros\nCons\n\n\n\n\nSearching for dataset names within Scopus\n\n\n\n\n\n\nSearching for dataset names within OpenAlex\n“Location” field set to “journal”\n\n\n\n\n\nDisambiguation of authors\n\n\n\n\n\n\nDisambiguation of institutions\n\n\n\n\n\n\nStandardization of institutions\n\n\n\n\n\n\nSearching based on the frequency of dataset appearance in journals\n\n\n\n\n\n\nMORE . . .\n\n\n\n\n\n\nFiltering on keywords to determine themes\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll appendices referenced throughout the report are located on this page.",
    "crumbs": [
      "Project Workflow",
      "Step 04: Compare Results"
    ]
  },
  {
    "objectID": "workflow/results/compare_pub_data.html#sec-matching",
    "href": "workflow/results/compare_pub_data.html#sec-matching",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "The goal of this step is to develop statistics that measure dataset tracking accuracy.\n\n\n\n\nContinuing with our case study, we use the datasets produced in Step 4 to produce counts of the number of journals with Ag Census publications that:\n\nonly appear in Scopus,\nonly appear in OpenAlex, or\nappear in both.\n\nFor journals that contain Ag Census data in both citation databases, we summarize the coverage of publications that appear in both Scopus and OpenAlex.\nThen, we investigate discrepancies based on factors like missing identifiers, mismatched journal information (ISSNs), and additional publications accessed through OpenAlex’s API.\nAdd here: What are the steps in producing Table AA\n\nScopusOpenAlex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis section presents results after matching (which type varies – deterministic vs fuzzy)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Matching Methods\n\n\n\n\n\n\nRule-based matching for exact matches\nProbabilistic matching for handling variations\nMachine learning methods for complex cases\n\n\n\n\nTable 1: Summary of Methods\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\nConsiderations\nExample\nPros\nCons\n\n\n\n\nSearching for dataset names within Scopus\n\n\n\n\n\n\nSearching for dataset names within OpenAlex\n“Location” field set to “journal”\n\n\n\n\n\nDisambiguation of authors\n\n\n\n\n\n\nDisambiguation of institutions\n\n\n\n\n\n\nStandardization of institutions\n\n\n\n\n\n\nSearching based on the frequency of dataset appearance in journals\n\n\n\n\n\n\nMORE . . .\n\n\n\n\n\n\nFiltering on keywords to determine themes\n\n\n\n\n\n\n\n\n\n\n\n\n\nAll appendices referenced throughout the report are located on this page.",
    "crumbs": [
      "Project Workflow",
      "Step 04: Compare Results"
    ]
  },
  {
    "objectID": "workflow.html#define-scope-of-data-assets-to-be-searched",
    "href": "workflow.html#define-scope-of-data-assets-to-be-searched",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Identify USDA datasets that will be searched for and tracked.\nCollect official dataset names along with common abbreviations, acronyms, and alternative references used.\n\nResult: A structured list of dataset names and aliases.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#extract-dataset-mentions-from-publications",
    "href": "workflow.html#extract-dataset-mentions-from-publications",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Conduct searches across citation databases using multiple methods:\n\nFull-Text (String) Search: Scan entire articles for relevant dataset names.\nReference Search: Identify dataset citations within publication references.\nMachine Learning Models: Apply Kaggle competition models trained to detect dataset mentions.\n\n\nNote: In cases where full-text search is not supported by the citation database API (e.g., Scopus), an initial seed corpus of publications was collected separately to train machine learning models. Refer to “Creating a Seed Corpus” for more details.\nResult: Publication dataset for each data asset across each citation database.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#pre-process-and-clean-publication-datasets",
    "href": "workflow.html#pre-process-and-clean-publication-datasets",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Pre-process and clean publication metadata generated from each citation database.\nStandardize journal, institution, and author names.\nDeduplicate records.\n\nResult: Cleaned publication metadata, removed of duplicates, inconsistencies, and missing information.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "workflow.html#compare-across-citation-databases",
    "href": "workflow.html#compare-across-citation-databases",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Compare dataset coverage across Scopus, OpenAlex, and Dimensions.\nApply fuzzy matching techniques to identify overlapping and unique dataset mentions.\nAnalyze differences in journal coverage, citation patterns, and author affiliations.\n\nResult: A set of statistics used to evaluate dataset tracking accuracy.",
    "crumbs": [
      "Project Workflow"
    ]
  },
  {
    "objectID": "index.html#sec-aims",
    "href": "index.html#sec-aims",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Evaluate differences in publication coverage across citation databases. Compare how well Scopus, OpenAlex, and Dimensions track dataset usage in research publications and assess variations in publication inclusion.\nAssess journal coverage. Determine which journals each platform indexes and analyze how these differences impact dataset visibility.\nAnalyze publication-level discrepancies. Compare how each platform captures research publications within indexed journals and identify potential gaps in dataset tracking.\nExamine author and institutional representation. Investigate how each platform attributes authorship and institutional affiliations, with a focus on variations by research institution type (e.g., Minority-Serving Institutions).\nDevelop a reproducible methodology for database comparison. Establish a systematic approach for evaluating other citation databases beyond Scopus, OpenAlex, and Dimensions.\n\nThese aims guide the development of a methodology for comparing citation databases, focusing on four areas:\n\nJournal coverage: Determining which journals each platform indexes\nPublication tracking: Comparing how each platform captures publications within indexed journals\nAuthor identification: Evaluating how each platform handles author names and affiliations\nInstitution recognition: Determining how each platform records and standardizes institutional information\n\nThe scope of work includes comparing publication coverage across Scopus, OpenAlex, and Dimensions. This inclusion provides a comprehensive assessment of citation databases, particularly in evaluating dataset coverage across both proprietary and open-access platforms. For more information on each citation database, refer to this Appendix.\nOur methodology provides a systematic approach for assessing citation databases’ strengths and limitations in tracking dataset usage across research papers. Beyond platform comparison, this methodology examines variations in dataset usage across different types of research institutions.\nThe methods described in this report can be applied to other citation databases as alternatives to current data sources.",
    "crumbs": [
      "Project Background"
    ]
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "",
    "text": "Appendix\nFile Path & Link\n\n\n\n\nCitation Database Summary\n\n\n\nData Schema\nAll data schema can be accessed through here.\n\n\nScopus File Inventory\n\n\n\nOpenAlex File Inventory\n\n\n\nDimensions File Inventory\n\n\n\nIPEDS File Inventory\n\n\n\nMSI File Inventory",
    "crumbs": [
      "Appendices"
    ]
  },
  {
    "objectID": "appendices/app_dimensions.html#sec-app-dimensions",
    "href": "appendices/app_dimensions.html#sec-app-dimensions",
    "title": "Dimensions Data Dictionary",
    "section": "",
    "text": "Research partners at the University of Utah have access to Dimensions.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Dimensions Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_ipeds.html#summary-of-the-ipeds-data",
    "href": "appendices/app_ipeds.html#summary-of-the-ipeds-data",
    "title": "Classifying Institutions with the IPEDS Database",
    "section": "Summary of the IPEDS Data",
    "text": "Summary of the IPEDS Data\n\nDistinct institutions by yearDistinct institutions by year and controlDistinct institutions by year and level: Public UniversitiesDistinct institutions by year and level: Private Universities\n\n\n\n\n\nYear\nNo. Institutions\n\n\n\n\n2017\n7153\n\n\n2018\n6857\n\n\n2019\n6559\n\n\n2020\n6440\n\n\n2021\n6289\n\n\n2022\n6256\n\n\n2023\n6163\n\n\n\n\n\n\n\n\nYear\nPrivate for-profit\nPrivate not-for-profit\nPublic\n\n\n\n\n2017\n3093\n1959\n2069\n\n\n2018\n2793\n1930\n2077\n\n\n2019\n2566\n1905\n2056\n\n\n2020\n2463\n1889\n2036\n\n\n2021\n2411\n1868\n1994\n\n\n2022\n2352\n1855\n2019\n\n\n2023\n2299\n1836\n1999\n\n\n\n“Control” is defined as Public, Private Nonprofit, and Private For-Profit.\n\n\n\n\n\nYear\n2-year\n4-year\n\n\n\n\n2017\n1003\n817\n\n\n2018\n989\n840\n\n\n2019\n968\n852\n\n\n2020\n949\n852\n\n\n2021\n930\n829\n\n\n2022\n924\n859\n\n\n2023\n899\n868\n\n\n\n\n\n\n\n\nYear\n2-year\n4-year\n\n\n\n\n2017\n1034\n2371\n\n\n2018\n917\n2167\n\n\n2019\n774\n2081\n\n\n2020\n736\n2046\n\n\n2021\n709\n2016\n\n\n2022\n681\n2009\n\n\n2023\n664\n1996",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Classifying Institutions with the IPEDS Database"
    ]
  },
  {
    "objectID": "appendices/app_ipeds.html#data-processing-and-standardization",
    "href": "appendices/app_ipeds.html#data-processing-and-standardization",
    "title": "Classifying Institutions with the IPEDS Database",
    "section": "Data Processing and Standardization",
    "text": "Data Processing and Standardization\n\nVariable Name Changes and Formatting:\n\nUNITID and year serve as the primary keys to merge datasets for analysis.\nyear was added to datasets that lacked it.\n\nHandling Missing Data and Filters:\n\nNon-relevant columns were removed.\nDatasets were filtered to retain only institutions with complete enrollment and classification data.\n\nMerging Strategy:\n\nDatasets can be joined using UNITID and year as unique identifiers.\nInstitutions missing UNITID were excluded.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Classifying Institutions with the IPEDS Database"
    ]
  },
  {
    "objectID": "appendices/app_ipeds.html#how-to-merge-with-msi-data",
    "href": "appendices/app_ipeds.html#how-to-merge-with-msi-data",
    "title": "Classifying Institutions with the IPEDS Database",
    "section": "How to Merge with MSI Data",
    "text": "How to Merge with MSI Data\nThe IPEDS dataset can be linked with the MSI dataset using the UNITID and year variables. This allows for:\n\nIdentifying MSI institutions within IPEDS to analyze institutional characteristics.\nComparing institutional characteristics of MSI and non-MSI institutions, such as enrollment size, Carnegie classification, and financial indicators.\n\n\n\n\n\n\n\nNote\n\n\n\nAfter merging the IPEDS-MSI data with the cleaned institutional data from the citation databases, this dataset also allows for assessing research output and dataset usage by institution type, and examining trends over time in MSI status and institutional characteristics.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Classifying Institutions with the IPEDS Database"
    ]
  },
  {
    "objectID": "appendices/app_ipeds.html#footnotes",
    "href": "appendices/app_ipeds.html#footnotes",
    "title": "Classifying Institutions with the IPEDS Database",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n“Control” is defined as Public, Private Nonprofit, and Private For-Profit.↩︎",
    "crumbs": [
      "Appendices",
      "Data Schema",
      "Classifying Institutions with the IPEDS Database"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#summary-of-the-msi-data",
    "href": "appendices/app_msi.html#summary-of-the-msi-data",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "Summary of the MSI data",
    "text": "Summary of the MSI data\n\nTotal Institutions by MSI StatusPrivate Institutions by MSI StatusPrivate For-Profit Institutions by MSI StatusPrivate Not-for-Profit Institutions by MSI StatusPublic Institutions by MSI StatusPublic 2-year Institutions by MSI StatusPublic 4-year Institutions by MSI StatusPrivate 2-year Institutions by MSI StatusPrivate 4-year Institutions by MSI Status\n\n\n\n\n\nYear\nTotal\nMSI %\nNot_MSI %\n\n\n\n\n2017\n5242\n8.13\n91.87\n\n\n2018\n5242\n7.69\n92.31\n\n\n2019\n5242\n7.84\n92.16\n\n\n2020\n5242\n7.80\n92.20\n\n\n2021\n5242\n8.81\n91.19\n\n\n2022\n5242\n16.73\n83.27\n\n\n2023\n5242\n16.58\n83.42\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n1837\n5.66\n94.34\n\n\n2018\n1837\n5.72\n94.28\n\n\n2019\n1837\n5.72\n94.28\n\n\n2020\n1837\n5.77\n94.23\n\n\n2021\n1837\n5.88\n94.12\n\n\n2022\n1837\n14.81\n85.19\n\n\n2023\n1837\n14.81\n85.19\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n1409\n0.00\n100.00\n\n\n2018\n1409\n0.00\n100.00\n\n\n2019\n1409\n0.00\n100.00\n\n\n2020\n1409\n0.00\n100.00\n\n\n2021\n1409\n0.00\n100.00\n\n\n2022\n1409\n0.14\n99.86\n\n\n2023\n1409\n0.21\n99.79\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n26\n0.00\n100.00\n\n\n2018\n26\n0.00\n100.00\n\n\n2019\n26\n0.00\n100.00\n\n\n2020\n26\n0.00\n100.00\n\n\n2021\n26\n0.00\n100.00\n\n\n2022\n26\n3.85\n96.15\n\n\n2023\n26\n3.85\n96.15\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n1678\n19.18\n80.81\n\n\n2018\n1678\n17.76\n82.24\n\n\n2019\n1678\n18.24\n81.76\n\n\n2020\n1678\n18.06\n81.94\n\n\n2021\n1678\n21.10\n78.90\n\n\n2022\n1678\n33.86\n66.15\n\n\n2023\n1678\n35.34\n64.66\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n933\n19.72\n80.28\n\n\n2018\n933\n16.93\n83.07\n\n\n2019\n933\n17.04\n82.96\n\n\n2020\n933\n17.15\n82.85\n\n\n2021\n933\n20.15\n79.85\n\n\n2022\n933\n36.66\n63.34\n\n\n2023\n933\n35.05\n64.95\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n807\n17.10\n82.90\n\n\n2018\n807\n17.35\n82.65\n\n\n2019\n807\n18.22\n81.78\n\n\n2020\n807\n17.82\n82.18\n\n\n2021\n807\n20.57\n79.43\n\n\n2022\n807\n32.09\n67.91\n\n\n2023\n807\n32.96\n67.04\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n139\n2.88\n97.12\n\n\n2018\n139\n2.88\n97.12\n\n\n2019\n139\n2.16\n97.84\n\n\n2020\n139\n2.16\n97.84\n\n\n2021\n139\n2.88\n97.12\n\n\n2022\n139\n12.23\n87.77\n\n\n2023\n139\n12.23\n87.77\n\n\n\n\n\n\n\n\nYear\nTotal\n% MSI\n% Not MSI\n\n\n\n\n2017\n1714\n5.83\n94.17\n\n\n2018\n1714\n5.84\n94.16\n\n\n2019\n1714\n5.95\n94.05\n\n\n2020\n1714\n6.01\n93.99\n\n\n2021\n1714\n6.07\n93.93\n\n\n2022\n1714\n14.88\n85.12\n\n\n2023\n1714\n14.88\n85.12",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#data-processing-and-standardization",
    "href": "appendices/app_msi.html#data-processing-and-standardization",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "Data Processing and Standardization",
    "text": "Data Processing and Standardization\n\nVariable Name Changes and Formatting:\n\nyear was added to track MSI status over time.\nUNITID is the primary key to merge MSI and IPEDS datasets.\n\nHandling Missing Data and Filters:\n\nInstitutions without UNITID were excluded.\nNon-relevant columns were removed.\n\nMerging Strategy:\n\nThe 2017-2021 MSI data was directly compatible with IPEDS.\nThe 2022-2023 MSI data required additional formatting before merging.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "appendices/app_msi.html#how-to-merge-with-ipeds-data",
    "href": "appendices/app_msi.html#how-to-merge-with-ipeds-data",
    "title": "Identifying Minority Serving Institutions (MSIs)",
    "section": "How to Merge with IPEDS Data",
    "text": "How to Merge with IPEDS Data\nThe MSI dataset can be linked with IPEDS data using the UNITID and year variables. This allows for:\n\nTracking institutional MSI status over time.\nComparing MSI and non-MSI institutions within Scopus, OpenAlex, and Dimensions.\nAssessing dataset usage patterns across different institution types.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Identifying Minority Serving Institutions (MSIs)"
    ]
  },
  {
    "objectID": "workflow/step02_01/03dimensions.html",
    "href": "workflow/step02_01/03dimensions.html",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Coming Soon"
  },
  {
    "objectID": "workflow/step02_01/03dimensions.html#dimensions",
    "href": "workflow/step02_01/03dimensions.html#dimensions",
    "title": "Citation Database Assessment",
    "section": "",
    "text": "Coming Soon"
  },
  {
    "objectID": "appendices/app_dyads.html",
    "href": "appendices/app_dyads.html",
    "title": "Data Dyads: Assets and Aliases",
    "section": "",
    "text": "Add file here"
  },
  {
    "objectID": "conclusions.html#next-steps",
    "href": "conclusions.html#next-steps",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Next Steps",
    "text": "Next Steps",
    "crumbs": [
      "Conclusions"
    ]
  },
  {
    "objectID": "appendices/app_crosswalk.html#file-inventory",
    "href": "appendices/app_crosswalk.html#file-inventory",
    "title": "Data Schemas and File Inventory",
    "section": "File Inventory",
    "text": "File Inventory\n\nScopus Files\n\n\n\nPrimary Table: publication\nSupporting Tables:\n\nagency_run\nasjc\nauthor\nauthor_affiliation\ndataset_alias\ndyad\ndyad_model\nissn\njournal\nmodel\npublication_affiliation\npublication_asjc\npublication_author\npublication_topic\npublication_ufc\npublisher\ntopic\n\n\nRefer to the schema for additional column-level details.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#publication.csv-primary-table",
    "href": "appendices/app_scopus.html#publication.csv-primary-table",
    "title": "Scopus Data Dictionary",
    "section": "1.1 publication.csv (Primary table)",
    "text": "1.1 publication.csv (Primary table)\n\nDescription: Central table with metadata about publications.\nColumns:\n\nid: Unique publication identifier\ntitle: Publication title\ndoi: Digital Object Identifier\nyear, month: Date of publication\ncitation_count, pub_type: Additional metadata",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#dataset_alias.csv",
    "href": "appendices/app_scopus.html#dataset_alias.csv",
    "title": "Scopus Data Dictionary",
    "section": "0.1 dataset_alias.csv",
    "text": "0.1 dataset_alias.csv\n\nDescription: Lists dataset aliases (alternate names).\nColumns:\n\nalias_id: Unique alias identifier\nparent_alias_id: Primary alias identifier (if alias_id = parent_alias_id, it’s primary)\nalias: Name of the alias\n\nHow to use:\n\nIdentify primary aliases where alias_id = parent_alias_id\nFind all aliases by filtering on parent_alias_id",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#dyad.csv",
    "href": "appendices/app_scopus.html#dyad.csv",
    "title": "Scopus Data Dictionary",
    "section": "0.2 dyad.csv",
    "text": "0.2 dyad.csv\n\nDescription: Links publications (publication.csv) and dataset aliases (dataset_alias.csv).\nColumns:\n\nid: Unique identifier for each mention (dyad)\npublication_id: Foreign key to publication.csv\nalias_id: Foreign key to dataset_alias.csv\nmention_candidate: Mention text from publication",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#model.csv",
    "href": "appendices/app_scopus.html#model.csv",
    "title": "Scopus Data Dictionary",
    "section": "0.3 `model.csv",
    "text": "0.3 `model.csv\n\nDescription: Lists methods/models for identifying dataset mentions.\nColumns:\n\nid: Unique model identifier\nname: Name of the identification model",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#dyad_model.csv",
    "href": "appendices/app_scopus.html#dyad_model.csv",
    "title": "Scopus Data Dictionary",
    "section": "0.4 dyad_model.csv",
    "text": "0.4 dyad_model.csv\n\nDescription: Connects dyads and models used to identify them.\nColumns:\n\ndyad_id: Foreign key to dyad.csv\nmodel_id: Foreign key to model.csv\nscore: Confidence or relevance score\n\nHow to use:\n\nFilter mentions by joining with dyad.csv on dyad_id and filtering by model_id",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#data-dictionary-.unnumbered",
    "href": "appendices/app_scopus.html#data-dictionary-.unnumbered",
    "title": "Scopus Data Dictionary",
    "section": "0.1 Data Dictionary {.unnumbered}}",
    "text": "0.1 Data Dictionary {.unnumbered}}\npublication.csv (Primary table) - Description: Central table with metadata about publications. - Columns: - id: Unique publication identifier - title: Publication title - doi: Digital Object Identifier - year, month: Date of publication - citation_count, pub_type: Additional metadata",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#data-dictionary",
    "href": "appendices/app_scopus.html#data-dictionary",
    "title": "Scopus Data Dictionary",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nDownload Scopus Source Files\n\n\n\nYou can download the source files from this link.\n\n\n\npublication.csv (Primary table)\n\nDescription: Central table with metadata about publications.\nColumns:\n\nid: Unique publication identifier\ntitle: Publication title\ndoi: Digital Object Identifier\nyear, month: Date of publication\ncitation_count, pub_type: Additional metadata\n\n\n\n\ndataset_alias.csv\n\nDescription: Lists dataset aliases (alternate names).\nColumns:\n\nalias_id: Unique alias identifier\nparent_alias_id: Primary alias identifier (if alias_id = parent_alias_id, it’s primary)\nalias: Name of the alias\n\nHow to use:\n\nIdentify primary aliases where alias_id = parent_alias_id\nFind all aliases by filtering on parent_alias_id\n\n\n\n\ndyad.csv\n\nDescription: Links publications (publication.csv) and dataset aliases (dataset_alias.csv).\nColumns:\n\nid: Unique identifier for each mention (dyad)\npublication_id: Foreign key to publication.csv\nalias_id: Foreign key to dataset_alias.csv\nmention_candidate: Mention text from publication\n\n\n\n\nmodel.csv\n\nDescription: Lists methods/models for identifying dataset mentions.\nColumns:\n\nid: Unique model identifier\nname: Name of the identification model\n\n\n\n\ndyad_model.csv\n\nDescription: Connects dyads and models used to identify them.\nColumns:\n\ndyad_id: Foreign key to dyad.csv\nmodel_id: Foreign key to model.csv\nscore: Confidence or relevance score\n\nHow to use:\n\nFilter mentions by joining with dyad.csv on dyad_id and filtering by model_id",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_scopus.html#sample-data",
    "href": "appendices/app_scopus.html#sample-data",
    "title": "Scopus Data Dictionary",
    "section": "Sample Data",
    "text": "Sample Data\n\npublication.csvdataset_alias.csvdyad.csvmodel.csvdyad_model.csv\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n\n\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "Scopus Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_openalex.html#data-dictionary",
    "href": "appendices/app_openalex.html#data-dictionary",
    "title": "OpenAlex Data Dictionary",
    "section": "Data Dictionary",
    "text": "Data Dictionary\n\n\n\n\n\n\nDownload OpenAlex Source Files\n\n\n\nYou can download the source files from this link.\n\n\n\ndataset.csv\n\nDescription: Lists all datasets identified in OpenAlex. This contains details of all USDA datasets.\n\n\n\npublication_dataset_links.csv\n\nDescription: Connects publications with one or more datasets in the OpenAlex data. Indicates which publications are associated with which datasets.\n\n\n\npublication.csv\n\nDescription: This file contains information about publications, which are the central entities in this dataset.\nKey Columns:\n\nid: Unique identifier for each publication.\ntitle: Title of the publication.\ndoi: Digital Object Identifier of the publication.\nyear and month: Publication date.\nOther metadata such as citation_count, pub_type, etc.\n\n\n\n\ndataset_alias.csv\n\nDescription: Contains all the aliases (alternative names) of datasets. This helps in identifying datasets that might be referred to by different names in publications.\nKey Columns:\n\nalias_id: Unique identifier for each alias.\nparent_alias_id: Identifies the main alias for a dataset. If parent_alias_id equals alias_id, it is the primary alias.\nalias: The alias name of the dataset.\n\n\nNote: The search in OpenAlex was performed using the same aliases and flag terms applied in the Scopus data, without any optimization.\nHow to Use:\n\nTo find the main alias of a dataset, look for rows where alias_id equals parent_alias_id.\nTo find all aliases of a dataset, filter by parent_alias_id corresponding to the main alias.\n\n\n\ndyad.csv\n\nDescription: Represents the mentions of dataset aliases found within publications. Acts as a linking table between publication.csv and dataset_alias.csv.\nKey Columns:\n\nid: Unique identifier for each dyad (mention).\npublication_id: References the id in publication.csv.\nalias_id: References the alias_id in dataset_alias.csv.\nmention_candidate: The actual text mentioning the dataset in the publication.\n\n\n\n\nmodel.csv\n\nDescription: Lists the different models or methods used to identify dataset mentions within publications.\nKey Columns:\n\nid: Unique identifier for each model.\nname: Name of the model (e.g., string_matching, refmatch).\n\n\nRelevant Models:\n\nModel ID 1: string_matching\nModel ID 5: refmatch\n\nThese are the models we are focusing on to compare with data extracted from OpenAlex, as no Kaggle model has been applied there.\n\n\ndyad_model.csv\n\nDescription: Connects dyads with the models that identified them. Allows filtering dyads based on the models used.\nKey Columns:\n\ndyad_id: References the id in dyad.csv.\nmodel_id: References the id in model.csv.\n\n\nHow to Use:\n\nTo filter dyads (and thus publications) identified by specific models, perform an inner join with dyad.csv on dyad_id and filter by model_id.\n\n\n\n\n\n\n\nHow the Files are Related\n\n\n\nThe files are structured to represent entities (publications, journals, institutions, authors) and their relationships. The main publication data is in publications_main.csv, and the details about journals, institutions, and authors are in their respective files.\nThe link files (publication_journal_links.csv, publication_institution_links.csv, publication_author_links.csv) represent the many-to-many relationships between publications and these entities.\nTo find all publications by a specific author, you can use authors.csv to find the author’s author_openalex_id and then use publication_author_links.csv to find the associated publications.\nTo analyze the distribution of publications across journals, you can join publications_main.csv, publication_journal_links.csv, and journals.csv on publication_openalex_id and journal_openalex_id.",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "OpenAlex Data Dictionary"
    ]
  },
  {
    "objectID": "appendices/app_openalex.html#sample-data",
    "href": "appendices/app_openalex.html#sample-data",
    "title": "OpenAlex Data Dictionary",
    "section": "Sample Data",
    "text": "Sample Data\n\npublication.csvdataset_alias.csvdyad.csvmodel.csvdyad_model.csv\n\n\n\n\n\n\n\n\n\n\n\n\nid\ntitle\ndoi\nyear\nmonth\n\n\n\n\n321613\nNew estimates for CRNA vacancies\n\n2009\n4\n\n\n321614\nCrossing county lines: The impact of crash location and driver’s…\n10.1016/j.aap.2006…\n2006\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nalias_id\nparent_alias_id\nalias\n\n\n\n\n1676\n87\n89\nCensus of Agriculture\n\n\n1673\n12\n282\nARMS Farm Financial and Crop Production Practices\n\n\n1671\n88\n89\nUSDA Census of Agriculture\n\n\n\n\n\n\n\n\nid\npublication_id\nalias_id\nmention_candidate\n\n\n\n\n2569\n1211491\n87\ncensus of agriculture\n\n\n2573\n1199598\n88\nusda census of agriculture\n\n\n\n\n\n\n\n\nid\nname\n\n\n\n\n1\nstring_matching\n\n\n5\nrefmatch\n\n\n\n\n\n\n\n\nid\ndyad_id\nmodel_id\nscore\n\n\n\n\n4928\n2569\n1\n2.0\n\n\n4929\n2569\n4\n1.0\n\n\n4930\n2569\n2\n1.0",
    "crumbs": [
      "Appendices",
      "Data Schemas and File Inventory",
      "OpenAlex Data Dictionary"
    ]
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#introduction",
    "href": "workflow/step02_02/02openalex.html#introduction",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nThis report describes the process of generating a Seed Corpus and Search Corpus in the context of the Democratizing Data project, specifically using OpenAlex as the publication catalog. The goal is to facilitate more accurate identification of USDA dataset mentions in scholarly publications by using local full-text searches."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#description-of-the-problem",
    "href": "workflow/step02_02/02openalex.html#description-of-the-problem",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.2 Description of the Problem",
    "text": "1.2 Description of the Problem\nPublications identified by dataset mention searches using Scopus were cross-verified in OpenAlex by searching their DOIs. These publications were confirmed to be open-access and available for full-text searches in OpenAlex. However, upon closer investigation, two primary issues were identified with OpenAlex’s full-text indexing methods:\n\nPDF vs. NGRAMS Indexing Methods:\n\nPDF Method: OpenAlex receives the publication’s full text in PDF format and indexes the content directly.\nNGRAMS Method: OpenAlex receives from the author or publisher a preprocessed set of words or phrases (ngrams) extracted from the publication’s full text.\n\nSpecific Issues:\n\nPDF Method Issue: Although undocumented, we observed that the text from the references section of the publications was not indexed by OpenAlex. This occurs because OpenAlex processes the references section specifically to create pointers to other OpenAlex works being referenced. While this approach functions well for publications referencing other scholarly publications, it fails to identify dataset mentions in the references.\nNGRAMS Method Issue: The provided set of ngrams might not include all relevant words or phrases required for dataset identification. For example, if searching for a specific alias such as “USDA Census,” the provided ngrams might not contain the exact phrase or all necessary words, causing missed dataset mentions.\n\n\nThese limitations with OpenAlex’s indexing methods highlighted the need to create dedicated seed and search corpora for accurate dataset mention identification."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#creating-the-seed-corpus",
    "href": "workflow/step02_02/02openalex.html#creating-the-seed-corpus",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.3 Creating the Seed Corpus",
    "text": "1.3 Creating the Seed Corpus\nThe seed corpus generation aims to create an effective subset of publications available in OpenAlex to download locally and subsequently run text searches for dataset aliases and flagged terms.\n\n1.3.1 Filtering Criteria\nTo define the seed corpus, several criteria were established based on topics, publication type, publication year, language, and open-access availability. Below are detailed descriptions of the chosen criteria and associated publication counts.\n\n\n1.3.2 Topics\nWe identified relevant topics based on their frequency and relevance. Below are the top 100 topics by publication count:\n\n\n\n\n\n\n\n\n\nTopic ID\nTopic Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nT11610\nImpact of Food Insecurity on Health Outcomes\n313\n78661\n\n\nT11610\nFood Security and Health in Diverse Populations\n236\n78661\n\n\nT10010\nGlobal Trends in Obesity and Overweight Research\n149\n111686\n\n\nT11066\nComparative Analysis of Organic Agricultural Practices\n141\n41275\n\n\nT12253\nUrban Agriculture and Community Development\n140\n27383\n\n\nT10010\nObesity, Physical Activity, Diet\n123\n111686\n\n\nT10367\nAgricultural Innovation and Livelihood Diversification\n110\n49818\n\n\nT11066\nOrganic Food and Agriculture\n106\n41275\n\n\nT11464\nImpact of Homelessness on Health and Well-being\n101\n101019\n\n\nT12253\nUrban Agriculture and Sustainability\n82\n27383\n\n\nT12033\nEuropean Agricultural Policy and Reform\n77\n88980\n\n\nT10367\nAgricultural Innovations and Practices\n76\n49818\n\n\nT11464\nHomelessness and Social Issues\n74\n101019\n\n\nT10841\nDiscrete Choice Models in Economics and Health Care\n72\n66757\n\n\nT10596\nMaternal and Child Nutrition in Developing Countries\n71\n118727\n\n\nT11898\nImpacts of Food Prices on Consumption and Poverty\n70\n29110\n\n\nT11259\nSustainable Diets and Environmental Impact\n65\n45082\n\n\nT12033\nAgricultural Economics and Policy\n60\n88980\n\n\nT10841\nEconomic and Environmental Valuation\n54\n66757\n\n\nT10439\nAdaptation to Climate Change in Agriculture\n50\n27311\n\n\nT10235\nImpact of Social Factors on Health Outcomes\n49\n86076\n\n\nT10866\nRole of Mediterranean Diet in Health Outcomes\n48\n76894\n\n\nT10596\nChild Nutrition and Water Access\n45\n118727\n\n\nT11259\nAgriculture Sustainability and Environmental Impact\n44\n45082\n\n\nT10330\nHydrological Modeling and Water Resource Management\n43\n132216\n\n\nT11886\nRisk Management and Vulnerability in Agriculture\n43\n44755\n\n\nT11898\nEconomics of Agriculture and Food Markets\n43\n29110\n\n\nT11311\nSoil and Water Nutrient Dynamics\n42\n52847\n\n\nT11311\nBiogeochemical Cycling of Nutrients in Aquatic Ecosystems\n42\n52847\n\n\nT10226\nGlobal Analysis of Ecosystem Services and Land Use\n40\n84104\n\n\nT10969\nOptimal Operation of Water Resources Systems\n39\n97570\n\n\nT12732\nImpact of Farming on Health and Safety\n33\n29731\n\n\nT10235\nHealth disparities and outcomes\n32\n86076\n\n\nT10226\nLand Use and Ecosystem Services\n31\n84104\n\n\nT11753\nForest Management and Policy\n31\n75196\n\n\nT10969\nWater resources management and optimization\n31\n97570\n\n\nT12098\nRural development and sustainability\n30\n62114\n\n\nT12724\nIntegrated Management of Water, Energy, and Food Resources\n30\n40148\n\n\nT11886\nAgricultural risk and resilience\n30\n44755\n\n\nT11753\nClimate Change Impacts on Forest Carbon Sequestration\n29\n75196\n\n\nT11711\nImpacts of COVID-19 on Global Economy and Markets\n29\n69059\n\n\nT10111\nRemote Sensing in Vegetation Monitoring and Phenology\n28\n56452\n\n\nT11404\nDeficit Irrigation for Agricultural Water Management\n27\n49715\n\n\nT10439\nClimate change impacts on agriculture\n27\n27311\n\n\nT11862\nAgroecology and Global Food Systems\n26\n34753\n\n\nT12583\nFood Waste Management and Reduction\n26\n27144\n\n\nT10004\nSoil Carbon Dynamics and Nutrient Cycling in Ecosystems\n26\n101907\n\n\nT10330\nHydrology and Watershed Management Studies\n26\n132216\n\n\nT10556\nGlobal Cancer Incidence and Mortality Patterns\n25\n64063\n\n\nT12098\nRural Development and Change in Agricultural Landscapes\n24\n62114\n\n\nT10111\nRemote Sensing in Agriculture\n24\n56452\n\n\nT10556\nGlobal Cancer Incidence and Screening\n24\n64063\n\n\nT10866\nNutritional Studies and Diet\n22\n76894\n\n\nT11560\nDynamics of Livestock Disease Transmission and Control\n22\n68578\n\n\nT10266\nGlobal Forest Drought Response and Climate Change\n22\n73291\n\n\nT12904\nAgricultural Education and School Gardening Research\n21\n110210\n\n\nT12003\nDevelopment and Impacts of Bioenergy Crops\n21\n36853\n\n\nT10298\nInfluence of Built Environment on Active Travel\n21\n86890\n\n\nT10029\nClimate Change and Variability Research\n20\n113541\n\n\nT11711\nCOVID-19 Pandemic Impacts\n20\n69059\n\n\nT10266\nPlant Water Relations and Carbon Dynamics\n20\n73291\n\n\nT11544\nGender Inequality and Labor Force Dynamics\n19\n98755\n\n\nT13388\nFactors Affecting Sagebrush Ecosystems and Wildlife Conservation\n19\n58614\n\n\nT12904\nDiverse Educational Innovations Studies\n18\n110210\n\n\nT12773\nWater Quality and Hydrogeology Research\n18\n50724\n\n\nT10435\nEnvironmental Impact and Sustainability\n18\n55580\n\n\nT11862\nAgriculture, Land Use, Rural Development\n18\n34753\n\n\nT10435\nLife Cycle Assessment and Environmental Impact Analysis\n18\n55580\n\n\nT13393\nFeeding Disorders in Children with Autism Spectrum Disorders\n17\n50595\n\n\nT12724\nWater-Energy-Food Nexus Studies\n17\n40148\n\n\nT11404\nIrrigation Practices and Water Management\n17\n49715\n\n\nT11560\nAnimal Disease Management and Epidemiology\n17\n68578\n\n\nT13393\nChild Nutrition and Feeding Issues\n16\n50595\n\n\nT11544\nGender, Labor, and Family Dynamics\n16\n98755\n\n\nT10298\nUrban Transport and Accessibility\n15\n86890\n\n\nT11789\nLand Tenure and Property Rights in Agriculture\n15\n46627\n\n\nT10391\nEconomics of Health Care Systems and Policies\n15\n260472\n\n\nT10692\nImpact of Urban Green Space on Public Health\n15\n40686\n\n\nT10889\nSoil Erosion and Agricultural Sustainability\n15\n72441\n\n\nT10004\nSoil Carbon and Nitrogen Dynamics\n14\n101907\n\n\nT10446\nIncome, Poverty, and Inequality\n14\n62906\n\n\nT12057\nImpact of Ultra-Processed Foods on Health\n14\n28199\n\n\nT12873\nImpact of Nutrition and Eating Habits on Health\n14\n43157\n\n\nT11645\nEffects of Residential Segregation on Communities and Individuals\n14\n50639\n\n\nT12583\nFood Waste Reduction and Sustainability\n13\n27144\n\n\nT12310\nFactors Affecting Maize Yield and Lodging Resistance\n13\n105863\n\n\nT10889\nSoil erosion and sediment transport\n13\n72441\n\n\nT10487\nImpact of Pollinator Decline on Ecosystems and Agriculture\n13\n218697\n\n\nT10576\nOpioid Epidemic in the United States\n13\n50143\n\n\nT11186\nGlobal Drought Monitoring and Assessment\n13\n35695\n\n\nT10552\nGlobal Trends in Colorectal Cancer Research\n13\n70491\n\n\nT11925\nFood Tourism and Gastronomy Research\n13\n95356\n\n\nT12399\nFactors Influencing Wine Tourism and Consumer Behavior\n13\n50383\n\n\nT12003\nBioenergy crop production and management\n13\n36853\n\n\nT13388\nRangeland and Wildlife Management\n12\n58614\n\n\nT10410\nModeling the Dynamics of COVID-19 Pandemic\n12\n67192\n\n\nT10190\nHealth Effects of Air Pollution\n12\n125501\n\n\nT12733\nBluetongue Virus and Culicoides-Borne Diseases in Europe\n12\n34477\n\n\nT11546\nPlant Physiology and Cultivation Studies\n12\n189058\n\n\nT11552\nGovernance of Global Value Chains and Production Networks\n12\n46357\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 1,192,809."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#journals",
    "href": "workflow/step02_02/02openalex.html#journals",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.4 Journals",
    "text": "1.4 Journals\nWe selected the top journals to further refine our corpus. The following table lists the top 100 journals by publication count:\n\n\n\n\n\n\n\n\n\nJournal ID\nJournal Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nS2764628096\nJournal of Agriculture Food Systems and Community Development\n57\n825\n\n\nS115427279\nPublic Health Nutrition\n51\n3282\n\n\nS206696595\nJournal of Nutrition Education and Behavior\n41\n3509\n\n\nS15239247\nInternational Journal of Environmental Research and Public Health\n39\n59130\n\n\nS4210201861\nApplied Economic Perspectives and Policy\n39\n647\n\n\nS10134376\nSustainability\n35\n87533\n\n\nS5832799\nJournal of Soil and Water Conservation\n34\n556\n\n\nS2739393555\nJournal of Agricultural and Applied Economics\n34\n329\n\n\nS202381698\nPLoS ONE\n30\n143568\n\n\nS124372222\nRenewable Agriculture and Food Systems\n30\n426\n\n\nS91754907\nAmerican Journal of Agricultural Economics\n28\n876\n\n\nS200437886\nBMC Public Health\n28\n18120\n\n\nS18733340\nJournal of the Academy of Nutrition and Dietetics\n27\n5301\n\n\nS78512408\nAgriculture and Human Values\n27\n938\n\n\nS2764593300\nAgricultural and Resource Economics Review\n25\n247\n\n\nS110785341\nNutrients\n25\n30911\n\n\nS4210212157\nFrontiers in Sustainable Food Systems\n23\n3776\n\n\nS69340840\nThe Journal of Rural Health\n20\n749\n\n\nS63571384\nFood Policy\n20\n1069\n\n\nS19383905\nAgricultural Finance Review\n18\n327\n\n\nS4210234824\nEDIS\n18\n3714\n\n\nS119228529\nJournal of Hunger & Environmental Nutrition\n17\n467\n\n\nS204691207\nHortTechnology\n14\n847\n\n\nS4210212179\nJournal of Extension\n14\n1004\n\n\nS43295729\nRemote Sensing\n14\n33899\n\n\nS2738397068\nLand\n14\n9774\n\n\nS80485027\nLand Use Policy\n14\n4559\n\n\nS4210217848\nJAMA Network Open\n13\n12933\n\n\nS139338987\nEnvironmental Research Letters\n13\n6399\n\n\nS2595931848\nFrontiers in Public Health\n12\n19316\n\n\nS122347013\nAmerican Journal of Obstetrics and Gynecology\n12\n15259\n\n\nS204847658\nWater Resources Research\n12\n5305\n\n\nS73449225\nFood Security\n12\n899\n\n\nS4210219560\nCurrent Developments in Nutrition\n12\n10807\n\n\nS183652945\nHortScience\n11\n2415\n\n\nS196734849\nScientific Reports\n11\n198095\n\n\nS139950591\nAgronomy Journal\n10\n2675\n\n\nS86852077\nThe Science of The Total Environment\n10\n56249\n\n\nS37976914\nJAWRA Journal of the American Water Resources Association\n9\n852\n\n\nS2764587901\nJournal of Applied Communications\n9\n250\n\n\nS2607323502\nScientific Data\n9\n5287\n\n\nS44455300\nJournal of Environmental Management\n9\n17835\n\n\nS4210220469\nJournal of Applied Farm Economics\n8\n39\n\n\nS141808269\nRemote Sensing of Environment\n8\n4135\n\n\nS129060628\nDiabetes\n8\n17439\n\n\nS2475403985\nPreventing Chronic Disease\n8\n1068\n\n\nS156283932\nCalifornia Agriculture\n8\n233\n\n\nS157560195\nAgricultural Systems\n8\n1722\n\n\nS136211407\nEcological Economics\n8\n2684\n\n\nS23642417\nSociety & Natural Resources\n8\n792\n\n\nS2574783\nGynecologic Oncology\n8\n8756\n\n\nS149285975\nLand Economics\n8\n399\n\n\nS2594976040\nFrontiers in Veterinary Science\n7\n9896\n\n\nS8391440\nCancer Epidemiology Biomarkers & Prevention\n7\n6099\n\n\nS6596815\nRural Sociology\n7\n467\n\n\nS4210180312\nJournal of the Agricultural and Applied Economics Association\n7\n161\n\n\nS4210202585\nAgriculture\n7\n9931\n\n\nS79054089\nBMJ Open\n7\n31973\n\n\nS2764832999\nScientific investigations report\n7\n1270\n\n\nS180723199\nAgribusiness\n7\n583\n\n\nS154775064\nAgricultural Economics\n7\n729\n\n\nS2738534743\nJournal of Nutritional Science\n6\n659\n\n\nS2754843627\nCancer Medicine\n6\n7317\n\n\nS2228914\nHealth Services Research\n6\n1855\n\n\nS2764680059\nStatistical Journal of the IAOS\n6\n852\n\n\nS76844451\nAnnual Review of Resource Economics\n6\n206\n\n\nS207068962\nCommunity Development\n6\n516\n\n\nS148307540\nEcology and Society\n6\n1281\n\n\nS4210194219\nAntarctica A Keystone in a Changing World\n6\n1110\n\n\nS4210186936\nJournal of Agricultural Science\n6\n2412\n\n\nS12132826\nThe International Food and Agribusiness Management Review\n6\n464\n\n\nS4210197466\nAgroecology and Sustainable Food Systems\n6\n645\n\n\nS204799461\nClimatic Change\n6\n1992\n\n\nS139838620\nObstetrics and Gynecology\n6\n8132\n\n\nS2596909297\nFrontiers in Nutrition\n6\n9700\n\n\nS168049282\nAmerican Journal of Public Health\n6\n4777\n\n\nS106822843\nSocial Science & Medicine\n5\n5847\n\n\nS134216166\nWater\n5\n25819\n\n\nS28036099\nJournal of Rural Studies\n5\n2034\n\n\nS2737313858\nAgricultural & Environmental Letters\n5\n262\n\n\nS32361082\nEuropean Review of Agricultural Economics\n5\n362\n\n\nS178566096\nPreventive Veterinary Medicine\n5\n1907\n\n\nS150168663\nCancer Causes & Control\n5\n1136\n\n\nS106908163\nNeuro-Oncology\n5\n18986\n\n\nS178182516\nJournal of Agromedicine\n5\n565\n\n\nS116775814\nComputers and Electronics in Agriculture\n5\n5965\n\n\nS176659572\nHealth & Social Care in the Community\n5\n2064\n\n\nS2764613780\nJournal of Agricultural Safety and Health\n5\n138\n\n\nS99400149\nJournal of Health Care for the Poor and Underserved\n5\n1197\n\n\nS42419699\nPrecision Agriculture\n5\n1130\n\n\nS130750583\nGlobal Environmental Change\n5\n1092\n\n\nS2492648963\nTransactions of the ASABE\n5\n894\n\n\nS135458494\nPlant Disease\n5\n8264\n\n\nS72684844\nJournal of Animal Science\n5\n15499\n\n\nS173554290\nJournal of Community Health\n5\n1126\n\n\nS28349394\nJournal of Dairy Science\n5\n8060\n\n\nS88153332\nJournal of Nutrition\n5\n3469\n\n\nS199825796\nApplied Engineering in Agriculture\n5\n722\n\n\nS95823145\nForest Policy and Economics\n5\n1509\n\n\nS104641133\nAgricultural Water Management\n5\n4298\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 770,522."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#authors",
    "href": "workflow/step02_02/02openalex.html#authors",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.5 Authors",
    "text": "1.5 Authors\nAuthors with American affiliations were selected to enhance corpus relevance:\n\n\n\n\n\n\n\n\n\nAuthor ID\nAuthor Name\nFirst Run Count\nOpenAlex Total Count\n\n\n\n\nA5016803484\nHeather A. Eicher‐Miller\n15\n140\n\n\nA5024975191\nEdward A. Frongillo\n13\n351\n\n\nA5055158106\nBecca B.R. Jablonski\n12\n60\n\n\nA5047780964\nMeredith T. Niles\n11\n200\n\n\nA5015017711\nJeffrey K. O’Hara\n10\n27\n\n\nA5062679478\nJ. Gordon Arbuckle\n10\n68\n\n\nA5068812455\nCindy W. Leung\n10\n170\n\n\nA5076121862\nSheri D. Weiser\n10\n241\n\n\nA5081656928\nWhitney E. Zahnd\n9\n147\n\n\nA5008463933\nCatherine Brinkley\n8\n34\n\n\nA5027684365\nDayton M. Lambert\n8\n110\n\n\nA5002438645\nPhyllis C. Tien\n8\n244\n\n\nA5081012770\nLinda J. Young\n8\n51\n\n\nA5030548116\nMichele Ver Ploeg\n8\n33\n\n\nA5035584432\nAngela D. Liese\n8\n172\n\n\nA5032940306\nLisa Harnack\n7\n89\n\n\nA5008296893\nEryka Wentz\n7\n33\n\n\nA5006129622\nCarmen Byker Shanks\n7\n103\n\n\nA5053170901\nAni L. Katchova\n7\n62\n\n\nA5024127854\nEduardo Villamor\n7\n84\n\n\nA5060802257\nTracey E. Wilson\n7\n102\n\n\nA5050792105\nJennifer L. Moss\n7\n90\n\n\nA5040727809\nGeorge B. Frisvold\n7\n66\n\n\nA5056021318\nNathan Hendricks\n7\n320\n\n\nA5034750133\nLila A. Sheira\n7\n61\n\n\nA5044317355\nDaniel Merenstein\n7\n113\n\n\nA5002732604\nJulia A. Wolfson\n7\n137\n\n\nA5015455112\nHikaru Hanawa Peterson\n7\n56\n\n\nA5024248662\nAdebola Adedimeji\n7\n137\n\n\nA5038610136\nChristopher N. Boyer\n7\n115\n\n\nA5101813658\nChristian J. Peters\n7\n32\n\n\nA5035164673\nStephan J. Goetz\n6\n90\n\n\nA5029397288\nAmy L. Yaroch\n6\n113\n\n\nA5022651324\nSeth A. Berkowitz\n6\n158\n\n\nA5083470674\nMardge H. Cohen\n6\n205\n\n\nA5070284513\nTimothy S. Griffin\n6\n59\n\n\nA5026810637\nJoleen C. Hadrich\n6\n30\n\n\nA5013419936\nNigel Key\n6\n25\n\n\nA5062332393\nAlessandro Bonanno\n6\n61\n\n\nA5012666568\nHilary K. Seligman\n6\n123\n\n\nA5071854708\nBurton C. English\n6\n68\n\n\nA5069981543\nMegan Konar\n6\n86\n\n\nA5083406390\nZach Conrad\n6\n123\n\n\nA5074296013\nSuat Irmak\n6\n151\n\n\nA5079036202\nJames A. Larson\n6\n49\n\n\nA5038417176\nAdaora A. Adimora\n6\n334\n\n\nA5045489628\nSelena Ahmed\n6\n89\n\n\nA5057302432\nAlan W. Hodges\n6\n73\n\n\nA5091590760\nCraig Gundersen\n6\n69\n\n\nA5089578074\nParke Wilde\n6\n107\n\n\nA5063008522\nA. D. Kendall\n6\n119\n\n\nA5100771544\nHanqin Tian\n6\n434\n\n\nA5072286156\nD. W. Hyndman\n6\n130\n\n\nA5052456209\nKartika Palar\n6\n59\n\n\nA5042679164\nJeffrey Gillespie\n6\n30\n\n\nA5091103546\nKimberly L. Jensen\n6\n65\n\n\nA5014800024\nKartik K. Venkatesh\n5\n244\n\n\nA5003088939\nFrances Hardin‐Fanning\n5\n32\n\n\nA5028409673\nLauri M. Baker\n5\n60\n\n\nA5087431618\nGabrielle Roesch‐McNally\n5\n36\n\n\nA5112481717\nJianhong E. Mu\n5\n20\n\n\nA5067158518\nLisa R. Metsch\n5\n133\n\n\nA5051019392\nDawn Thilmany McFadden\n5\n20\n\n\nA5065308164\nEdward C. Jaenicke\n5\n59\n\n\nA5035062421\nKatherine Dentzman\n5\n35\n\n\nA5011693138\nRyan S. Miller\n5\n163\n\n\nA5019910416\nHolly Gibbs\n5\n69\n\n\nA5014991206\nMargarita Velandia\n5\n29\n\n\nA5081521085\nMark Lubell\n5\n80\n\n\nA5007931812\nTyler J. Lark\n5\n77\n\n\nA5078358162\nJanet M. Turan\n5\n189\n\n\nA5089582462\nLynn M. Yee\n5\n426\n\n\nA5040186224\nNathanael M. Thompson\n5\n32\n\n\nA5070695418\nIghovwerha Ofotokun\n5\n105\n\n\nA5018179894\nAmir M. Rahmani\n5\n264\n\n\nA5057015263\nDawn Thilmany\n5\n34\n\n\nA5038972534\nJyotsna S. Jagai\n5\n48\n\n\nA5003676504\nLandon Marston\n5\n84\n\n\nA5079390198\nChen Zhen\n5\n48\n\n\nA5043968039\nW. David Mulkey\n5\n7\n\n\nA5072793478\nClayton Hallman\n5\n11\n\n\nA5016915956\nJohn Tyndall\n5\n35\n\n\nA5044462604\nJohn M. Antle\n5\n49\n\n\nA5105267439\nColleen T. Webb\n5\n44\n\n\nA5016997673\nMiguel I. Gómez\n5\n138\n\n\nA5006518901\nAndrea Leschewski\n5\n18\n\n\nA5022734861\nAllison Bauman\n5\n32\n\n\nA5010819579\nLisa Chase\n5\n34\n\n\nA5044003446\nW. Jay Christian\n5\n50\n\n\nA5022220353\nBailey Houghtaling\n5\n72\n\n\nA5066919611\nRick Welsh\n5\n20\n\n\nA5053832932\nEric M. Clark\n4\n41\n\n\nA5002482027\nBenjamin M. Gramig\n4\n46\n\n\nA5029506929\nCourtney D. Lynch\n4\n84\n\n\nA5031318120\nJessica Rudnick\n4\n15\n\n\nA5046729938\nSteven R. Browning\n4\n23\n\n\nA5052396793\nLindsay M. Beck‐Johnson\n4\n13\n\n\nA5027592346\nDennis P. Swaney\n4\n26\n\n\nA5042166415\nDavid H. Fleisher\n4\n69\n\n\nA5008867112\nKatie Portacci\n4\n14\n\n\n\nFilters applied: - Language: English (en) - Publication Year: &gt;2017 - Type: article, review - Open Access: True\nFiltered publications count: 3,714."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#generating-the-search-corpus",
    "href": "workflow/step02_02/02openalex.html#generating-the-search-corpus",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.6 Generating the Search Corpus",
    "text": "1.6 Generating the Search Corpus\nUpon applying the agreed filters, the final seed corpus resulted in 1,774,245 unique publications. An initial Python script was developed to collect full texts of these publications.\n\n1.6.1 Initial Full Text Download Results\n\n\n\nMetric\nResult\n\n\n\n\nTotal publications attempted\n2,774\n\n\nSuccessfully downloaded full-texts\n974\n\n\nSuccess Rate\n35%\n\n\nEstimated full texts (projected)\n625,000 out of 1,774,245\n\n\nTotal estimated processing time\n~124 days\n\n\n\nThe relatively low success rate indicates significant challenges in accessing full texts, primarily due to missing or inaccessible OA URLs."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#current-limitations-and-considerations",
    "href": "workflow/step02_02/02openalex.html#current-limitations-and-considerations",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.7 Current Limitations and Considerations",
    "text": "1.7 Current Limitations and Considerations\n\nOnly 35% success rate in downloading full texts.\nThe existing process is slow, computationally intensive, and likely to require improvements or distributed computing.\nComparison with OpenAlex’s built-in full-text search shows it might be sufficient in certain cases, potentially reducing the necessity of local processing."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#next-steps",
    "href": "workflow/step02_02/02openalex.html#next-steps",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.8 Next Steps",
    "text": "1.8 Next Steps\n\nImplement distributed processing to accelerate corpus generation.\nAssess the adequacy of OpenAlex’s built-in full-text search for practical usage scenarios.\nBalance the need for accuracy with available resources (time and cost)."
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#references",
    "href": "workflow/step02_02/02openalex.html#references",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.9 References",
    "text": "1.9 References\n\nOpenAlex API documentation: OpenAlex Works API\nOA filtered results: OpenAlex Filtered Corpus\nOpenAlex: https://docs.openalex.org"
  },
  {
    "objectID": "workflow/step02_02/02openalex.html#conclusion",
    "href": "workflow/step02_02/02openalex.html#conclusion",
    "title": "1 Detailed Report: Seed and Search Corpus Generation using OpenAlex",
    "section": "1.10 Conclusion",
    "text": "1.10 Conclusion\nCreating a locally processed seed corpus from OpenAlex significantly improves dataset mention accuracy but poses considerable resource demands. While local full-text processing enhances specificity, careful consideration is required regarding when OpenAlex’s native search capabilities are sufficient.\n\nReferences: - OpenAlex API Documentation: https://docs.openalex.org - Democratizing Data project repository and guidelines (internal documentation, 2025). - USDA Dataset Project Documentation (Internal Document, 2025)."
  },
  {
    "objectID": "report.html#institutional-comparison",
    "href": "report.html#institutional-comparison",
    "title": "Methodology for Comparing Citation Database Coverage of Dataset Usage",
    "section": "Institutional Comparison",
    "text": "Institutional Comparison\n\n  \n\nSource code used to generate graphic: Available here.",
    "crumbs": [
      "Findings"
    ]
  }
]